# Comparing `tmp/e2eqavn-0.1.1-py2.py3-none-any.whl.zip` & `tmp/e2eqavn-0.1.3-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,42 +1,42 @@
-Zip file size: 36304 bytes, number of entries: 40
--rw-rw-r--  2.0 unx      498 b- defN 23-Jun-20 02:12 e2eqavn/__init__.py
--rw-rw-r--  2.0 unx     9274 b- defN 23-Jun-20 02:07 e2eqavn/cli.py
--rw-rw-r--  2.0 unx     2494 b- defN 23-Jun-20 02:06 e2eqavn/keywords.py
--rw-rw-r--  2.0 unx     2973 b- defN 23-May-20 15:46 e2eqavn/datasets/MRCDataset.py
+Zip file size: 37577 bytes, number of entries: 40
+-rw-rw-r--  2.0 unx      498 b- defN 23-Jul-01 11:27 e2eqavn/__init__.py
+-rw-rw-r--  2.0 unx    10774 b- defN 23-Jun-23 18:38 e2eqavn/cli.py
+-rw-rw-r--  2.0 unx     2716 b- defN 23-Jun-20 14:02 e2eqavn/keywords.py
+-rw-rw-r--  2.0 unx     3639 b- defN 23-Jun-23 18:48 e2eqavn/datasets/MRCDataset.py
 -rw-rw-r--  2.0 unx     1386 b- defN 23-Apr-08 19:20 e2eqavn/datasets/TripletDataset.py
 -rw-rw-r--  2.0 unx      124 b- defN 23-May-02 14:04 e2eqavn/datasets/__init__.py
--rw-rw-r--  2.0 unx     3660 b- defN 23-May-30 15:21 e2eqavn/datasets/data_collator.py
+-rw-rw-r--  2.0 unx     3814 b- defN 23-Jun-20 08:22 e2eqavn/datasets/data_collator.py
 -rw-rw-r--  2.0 unx       85 b- defN 23-Apr-08 14:59 e2eqavn/documents/__init__.py
 -rw-rw-r--  2.0 unx    12681 b- defN 23-Jun-12 11:14 e2eqavn/documents/corpus.py
 -rw-rw-r--  2.0 unx      251 b- defN 23-Apr-01 19:32 e2eqavn/documents/document_store.py
 -rw-rw-r--  2.0 unx       93 b- defN 23-May-02 08:29 e2eqavn/evaluate/__init__.py
 -rw-rw-r--  2.0 unx      283 b- defN 23-Apr-10 04:59 e2eqavn/evaluate/bm25_evaluate_retrieval.py
 -rw-rw-r--  2.0 unx     3408 b- defN 23-May-20 17:10 e2eqavn/evaluate/information_retrieval_evaluator_custom.py
 -rw-rw-r--  2.0 unx     1958 b- defN 23-May-17 02:36 e2eqavn/evaluate/mrc_evaluator.py
 -rw-rw-r--  2.0 unx      110 b- defN 23-May-29 09:53 e2eqavn/mrc/__init__.py
--rw-rw-r--  2.0 unx      797 b- defN 23-May-25 08:47 e2eqavn/mrc/base.py
--rw-rw-r--  2.0 unx    10739 b- defN 23-Jun-14 13:28 e2eqavn/mrc/mrc_model.py
+-rw-rw-r--  2.0 unx      887 b- defN 23-Jun-20 03:52 e2eqavn/mrc/base.py
+-rw-rw-r--  2.0 unx    11913 b- defN 23-Jun-24 07:59 e2eqavn/mrc/mrc_model.py
 -rw-rw-r--  2.0 unx       61 b- defN 23-Apr-09 14:32 e2eqavn/pipeline/__init__.py
 -rw-rw-r--  2.0 unx     2083 b- defN 23-May-25 09:03 e2eqavn/pipeline/e2e_question_answering.py
 -rw-rw-r--  2.0 unx     2090 b- defN 23-Apr-14 20:19 e2eqavn/pipeline/pipeline.py
 -rw-rw-r--  2.0 unx      145 b- defN 23-May-06 10:41 e2eqavn/processor/__init__.py
--rw-rw-r--  2.0 unx     4462 b- defN 23-Apr-17 14:59 e2eqavn/processor/bm25.py
+-rw-rw-r--  2.0 unx     4463 b- defN 23-Jun-20 08:08 e2eqavn/processor/bm25.py
 -rw-rw-r--  2.0 unx     2248 b- defN 23-Apr-01 18:52 e2eqavn/processor/chunk.py
--rw-rw-r--  2.0 unx     5957 b- defN 23-Jun-07 15:14 e2eqavn/processor/qa_ext_processor.py
+-rw-rw-r--  2.0 unx     7252 b- defN 23-Jun-23 18:36 e2eqavn/processor/qa_ext_processor.py
 -rw-rw-r--  2.0 unx     6506 b- defN 23-Apr-17 15:11 e2eqavn/processor/retrieval_sampling.py
 -rw-rw-r--  2.0 unx      138 b- defN 23-Apr-08 20:11 e2eqavn/retrieval/__init__.py
 -rw-rw-r--  2.0 unx      644 b- defN 23-Apr-15 09:08 e2eqavn/retrieval/base.py
 -rw-rw-r--  2.0 unx     3322 b- defN 23-Apr-16 17:04 e2eqavn/retrieval/bm25_retrieval.py
 -rw-rw-r--  2.0 unx    10419 b- defN 23-May-25 09:03 e2eqavn/retrieval/sbert_retrieval.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Mar-18 08:56 e2eqavn/utils/__init__.py
--rw-rw-r--  2.0 unx    12493 b- defN 23-Jun-07 16:27 e2eqavn/utils/calculate.py
+-rw-rw-r--  2.0 unx    12985 b- defN 23-Jun-20 09:17 e2eqavn/utils/calculate.py
 -rw-rw-r--  2.0 unx      603 b- defN 23-May-31 16:18 e2eqavn/utils/io.py
 -rw-rw-r--  2.0 unx      998 b- defN 23-May-03 09:44 e2eqavn/utils/preprocess.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Mar-19 07:27 test/__init__.py
 -rw-rw-r--  2.0 unx      334 b- defN 23-Apr-08 14:59 test/test_chunking.py
--rw-rw-r--  2.0 unx      679 b- defN 23-Jun-20 02:13 e2eqavn-0.1.1.dist-info/METADATA
--rw-rw-r--  2.0 unx      110 b- defN 23-Jun-20 02:13 e2eqavn-0.1.1.dist-info/WHEEL
--rw-rw-r--  2.0 unx       52 b- defN 23-Jun-20 02:13 e2eqavn-0.1.1.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx       13 b- defN 23-Jun-20 02:13 e2eqavn-0.1.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     3377 b- defN 23-Jun-20 02:13 e2eqavn-0.1.1.dist-info/RECORD
-40 files, 107548 bytes uncompressed, 30888 bytes compressed:  71.3%
+-rw-rw-r--  2.0 unx      679 b- defN 23-Jul-01 11:27 e2eqavn-0.1.3.dist-info/METADATA
+-rw-rw-r--  2.0 unx      110 b- defN 23-Jul-01 11:27 e2eqavn-0.1.3.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       52 b- defN 23-Jul-01 11:27 e2eqavn-0.1.3.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx       13 b- defN 23-Jul-01 11:27 e2eqavn-0.1.3.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3378 b- defN 23-Jul-01 11:27 e2eqavn-0.1.3.dist-info/RECORD
+40 files, 113143 bytes uncompressed, 32161 bytes compressed:  71.6%
```

## zipnote {}

```diff
@@ -99,23 +99,23 @@
 
 Filename: test/__init__.py
 Comment: 
 
 Filename: test/test_chunking.py
 Comment: 
 
-Filename: e2eqavn-0.1.1.dist-info/METADATA
+Filename: e2eqavn-0.1.3.dist-info/METADATA
 Comment: 
 
-Filename: e2eqavn-0.1.1.dist-info/WHEEL
+Filename: e2eqavn-0.1.3.dist-info/WHEEL
 Comment: 
 
-Filename: e2eqavn-0.1.1.dist-info/entry_points.txt
+Filename: e2eqavn-0.1.3.dist-info/entry_points.txt
 Comment: 
 
-Filename: e2eqavn-0.1.1.dist-info/top_level.txt
+Filename: e2eqavn-0.1.3.dist-info/top_level.txt
 Comment: 
 
-Filename: e2eqavn-0.1.1.dist-info/RECORD
+Filename: e2eqavn-0.1.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## e2eqavn/__init__.py

```diff
@@ -16,9 +16,9 @@
 
 stream_handler.setFormatter(formatted)
 logger.addHandler(stream_handler)
 logger.setLevel(logging.INFO)
 logger.propagate = False
 
 __author__ = 'khanhdm'
-__version__ = '0.1.1'
+__version__ = '0.1.3'
```

## e2eqavn/cli.py

```diff
@@ -34,15 +34,15 @@
 @click.command()
 @click.option(
     '--config', '-c',
     required=True,
     default='config/config.yaml',
     help='Path config model'
 )
-def train(config: Union[str, Text], mode: str):
+def train(config: Union[str, Text]):
     config_pipeline = load_yaml_file(config)
     train_corpus = Corpus.parser_uit_squad(
         config_pipeline[DATA][PATH_TRAIN],
         **config_pipeline.get(CONFIG_DATA, {})
     )
     retrieval_config = config_pipeline.get(RETRIEVAL, None)
     reader_config = config_pipeline.get(READER, None)
@@ -67,18 +67,20 @@
             config_pipeline[DATA][PATH_EVALUATOR],
             **config_pipeline.get(CONFIG_DATA, {})
         )
         mrc_dataset = MRCDataset.init_mrc_dataset(
             corpus_train=train_corpus,
             corpus_eval=eval_corpus,
             model_name_or_path=reader_config[MODEL].get(MODEL_NAME_OR_PATH, 'khanhbk20/mrc_testing'),
-            max_length=reader_config[MODEL].get(MAX_LENGTH, 368)
+            max_length=reader_config[MODEL].get(MAX_LENGTH, 368),
+            **reader_config.get(DATA_ARGUMENT, {})
         )
         reader_model = MRCReader.from_pretrained(
-            model_name_or_path=reader_config[MODEL].get(MODEL_NAME_OR_PATH, 'khanhbk20/mrc_testing')
+            model_name_or_path=reader_config[MODEL].get(MODEL_NAME_OR_PATH, 'khanhbk20/mrc_testing'),
+            lambda_weight=reader_config.get(DATA_ARGUMENT, {}).get(LAMBDA_WEIGHT, 0.6)
         )
         reader_model.init_trainer(mrc_dataset=mrc_dataset, **reader_config[MODEL])
         reader_model.train()
 
 
 @click.command()
 @click.option(
@@ -183,19 +185,44 @@
                 idx += 1
         pred_answers = pipeline.run(
             queries=list_questions,
             top_k_bm25=top_k_bm25,
             top_k_sbert=top_k_sbert,
             top_k_qa=1
         )
+        if logging_result_pipeline:
+            results_logging = []
         for idx, ans_pred in enumerate(pred_answers['answer']):
             predictions.append(
                 {'prediction_text': ans_pred[0].get('answer', ""), 'id': str(idx)}
             )
-        logger.info(f"Evaluate E2E pipeline: {metric_fn.compute(predictions=predictions, reference=ground_truth)}")
+            ground_truth[idx]['answers']['answer_start'] = [ans_pred[0]['answer_start_idx']] * len(
+                ground_truth[idx]['answers']['text'])
+            if logging_result_pipeline:
+                results_logging.append({
+                    'question': list_questions[idx],
+                    'answer_pred': ans_pred[0].get('answer', ""),
+                    'answer_truth': ground_truth[idx],
+                    'logging': [
+                        {
+                            'doc': doc_retrieval.document_context,
+                            'bm25_score': doc_retrieval.bm25_score,
+                            'score_start': doc_reader.get('score_start', 0),
+                            'score_end': doc_reader.get('score_end', 0),
+                            'reader_score': doc_reader.get('score', 0),
+                            'answer_start_idx': doc_reader.get('answer_start_idx', 0),
+                            'answer_end_idx': doc_reader.get('answer_end_idx', 0)
+                        } for doc_retrieval, doc_reader in
+                        zip(pred_answers['documents'][idx], pred_answers['reader_logging'][idx])
+                    ]
+                }
+                )
+        if logging_result_pipeline:
+            write_json_file(results_logging, 'logging.json')
+        logger.info(f"Evaluate E2E pipeline: {metric_fn.compute(predictions=predictions, references=ground_truth)}")
 
 
 @click.command()
 @click.option(
     '--config', '-c',
     required=True,
     help='Path config model'
@@ -240,15 +267,15 @@
         pipeline.add_component(
             component=retrieval_model,
             name_component='retrieval_2'
         )
 
     if mode in ['reader', 'pipeline'] and reader_config:
         reader_model = MRCReader.from_pretrained(
-            model_name_or_path=reader_config[MODEL].get(MODEL_NAME_OR_PATH, 'khanhbk20/mrc_testing')
+            model_name_or_path=reader_config[MODEL].get(MODEL_NAME_OR_PATH, 'khanhbk20/mrc_dev')
         )
         pipeline.add_component(
             component=reader_model,
             name_component='reader'
         )
     output = pipeline.run(
         queries=question,
```

## e2eqavn/keywords.py

```diff
@@ -83,11 +83,17 @@
 SAVE_TOTAL_LIMIT = 'save_total_limit'
 EVALUATION_STRATEGY = 'evaluation_strategy'
 LOGGING_STRATEGY = 'logging_strategy'
 BATCH_SIZE_TRAINING = 'batch_size_training'
 BATCH_SIZE_EVAL = 'batch_size_eval'
 BATCH_SIZE = 'batch_size'
 EPOCHS = 'epochs'
+MAKE_NEGATIVE_MRC = 'make_negative_mrc'
+N_NEGATIVE_MRC = 'n_negative_mrc'
+IS_NEGATIVE_SAMPLE = 'is_negative_sample'
+DATA_ARGUMENT = 'data_argument'
+LAMBDA_WEIGHT = 'lambda_weight'
+THRESHOLD_SAMPLING = 'threshold_sampling'
```

## e2eqavn/datasets/MRCDataset.py

```diff
@@ -22,25 +22,37 @@
 
     @classmethod
     def make_dataset(cls, corpus: Corpus, mode: str, **kwargs):
         logger.info(f"Start prepare {mode} dataset")
         logger.info(f"Max length sentence = {kwargs.get(MAX_LENGTH, 512)}")
         if MODEL_NAME_OR_PATH not in kwargs:
             raise Exception("You must provide pretrained name for QA")
-        tokenizer = AutoTokenizer.from_pretrained(kwargs.get(MODEL_NAME_OR_PATH))
+
+        try:
+            tokenizer = AutoTokenizer.from_pretrained(kwargs.get(MODEL_NAME_OR_PATH))
+        except:
+            path_config = os.path.join(kwargs.get(MODEL_NAME_OR_PATH), 'config.json')
+            if os.path.isfile(path_config):
+                logger.info(f"Start load config path at {path_config}")
+                config_checkpoint = load_json_data(os.path.join(path_config))
+                name_pretrained = config_checkpoint['_name_or_path']
+                tokenizer = AutoTokenizer.from_pretrained(name_pretrained)
+            else:
+                raise Exception(f"Can't load tokenizer from {path_config}")
+
         num_proc = kwargs.get(NUM_PROC, 5)
         qa_text_processor = QATextProcessor(
             context_key=kwargs.get(CONTEXT_KEY, 'context'),
             question_key=kwargs.get(QUESTION_KEY, 'question'),
             answer_key=kwargs.get(ANSWER_KEY, 'answer'),
             answer_start_key=kwargs.get(ANSWER_START, 'answer_start'),
             answer_word_start_idx_key=kwargs.get(ANSWER_WORD_START_IDX, 'answer_word_start_idx'),
             answer_word_end_idx_key=kwargs.get(ANSWER_WORD_END_IDX, 'answer_word_end_idx')
         )
-        examples = qa_text_processor.make_example(corpus)
+        examples = qa_text_processor.make_example(corpus, **kwargs)
         dir_save = kwargs.get(FOLDER_QA_SAVE, 'data/qa')
         if not os.path.exists(dir_save):
             os.makedirs(dir_save, exist_ok=True)
         logger.info(f"Dataset for {mode} has {len(examples)} sample")
         examples = {'data': examples}
         write_json_file(examples, os.path.join(dir_save, f"{mode}.json"))
         dataset = load_dataset(
@@ -65,11 +77,13 @@
     def init_mrc_dataset(cls, corpus_train: Corpus = None, corpus_eval: Corpus = None, **kwargs):
         if corpus_train is not None:
             train_dataset = cls.make_dataset(corpus_train, mode='train', **kwargs)
         else:
             train_dataset = None
 
         if corpus_eval is not None:
+            if kwargs.get(MAKE_NEGATIVE_MRC, False):
+                kwargs[MAKE_NEGATIVE_MRC] = False
             eval_dataset = cls.make_dataset(corpus_eval, mode='validation', **kwargs)
         else:
             eval_dataset = None
         return cls(train_dataset, eval_dataset)
```

## e2eqavn/datasets/data_collator.py

```diff
@@ -44,21 +44,23 @@
                 sample[SPAN_ANSWER_IDS] = sample[INPUT_IDS][start_idx: end_idx]
             span_answer_ids = collate_fn(
                 [torch.tensor(sample[SPAN_ANSWER_IDS]) for sample in batch],
                 padding_value=-100
             )
             start_idxs = torch.tensor([sample[START_IDX] for sample in batch])
             end_idxs = torch.tensor([sample[END_IDX] for sample in batch])
+            is_negative_sample = torch.Tensor([sample[IS_NEGATIVE_SAMPLE] for sample in batch])
             return {
                 'input_ids': input_ids,
                 'attention_mask': attention_masks,
                 'start_positions': start_idxs,
                 'end_positions': end_idxs,
                 'words_length': words_length,
-                'span_answer_ids': span_answer_ids
+                'span_answer_ids': span_answer_ids,
+                'is_negative_sample': is_negative_sample
             }
 
         data = {
             'input_ids': input_ids,
             'attention_mask': attention_masks,
             'words_length': words_length,
         }
```

## e2eqavn/mrc/base.py

```diff
@@ -15,12 +15,14 @@
         if len(documents) == 0:
             return {
                 "query": queries,
                 "answer": [],
                 **kwargs
             }
         else:
+            predict, raw_predict = self.predict(queries, documents, **kwargs)
             return {
                 "query": queries,
                 "documents": documents,
-                "answer": self.predict(queries, documents, **kwargs)
+                "answer": predict,
+                'reader_logging': raw_predict
             }
```

## e2eqavn/mrc/mrc_model.py

```diff
@@ -18,32 +18,34 @@
 
 class MRCQuestionAnsweringModel(RobertaPreTrainedModel, ABC):
     config_class = RobertaConfig
 
     # _keys_to_ignore_on_load_unexpected = [r"pooler"]
     # _keys_to_ignore_on_load_missing = [r"position_ids"]
 
-    def __init__(self, config):
+    def __init__(self, config, lambda_weight: float = 0.6):
         super().__init__(config)
         self.roberta = RobertaModel(config, add_pooling_layer=False)
         self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)
+        self.lambda_weight = lambda_weight
 
     def forward(self,
                 input_ids: Tensor = None,
                 words_length: Tensor = None,
                 start_idx=None,
                 end_idx=None,
                 attention_mask: Tensor = None,
                 token_type_ids=None,
                 position_ids=None,
                 head_mask=None,
                 inputs_embeds=None,
                 start_positions: Tensor = None,
                 end_positions: Tensor = None,
                 span_answer_ids=None,
+                is_negative_sample=None,
                 output_attentions=None,
                 output_hidden_states=None,
                 return_dict=None, ):
 
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
         outputs = self.roberta(
@@ -76,29 +78,47 @@
         context_embedding_align = torch.bmm(align_matrix, context_embedding)
 
         logits = self.qa_outputs(context_embedding_align)
         start_logits, end_logits = logits.split(1, dim=-1)
         start_logits = start_logits.squeeze(-1).contiguous()
         end_logits = end_logits.squeeze(-1).contiguous()
 
-        total_loss = None
+        total_loss = 0
         if start_positions is not None and end_positions is not None:
             # If we are on multi-GPU, split add a dimension
             if len(start_positions.size()) > 1:
                 start_positions = start_positions.squeeze(-1)
             if len(end_positions.size()) > 1:
                 end_positions = end_positions.squeeze(-1)
-            # sometimes the start/end positions are outside our model inputs, we ignore these terms
+
             ignored_index = start_logits.size(1)
             start_positions = start_positions.clamp(0, ignored_index)
             end_positions = end_positions.clamp(0, ignored_index)
+            list_negative = torch.where(is_negative_sample == 0)[0]
+            list_positive = torch.where(is_negative_sample == 1)[0]
             loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)
-            start_loss = loss_fct(start_logits, start_positions)
-            end_loss = loss_fct(end_logits, end_positions)
-            total_loss = (start_loss + end_loss) / 2
+            if list_positive.size(0) > 0:
+                start_loss = loss_fct(start_logits[list_positive, :], start_positions[list_positive])
+                end_loss = loss_fct(end_logits[list_positive, :], end_positions[list_positive])
+                total_loss += (start_loss + end_loss) / 2
+
+            if list_negative.size(0) > 0:
+                total_loss += 1 / 2 * self.lambda_weight * (
+                        loss_fct(start_logits[list_negative, :], start_positions[list_negative]) +
+                        loss_fct(end_logits[list_negative, :], end_positions[list_negative])
+                )
+                total_loss += self.lambda_weight * (
+                    torch.sum(torch.clamp(
+                        torch.max(start_logits, dim=-1)[0] - 0.9, min=0
+                    ))
+                    +
+                    torch.sum(torch.clamp(
+                        torch.max(end_logits, dim=-1)[0] - 0.9, min=0
+                    ))
+                )
 
         if not return_dict:
             output = (start_logits, end_logits) + outputs[2:]
             return ((total_loss,) + output) if total_loss is not None else output
 
         return QuestionAnsweringModelOutput(
             loss=total_loss,
@@ -121,15 +141,15 @@
         self.tokenizer = tokenizer
         self.data_collator = DataCollatorCustom(tokenizer=self.tokenizer)
         self.compute_metrics = MRCEvaluator(tokenizer=self.tokenizer)
 
     @classmethod
     def from_pretrained(cls, model_name_or_path: str, **kwargs):
         device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-        model = MRCQuestionAnsweringModel.from_pretrained(model_name_or_path).to(device)
+        model = MRCQuestionAnsweringModel.from_pretrained(model_name_or_path, lambda_weight=kwargs.get(LAMBDA_WEIGHT, 0.6)).to(device)
         try:
             tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
         except:
             config_pretrained = load_json_data(os.path.join(model_name_or_path, 'config.json'))
             tokenizer = AutoTokenizer.from_pretrained(config_pretrained['_name_or_path'])
         return cls(model, tokenizer, device)
 
@@ -191,33 +211,35 @@
                 answer = " "
             score_start = torch.max(torch.softmax(start_logit, dim=-1)).cpu().detach().numpy().tolist()
             score_end = torch.max(torch.softmax(end_logit, dim=-1)).cpu().detach().numpy().tolist()
             results.append({
                 "answer": answer,
                 "score_start": score_start,
                 "score_end": score_end,
-                "score": score_end * score_start
+                "score": score_end * score_start,
+                'answer_start_idx': answer_start_idx,
+                'answer_end_idx': answer_end_idx
             })
         return sorted(results, key=lambda x: x['score'], reverse=True)
 
     def predict(self, queries: List[str], documents: List[List[Document]], **kwargs):
         logger.info(f'Number documents: {len(documents)}')
         assert len(queries) == len(documents), "Number question must equal number document"
-        results = []
+        results, results_raw = [], []
         for question, list_document in tqdm(zip(queries, documents), total=len(documents)):
-            results.append(
-                self.qa_inference(
-                    question=question,
-                    documents=[
-                        doc.document_context for doc in list_document
-                    ],
-                    **kwargs
-                )
+            tmp_pred, tmp_pred_raw = self.qa_inference(
+                question=question,
+                documents=[
+                    doc.document_context for doc in list_document
+                ],
+                **kwargs
             )
-        return results
+            results.append(tmp_pred)
+            results_raw.append(tmp_pred_raw)
+        return results, results_raw
 
     def qa_inference(self, question: str, documents: List[str], **kwargs):
         questions = [question] * len(documents)
         top_k_qa = kwargs.get(TOP_K_QA, 1)
         input_features_raw = make_input_feature_qa(
             questions=questions,
             documents=documents,
@@ -225,16 +247,16 @@
             max_length=368
         )
         input_features = self.data_collator(input_features_raw)
         for key, value in input_features.items():
             if isinstance(value, Tensor):
                 input_features[key] = value.to(self.device)
         outs = self.model(**input_features)
-        results = self.extract_answer(input_features_raw, outs)[:top_k_qa]
-        return results
+        results = self.extract_answer(input_features_raw, outs)
+        return results[:top_k_qa], results
 
     def train(self):
         wandb_api_key = os.getenv("WANDB_API")
         wandb.login(key=wandb_api_key)
         self.trainer.train()
         self.compute_metrics.log_predict = []  # refresh log
         self.evaluate(self.eval_dataset)
```

## e2eqavn/processor/bm25.py

```diff
@@ -115,7 +115,8 @@
 
     def get_top_k(self, query: Union[List[str], str], top_k: int):
         if isinstance(query, str):
             query = query.lower().split(" ")
         scores = self.get_scores(query)
         top_k_idxs = np.argsort(scores)[-top_k:]
         return {idx: scores[idx] for idx in top_k_idxs}
+
```

## e2eqavn/processor/qa_ext_processor.py

```diff
@@ -1,14 +1,17 @@
 import nltk
 from nltk import word_tokenize
 import logging
 import re
 import random
 from e2eqavn.documents import Corpus
 from e2eqavn.keywords import *
+from e2eqavn.processor import BM25Scoring
+from tqdm import tqdm
+import random
 
 logger = logging.getLogger(__name__)
 
 
 class QATextProcessor:
     def __init__(self, context_key: str = 'context',
                  question_key: str = 'question',
@@ -113,17 +116,22 @@
                 self.answer_word_start_idx_key: 0,
                 self.answer_word_end_idx_key: 0,
                 IS_VALID: False
             }
 
         return example
 
-    def make_example(self, corpus: Corpus):
+    def make_example(self, corpus: Corpus, **kwargs):
+        if kwargs.get(MAKE_NEGATIVE_MRC, False):
+            logger.info("Turn on mode make negative sample for mrc")
+            logger.info(f"Start sampling negative by BM25 with {kwargs.get(THRESHOLD_SAMPLING, 0.2) *100} % corpus")
+            bm25_scoring = BM25Scoring(corpus=[doc.document_context for doc in corpus.list_document])
+        list_documents = corpus.list_document
         examples = []
-        for document in corpus.list_document:
+        for index, document in tqdm(enumerate(corpus.list_document), total=len(corpus.list_document)):
             if len(document.list_pair_question_answers) == 0:
                 continue
             document_context = document.document_context
             for question_answer in document.list_pair_question_answers:
                 question = question_answer.question
                 list_dict_answer = question_answer.list_dict_answer
                 dict_answer = random.choice(list_dict_answer)
@@ -135,12 +143,25 @@
                         self.answer_key: answer,
                         self.answer_start_key: dict_answer.get(self.answer_start_key, None)
                     }
                 )
                 if not example[IS_VALID]:
                     continue
                 examples.append(example)
+                if kwargs.get(MAKE_NEGATIVE_MRC, False) and random.random() < kwargs.get(THRESHOLD_SAMPLING, 0.2):
+                    top_k_doc = bm25_scoring.get_top_k(question, top_k=10)
+                    for idx in top_k_doc:
+                        if idx != index:
+                            examples.append({
+                                self.context_key: list_documents[idx].document_context,
+                                self.question_key: question,
+                                self.answer_key: None,
+                                self.answer_word_start_idx_key: 0,
+                                self.answer_word_end_idx_key: 0,
+                                IS_VALID: True
+                            })
+                            break
 
         logger.info(f"*" * 50)
         logger.info(f"Total {self.cnt_failed} document failed")
         logger.info(f"*" * 50)
         return examples
```

## e2eqavn/utils/calculate.py

```diff
@@ -189,32 +189,40 @@
             IS_VALID: False
         }
     original_max_length = max_length
     context = example[CONTEXT]
     question = example[QUESTION]
     answer_start_idx = example[ANSWER_WORD_START_IDX]
     answer_end_idx = example[ANSWER_WORD_END_IDX]
+    answer_raw = example[ANSWER]
     context_ids = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word)) for word in context.split()]
     question_ids = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word)) for word in question.split()]
     arr_size_sub_word_context_ids = [len(sub_ids) for sub_ids in context_ids]
     arr_size_sub_word_question_ids = [len(sub_ids) for sub_ids in question_ids]
     is_valid = True
     if sum(arr_size_sub_word_question_ids) + sum(arr_size_sub_word_context_ids) > max_length - 5:
         if sum(arr_size_sub_word_question_ids) + sum(
                 arr_size_sub_word_context_ids[:answer_end_idx + 1]) > max_length - 5:
             is_valid = False
         else:
-            current_length = sum(arr_size_sub_word_question_ids) + sum(
-                arr_size_sub_word_context_ids[: answer_end_idx + 1]) + 3  # for 3 special token
-            tmp = answer_end_idx + 1
-            while current_length + arr_size_sub_word_context_ids[tmp] < max_length and tmp < len(
-                    arr_size_sub_word_context_ids) - 1:
-                current_length += arr_size_sub_word_context_ids[tmp]
-                tmp += 1
-            context_ids = context_ids[: tmp]
+            if answer_raw is not None:
+                current_length = sum(arr_size_sub_word_question_ids) + sum(
+                    arr_size_sub_word_context_ids[: answer_end_idx + 1]) + 3  # for 3 special token
+                tmp = answer_end_idx + 1
+                while current_length + arr_size_sub_word_context_ids[tmp] < max_length and tmp < len(
+                        arr_size_sub_word_context_ids) - 1:
+                    current_length += arr_size_sub_word_context_ids[tmp]
+                    tmp += 1
+                context_ids = context_ids[: tmp]
+            else:
+                i = len(context_ids)
+                while sum(arr_size_sub_word_question_ids) + sum(arr_size_sub_word_context_ids[:i]) > max_length - 3:
+                    i -= 1
+                context_ids = context_ids[: i]
+
     if tokenizer.bos_token_id is not None and tokenizer.eos_token_id is not None:
         bos_token_id = tokenizer.bos_token_id
         eos_token_id = tokenizer.eos_token_id
     else:
         temp_sample = tokenizer('xin chào')
         bos_token_id = temp_sample['input_ids'][0]
         eos_token_id = temp_sample['input_ids'][-1]
@@ -223,22 +231,24 @@
     context_final_ids = context_ids + [[eos_token_id]]
     input_ids = [id for sub_ids in question_final_ids + context_final_ids for id in sub_ids]
     words_length = [len(item) for item in question_final_ids + context_final_ids]
     if len(input_ids) > original_max_length:
         is_valid = False
 
     attention_mask = [1] * len(input_ids)
-
     return {
         INPUT_IDS: input_ids,
         ATTENTION_MASK: attention_mask,
-        START_IDX: (answer_start_idx + len(question_final_ids)) if len(example[ANSWER]) > 0 else 0,
-        END_IDX: (answer_end_idx + len(question_final_ids)) if len(example[ANSWER]) > 0 else 0,
+        START_IDX: (answer_start_idx + len(question_final_ids)) if answer_raw is not None and len(
+            example[ANSWER]) > 0 else 0,
+        END_IDX: (answer_end_idx + len(question_final_ids)) if answer_raw is not None and len(
+            example[ANSWER]) > 0 else 0,
         WORDS_LENGTH: words_length,
-        'is_valid': is_valid
+        'is_valid': is_valid,
+        'is_negative_sample': 1 if answer_raw is None else 0
     }
 
 
 def prepare_input_for_retrieval_evaluator(data: List[Dict], **kwargs):
     """
     :param data: List dictionary data
         Exammple:
```

## Comparing `e2eqavn-0.1.1.dist-info/METADATA` & `e2eqavn-0.1.3.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: e2eqavn
-Version: 0.1.1
+Version: 0.1.3
 Summary: e2eqavn is end to end pipeline for question answering
 Author: khanhdm
 Author-email: khanhc1k36@gmail.com
 Requires-Python: >3.6.0
 Requires-Dist: numpy
 Requires-Dist: PyYAML
 Requires-Dist: sentence-transformers
```

## Comparing `e2eqavn-0.1.1.dist-info/RECORD` & `e2eqavn-0.1.3.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,40 +1,40 @@
-e2eqavn/__init__.py,sha256=wYGyJdJs_4vfdWoUD7vU68ZgMzpqK0Q1mY2hCqp6o1g,498
-e2eqavn/cli.py,sha256=TiTdxs2fE5zL4iZdmhjmnX4OUa-UX4OdUQZCDpCYmBQ,9274
-e2eqavn/keywords.py,sha256=wCyDNupichCld5BLkiXUxKv5cVbAhfMnU54g5YLEh3g,2494
-e2eqavn/datasets/MRCDataset.py,sha256=q0TDZ0dOR3O63hzzD_b1mAkPSb7jpHpnh5ywuJIXOuA,2973
+e2eqavn/__init__.py,sha256=TDGvDwNpwuaOeGOVthQ0p16W2YtBjWQ4G9duKTkFTFU,498
+e2eqavn/cli.py,sha256=YV4JVKLr_kQLre7rbWUh9E76e9KWYif8m9PEu6Rgmus,10774
+e2eqavn/keywords.py,sha256=RUpkhbJ14i4XbIojmiSaXpjN9lI4bgPnpIY_ZnivTrc,2716
+e2eqavn/datasets/MRCDataset.py,sha256=EDQ656zUVz-PFZoBbA8D1WpyrMSdyTUzDHWbfheHdsk,3639
 e2eqavn/datasets/TripletDataset.py,sha256=UalsiPb-sVq79gE923C_Gu8mqih_KJ-NCAI2MEV-VCY,1386
 e2eqavn/datasets/__init__.py,sha256=PVd6KkEbmwFhGlSdv8lCnLcyIT6MrIOU2zeLEDnNBQM,124
-e2eqavn/datasets/data_collator.py,sha256=i1eE-WHBmLH2uI8CIv-WZpAjhzwbKTwTtgr9Ytw5FAA,3660
+e2eqavn/datasets/data_collator.py,sha256=9anyZh7fMzgr0R3tmqO63d-kBUE2UgnrBbU0jPuTEJc,3814
 e2eqavn/documents/__init__.py,sha256=KhJ-7DiMYhKRaNBXpMFooJ4EioOOXKxFqjTVB__1JG8,85
 e2eqavn/documents/corpus.py,sha256=mKMcqgpmH8kYylhWI-fXVQ8Zp9hl-_o3V7-BvIkGfN8,12681
 e2eqavn/documents/document_store.py,sha256=SI0eQivEdTGucoRkEzOiHVXcL4IeP-3R9I8JC7iUvk0,251
 e2eqavn/evaluate/__init__.py,sha256=FoaGcO_g8rveNGpk_6_n4RFw4P_dpRxmMGk0B3T1DMI,93
 e2eqavn/evaluate/bm25_evaluate_retrieval.py,sha256=eyvV7MFGQcd2f5BXph8kn5B3wqdL2kzXOyXWzNJOrYc,283
 e2eqavn/evaluate/information_retrieval_evaluator_custom.py,sha256=qStzIgsOGq8Tu3nt6W2lu6ajZiXxNWaq2EQOv_pnSoo,3408
 e2eqavn/evaluate/mrc_evaluator.py,sha256=dTn_8L2nDLR8frBxZWBuY_bdcDZ7aXHSy7nRvrnf1V0,1958
 e2eqavn/mrc/__init__.py,sha256=7wccJ-QQrMqjYIPIPmOMcQzWhjFgne7QbPZlxp5njPU,110
-e2eqavn/mrc/base.py,sha256=YiACw2yzdkVuV8wtrTblhuiCWYYTpKVQfC5PfjPhZ7w,797
-e2eqavn/mrc/mrc_model.py,sha256=Kte50wBO_tcy61UfItDfM9tmx19IIBAzVFLzG8VvvRU,10739
+e2eqavn/mrc/base.py,sha256=MOQRuPmEGCK_nnZYgFXEirPa5UHApspBV0e66Q8gWes,887
+e2eqavn/mrc/mrc_model.py,sha256=652sb2UDaItiGRP1uw-0ABY1EjiiViTMaR7bu6q07Gk,11913
 e2eqavn/pipeline/__init__.py,sha256=nHdcilMUCQJATXwseGf_ejz-wFKlGSMs0sM7HneaZjQ,61
 e2eqavn/pipeline/e2e_question_answering.py,sha256=vre0tEAI9klK9Musj8VRWf2DTrnQkpIa43ku-lx7GQo,2083
 e2eqavn/pipeline/pipeline.py,sha256=kRQmaA9GDAsU4zfB3TOY92jxVF5r3N-JWg2xqvCu1o4,2090
 e2eqavn/processor/__init__.py,sha256=7Y91pODl0Ba5-z0UHyw4uGkWO8M4r5S4vtzDx30Qlhs,145
-e2eqavn/processor/bm25.py,sha256=hkw_usvZ03MA37X5nlTvFpCkjwHzXBqRI_JKGvPlcrM,4462
+e2eqavn/processor/bm25.py,sha256=Lp2s4FTPHFHRMbVVY6B78ulip9yiORec4OFY32_DpJ8,4463
 e2eqavn/processor/chunk.py,sha256=JSpowb5eqngBsFPmJEr_X___cGMSVwbzkiznJYAjTUo,2248
-e2eqavn/processor/qa_ext_processor.py,sha256=DLANQ99nwjZS3QPZpsdywBNUTqc2XZSebSAvq1i9Vnk,5957
+e2eqavn/processor/qa_ext_processor.py,sha256=yN1OoCnFIt8EisRupSL9Z-w0T0-4VZREBDz9u_v4YkM,7252
 e2eqavn/processor/retrieval_sampling.py,sha256=RajiTjF8zxXbwTD_ig2XICQcSkToPIHQ7OFFpF1aplA,6506
 e2eqavn/retrieval/__init__.py,sha256=SiDAppt0_X9DRa9dhiCxmle7RpKQOW0dtYpA84y0W18,138
 e2eqavn/retrieval/base.py,sha256=XGkgyS5es4DFJyS5HwZKzfWBWaKekJowk6UvLuqMVhs,644
 e2eqavn/retrieval/bm25_retrieval.py,sha256=EyxojDjg8saKj-xEozm9PhrGyu4iV63P98a-UrDjS48,3322
 e2eqavn/retrieval/sbert_retrieval.py,sha256=KhdDq2CBF_E9nhWpQExyNtcjQOtxvOaZybj1ZAV7idQ,10419
 e2eqavn/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-e2eqavn/utils/calculate.py,sha256=yf5HPZyPodws38PNr2Zl3u7aqe_MTfQt1kAE0wHxuSk,12493
+e2eqavn/utils/calculate.py,sha256=LOaXVbVNIkhbH9UyKD004Evb_lbDY0SMnVSC7CnMgcI,12985
 e2eqavn/utils/io.py,sha256=StKqF8nRhuApqlI0vPlme8Y50Do4epId8KeW4oJ5uTI,603
 e2eqavn/utils/preprocess.py,sha256=1yhNSGZ5FRzIglW0IRiZyxS1b3Rr5CNzLN78lQsRQjQ,998
 test/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 test/test_chunking.py,sha256=tW3Cg_Ll8mtfYjmPmNy5MgElUSe8eIKhcWAND20SRDA,334
-e2eqavn-0.1.1.dist-info/METADATA,sha256=z-wd2vDaczoai5sqhV6V7CJp0K3EGrg729UqmfRLFVA,679
-e2eqavn-0.1.1.dist-info/WHEEL,sha256=bb2Ot9scclHKMOLDEHY6B2sicWOgugjFKaJsT7vwMQo,110
-e2eqavn-0.1.1.dist-info/entry_points.txt,sha256=pPUmMeBtyeitA4rbQwBo_glm5HRGPLREjN9eeAGu_Bk,52
-e2eqavn-0.1.1.dist-info/top_level.txt,sha256=Qu5Dlk8CtzRo5i3_O722D1zBatiVaNkiPAbSDCOA2pA,13
-e2eqavn-0.1.1.dist-info/RECORD,,
+e2eqavn-0.1.3.dist-info/METADATA,sha256=QHa-9Ewdx5tPaiiiH9DDs6acQ3XsHbPCCDocfY93IDU,679
+e2eqavn-0.1.3.dist-info/WHEEL,sha256=bb2Ot9scclHKMOLDEHY6B2sicWOgugjFKaJsT7vwMQo,110
+e2eqavn-0.1.3.dist-info/entry_points.txt,sha256=pPUmMeBtyeitA4rbQwBo_glm5HRGPLREjN9eeAGu_Bk,52
+e2eqavn-0.1.3.dist-info/top_level.txt,sha256=Qu5Dlk8CtzRo5i3_O722D1zBatiVaNkiPAbSDCOA2pA,13
+e2eqavn-0.1.3.dist-info/RECORD,,
```

