# Comparing `tmp/qworker-1.8.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip` & `tmp/qworker-1.9.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,37 +1,37 @@
-Zip file size: 319192 bytes, number of entries: 35
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-29 23:41 qworker.libs/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-29 23:41 qworker-1.8.8.dist-info/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-29 23:41 qw/
--rw-r--r--  2.0 unx     3170 b- defN 23-Jun-29 23:41 qworker-1.8.8.dist-info/METADATA
--rw-r--r--  2.0 unx       40 b- defN 23-Jun-29 23:41 qworker-1.8.8.dist-info/entry_points.txt
--rw-r--r--  2.0 unx     1070 b- defN 23-Jun-29 23:41 qworker-1.8.8.dist-info/LICENSE
--rw-r--r--  2.0 unx        3 b- defN 23-Jun-29 23:41 qworker-1.8.8.dist-info/top_level.txt
--rw-r--r--  2.0 unx      217 b- defN 23-Jun-29 23:41 qworker-1.8.8.dist-info/WHEEL
--rw-rw-r--  2.0 unx     2189 b- defN 23-Jun-29 23:41 qworker-1.8.8.dist-info/RECORD
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-29 23:41 qw/executor/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-29 23:41 qw/utils/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-29 23:41 qw/queues/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-29 23:41 qw/wrappers/
--rw-r--r--  2.0 unx     2946 b- defN 23-Jun-29 23:41 qw/conf.py
--rw-r--r--  2.0 unx      622 b- defN 23-Jun-29 23:41 qw/version.py
--rw-r--r--  2.0 unx      137 b- defN 23-Jun-29 23:41 qw/__init__.py
--rw-r--r--  2.0 unx     4129 b- defN 23-Jun-29 23:41 qw/protocols.py
--rw-r--r--  2.0 unx    16261 b- defN 23-Jun-29 23:41 qw/client.py
--rw-r--r--  2.0 unx     2160 b- defN 23-Jun-29 23:41 qw/discovery.py
--rw-r--r--  2.0 unx     8058 b- defN 23-Jun-29 23:41 qw/process.py
--rw-r--r--  2.0 unx    19258 b- defN 23-Jun-29 23:41 qw/server.py
--rw-r--r--  2.0 unx      380 b- defN 23-Jun-29 23:41 qw/decorators.py
--rwxr-xr-x  2.0 unx   568544 b- defN 23-Jun-29 23:41 qw/exceptions.cpython-39-x86_64-linux-gnu.so
--rw-r--r--  2.0 unx     2472 b- defN 23-Jun-29 23:41 qw/__main__.py
--rw-r--r--  2.0 unx     4265 b- defN 23-Jun-29 23:41 qw/executor/__init__.py
--rw-r--r--  2.0 unx       46 b- defN 23-Jun-29 23:41 qw/utils/__init__.py
--rw-r--r--  2.0 unx      512 b- defN 23-Jun-29 23:41 qw/utils/functions.py
--rwxr-xr-x  2.0 unx   434800 b- defN 23-Jun-29 23:41 qw/utils/json.cpython-39-x86_64-linux-gnu.so
--rw-r--r--  2.0 unx      597 b- defN 23-Jun-29 23:41 qw/utils/versions.py
--rw-r--r--  2.0 unx       62 b- defN 23-Jun-29 23:41 qw/queues/__init__.py
--rw-r--r--  2.0 unx     6008 b- defN 23-Jun-29 23:41 qw/queues/manager.py
--rw-r--r--  2.0 unx     1246 b- defN 23-Jun-29 23:41 qw/wrappers/func.py
--rw-r--r--  2.0 unx     4658 b- defN 23-Jun-29 23:41 qw/wrappers/di_task.py
--rw-r--r--  2.0 unx      320 b- defN 23-Jun-29 23:41 qw/wrappers/__init__.py
--rw-r--r--  2.0 unx     1483 b- defN 23-Jun-29 23:41 qw/wrappers/base.py
-35 files, 1085653 bytes uncompressed, 315086 bytes compressed:  71.0%
+Zip file size: 319998 bytes, number of entries: 35
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-01 02:08 qworker.libs/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-01 02:08 qworker-1.9.0.dist-info/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-01 02:08 qw/
+-rw-r--r--  2.0 unx     3170 b- defN 23-Jul-01 02:08 qworker-1.9.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       40 b- defN 23-Jul-01 02:08 qworker-1.9.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx     1070 b- defN 23-Jul-01 02:08 qworker-1.9.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx        3 b- defN 23-Jul-01 02:08 qworker-1.9.0.dist-info/top_level.txt
+-rw-r--r--  2.0 unx      217 b- defN 23-Jul-01 02:08 qworker-1.9.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx     2189 b- defN 23-Jul-01 02:08 qworker-1.9.0.dist-info/RECORD
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-01 02:08 qw/executor/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-01 02:08 qw/utils/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-01 02:08 qw/queues/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-01 02:08 qw/wrappers/
+-rw-r--r--  2.0 unx     3025 b- defN 23-Jul-01 02:08 qw/conf.py
+-rw-r--r--  2.0 unx      622 b- defN 23-Jul-01 02:08 qw/version.py
+-rw-r--r--  2.0 unx      137 b- defN 23-Jul-01 02:08 qw/__init__.py
+-rw-r--r--  2.0 unx     4129 b- defN 23-Jul-01 02:08 qw/protocols.py
+-rw-r--r--  2.0 unx    18156 b- defN 23-Jul-01 02:08 qw/client.py
+-rw-r--r--  2.0 unx     2160 b- defN 23-Jul-01 02:08 qw/discovery.py
+-rw-r--r--  2.0 unx     8058 b- defN 23-Jul-01 02:08 qw/process.py
+-rw-r--r--  2.0 unx    22119 b- defN 23-Jul-01 02:08 qw/server.py
+-rw-r--r--  2.0 unx      380 b- defN 23-Jul-01 02:08 qw/decorators.py
+-rwxr-xr-x  2.0 unx   568544 b- defN 23-Jul-01 02:08 qw/exceptions.cpython-39-x86_64-linux-gnu.so
+-rw-r--r--  2.0 unx     2472 b- defN 23-Jul-01 02:08 qw/__main__.py
+-rw-r--r--  2.0 unx     4265 b- defN 23-Jul-01 02:08 qw/executor/__init__.py
+-rw-r--r--  2.0 unx       46 b- defN 23-Jul-01 02:08 qw/utils/__init__.py
+-rw-r--r--  2.0 unx      512 b- defN 23-Jul-01 02:08 qw/utils/functions.py
+-rwxr-xr-x  2.0 unx   434800 b- defN 23-Jul-01 02:08 qw/utils/json.cpython-39-x86_64-linux-gnu.so
+-rw-r--r--  2.0 unx      597 b- defN 23-Jul-01 02:08 qw/utils/versions.py
+-rw-r--r--  2.0 unx       62 b- defN 23-Jul-01 02:08 qw/queues/__init__.py
+-rw-r--r--  2.0 unx     6008 b- defN 23-Jul-01 02:08 qw/queues/manager.py
+-rw-r--r--  2.0 unx     1246 b- defN 23-Jul-01 02:08 qw/wrappers/func.py
+-rw-r--r--  2.0 unx     4658 b- defN 23-Jul-01 02:08 qw/wrappers/di_task.py
+-rw-r--r--  2.0 unx      320 b- defN 23-Jul-01 02:08 qw/wrappers/__init__.py
+-rw-r--r--  2.0 unx     1483 b- defN 23-Jul-01 02:08 qw/wrappers/base.py
+35 files, 1090488 bytes uncompressed, 315892 bytes compressed:  71.0%
```

## zipnote {}

```diff
@@ -1,32 +1,32 @@
 Filename: qworker.libs/
 Comment: 
 
-Filename: qworker-1.8.8.dist-info/
+Filename: qworker-1.9.0.dist-info/
 Comment: 
 
 Filename: qw/
 Comment: 
 
-Filename: qworker-1.8.8.dist-info/METADATA
+Filename: qworker-1.9.0.dist-info/METADATA
 Comment: 
 
-Filename: qworker-1.8.8.dist-info/entry_points.txt
+Filename: qworker-1.9.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: qworker-1.8.8.dist-info/LICENSE
+Filename: qworker-1.9.0.dist-info/LICENSE
 Comment: 
 
-Filename: qworker-1.8.8.dist-info/top_level.txt
+Filename: qworker-1.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: qworker-1.8.8.dist-info/WHEEL
+Filename: qworker-1.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: qworker-1.8.8.dist-info/RECORD
+Filename: qworker-1.9.0.dist-info/RECORD
 Comment: 
 
 Filename: qw/executor/
 Comment: 
 
 Filename: qw/utils/
 Comment:
```

## qw/conf.py

```diff
@@ -42,15 +42,16 @@
 WORKER_SECRET_KEY = config.get('WORKER_SECRET_KEY')
 
 
 ### Redis Transport
 REDIS_HOST = config.get('REDIS_HOST', fallback='localhost')
 REDIS_PORT = config.getint('REDIS_PORT', fallback=6379)
 REDIS_WORKER_DB = config.getint('REDIS_WORKER_DB', fallback=4)
-REDIS_WORKER_CHANNEL = config.get('REDIS_WORKER_CHANNEL', fallback='WorkerChannel')
+REDIS_WORKER_GROUP = config.get('REDIS_WORKER_CHANNEL', fallback='QWorkerGroup')
+REDIS_WORKER_STREAM = config.get('REDIS_WORKER_STREAM', fallback='QWorkerStream')
 
 WORKER_REDIS = f"redis://{REDIS_HOST}:{REDIS_PORT}/{REDIS_WORKER_DB}"
 
 WORKERS = [e.strip() for e in list(config.get(
     'WORKER_LIST', fallback='127.0.0.1:8181').split(","))]
 WORKER_LIST = get_worker_list(WORKERS)
```

## qw/version.py

```diff
@@ -2,15 +2,15 @@
    QueueWorker is a asyncio-based Worker for distributed functions.
 """
 
 __title__ = 'qworker'
 __description__ = ('QueueWorker is asynchronous Task Queue implementation '
                    'built on top of Asyncio.'
                    'Can you spawn distributed workers to run functions inside workers.')
-__version__ = '1.8.8'
+__version__ = '1.9.0'
 __author__ = 'Jesus Lara'
 __author_email__ = 'jesuslarag@gmail.com'
 __license__ = 'MIT'
 
 def get_version() -> tuple:  # pragma: no cover
     """ Get Queue Worker version as tuple.
     """
```

## qw/client.py

```diff
@@ -1,18 +1,20 @@
 """QueueWorker Client."""
 import asyncio
 import itertools
 import random
+import uuid
 import warnings
-import inspect
+import socket
+import base64
 from typing import Any, Union
 from collections.abc import Callable, Awaitable
 from collections import defaultdict
 from functools import partial
-import aioredis
+from redis import asyncio as aioredis
 import pickle
 import cloudpickle
 import jsonpickle
 import orjson
 import uvloop
 from navconfig.logging import logging
 from qw.discovery import get_client_discovery
@@ -22,14 +24,15 @@
     QWException,
     DiscardedTask
 )
 from .conf import (
     WORKER_DEFAULT_HOST,
     WORKER_DEFAULT_PORT,
     WORKER_REDIS,
+    REDIS_WORKER_STREAM,
     USE_DISCOVERY,
     WORKER_SECRET_KEY,
     expected_message
 )
 from .process import QW_WORKER_LIST
 from .wrappers import FuncWrapper, TaskWrapper
 
@@ -474,7 +477,65 @@
                 "message": received
             }
             return serialized_result
         except Exception as err:  # pylint: disable=W0703
             self.logger.exception(
                 f'Error Serializing Task: {err!s}'
             )
+
+    async def publish(self, fn: Any, *args, use_wrapper: bool = True, **kwargs):
+        """Publish a function into a Pub/Sub Channel.
+
+        Send & Forget functionality to send a task to Queue Worker using Pub/Sub.
+
+        Args:
+            fn: Any Function, object or callable to be send to Worker.
+            args: any non-keyword arguments
+            kwargs: keyword arguments.
+
+        Returns:
+            None.
+
+        Raises:
+            ConfigError: bad instructions to Worker Client.
+            ConnectionError: unable to connect to Worker.
+            Exception: Any Unhandled error.
+        """
+        self.logger.debug(
+            f'Sending function {fn!s} to Pub/Sub Channel {REDIS_WORKER_STREAM}'
+        )
+        host = socket.gethostbyname(socket.gethostname())
+        # serializing
+        func = self.get_wrapped_function(
+            fn,
+            host,
+            *args,
+            use_wrapper=use_wrapper,
+            queued=True,
+            **kwargs
+        )
+        if use_wrapper is True:
+            uid = func.id
+        else:
+            uid = uuid.uuid1(
+                node=random.getrandbits(48) | 0x010000000000
+            )
+        serialized_task = cloudpickle.dumps(func)
+        encoded_task = base64.b64encode(serialized_task).decode('utf-8')
+        conn = aioredis.from_url(
+            WORKER_REDIS,
+            decode_responses=True,
+            encoding='utf-8'
+        )
+        message = {
+            "uid": str(uid),
+            "task": encoded_task
+        }
+        # check if published
+        # Add the data to the stream
+        result = await conn.xadd(REDIS_WORKER_STREAM, message)
+        serialized_result = {
+            "status": "Queued",
+            "task": f"{func!r}",
+            "message": result
+        }
+        return serialized_result
```

## qw/server.py

```diff
@@ -1,12 +1,13 @@
 """QueueWorker Server Implementation"""
 import os
 import time
 import socket
 import uuid
+import base64
 import asyncio
 import inspect
 import random
 from typing import Any
 from collections.abc import Callable
 import multiprocessing as mp
 import cloudpickle
@@ -14,20 +15,22 @@
 from qw.exceptions import (
     QWException,
     ParserError,
     DiscardedTask
 )
 from qw.utils import make_signature
 from redis import asyncio as aioredis
+from redis.exceptions import ResponseError
 from .conf import (
     WORKER_DEFAULT_HOST,
     WORKER_DEFAULT_PORT,
     expected_message,
     WORKER_SECRET_KEY,
-    REDIS_WORKER_CHANNEL,
+    REDIS_WORKER_STREAM,
+    REDIS_WORKER_GROUP,
     WORKER_REDIS
 )
 from .utils.json import json_encoder
 from .utils.versions import get_versions
 from .utils import cPrint
 from .queues import QueueManager
 from .wrappers import (
@@ -87,53 +90,111 @@
             WORKER_REDIS,
             encoding='utf8',
             decode_responses=True,
             max_connections=5000
         )
         self.redis = aioredis.Redis(connection_pool=self.pool)
 
-    async def start_subscription(self):
-        """Starts PUB/SUB system based on Redis."""
+    async def ensure_group_exists(self):
+        try:
+            # Try to create the group. This will fail if the group already exists.
+            await self.redis.xgroup_create(
+                REDIS_WORKER_STREAM, REDIS_WORKER_GROUP, id='$', mkstream=True
+            )
+        except ResponseError as e:
+            if "BUSYGROUP Consumer Group name already exists" not in str(e):
+                raise
+        except Exception as exc:
+            self.logger.exception(exc, stack_info=True)
+            raise
         try:
-            self.pubsub = self.redis.pubsub()
-            await self.pubsub.subscribe(REDIS_WORKER_CHANNEL)
+            # create the consumer:
+            await self.redis.xgroup_createconsumer(
+                REDIS_WORKER_STREAM, REDIS_WORKER_GROUP, self._name
+            )
+            self.logger.debug(
+                f":: Creating Consumer {self._name} on Stream {REDIS_WORKER_STREAM}"
+            )
+        except Exception as exc:
+            print(exc)
+            self.logger.exception(exc, stack_info=True)
+            raise
 
+    async def start_subscription(self):
+        """Starts stream consumer group based on Redis."""
+        try:
+            await self.ensure_group_exists()
+            info = await self.redis.xinfo_groups(REDIS_WORKER_STREAM)
+            self.logger.debug(f'Groups Info: {info}')
             while self._running:
                 try:
-                    msg = await self.pubsub.get_message()
-                    if msg and msg['type'] == 'message':
-                        self.logger.debug(f'Received Task: {msg}')
+                    message_groups = await self.redis.xreadgroup(
+                        REDIS_WORKER_GROUP,
+                        self._name,
+                        streams={REDIS_WORKER_STREAM: '>'},
+                        block=100,
+                        count=1
+                    )
+                    for _, messages in message_groups:
+                        for _id, fn in messages:
+                            try:
+                                encoded_task = fn['task']
+                                task_id = fn['uid']
+                                # Process the task
+                                serialized_task = base64.b64decode(encoded_task)
+                                task = cloudpickle.loads(serialized_task)
+                                self.logger.info(
+                                    f'TASK RECEIVED: {task} with id {task_id} at {int(time.time())}'
+                                )
+                                try:
+                                    executor = TaskExecutor(task)
+                                    await executor.run()
+                                    self.logger.info(
+                                        f":: TASK {task}.{task_id} Executed at {int(time.time())}"
+                                    )
+                                except Exception as e:
+                                    self.logger.error(
+                                        f"Task {task}:{task_id} failed with error {e}"
+                                    )
+                                # If processing raises an exception, the next line won't be executed
+                                await self.redis.xack(
+                                    REDIS_WORKER_STREAM,
+                                    REDIS_WORKER_GROUP,
+                                    _id
+                                )
+                            except Exception as e:
+                                self.logger.error(f"Error processing message: {e}")
                     await asyncio.sleep(0.001)  # sleep a bit to prevent high CPU usage
                 except ConnectionResetError:
                     self.logger.error(
                         "Connection was closed, trying to reconnect."
                     )
                     await asyncio.sleep(1)  # Wait for a bit before trying to reconnect
                     await self.start_subscription()  # Try to restart the subscription
                 except asyncio.CancelledError:
-                    await self.pubsub.unsubscribe(REDIS_WORKER_CHANNEL)
                     break
                 except KeyboardInterrupt:
                     break
                 except Exception as exc:
                     # Handle other exceptions as necessary
                     self.logger.error(
-                        f"Error in start_subscription: {exc}"
+                        f"Error Getting Message: {exc}"
                     )
-                    break
         except Exception as exc:
             self.logger.error(
                 f"Could not establish initial connection: {exc}"
             )
 
     async def close_redis(self):
         try:
-            # Get a new pubsub object and unsubscribe from 'channel'
             try:
-                await self.pubsub.unsubscribe(REDIS_WORKER_CHANNEL)
+                # create the consumer:
+                await self.redis.xgroup_delconsumer(
+                    REDIS_WORKER_STREAM, REDIS_WORKER_GROUP, self._name
+                )
                 await asyncio.wait_for(self.redis.close(), timeout=2.0)
             except asyncio.TimeoutError:
                 self.logger.error(
                     "Redis took too long to close."
                 )
             await self.pool.disconnect(
                 inuse_connections=True
@@ -479,15 +540,14 @@
                 await self.queue.put(task, id=task_uuid)
                 result = f'Task {task!s} was Queued.'.encode('utf-8')
             except asyncio.QueueFull:
                 return await self.discard_task(
                     message=f'Task {task!s} was discarded, queue full',
                     writer=writer
                 )
-        print('RESULT > ', result)
         if result is None:
             # Not always a Task returns Value, sometimes returns None.
             result = [
                 {
                     "task": task,
                     "uuid": task_uuid,
                     "worker": self.name
@@ -569,10 +629,12 @@
         if loop and worker:
             worker.logger.info(
                 f'Shutting down Worker {worker.name if worker else "unknown"}'
             )
             loop.run_until_complete(
                 worker.shutdown()
             )
+    except Exception:
+        pass
     finally:
         if loop:
             loop.close()  # Close the event loop
```

## Comparing `qworker-1.8.8.dist-info/METADATA` & `qworker-1.9.0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: qworker
-Version: 1.8.8
+Version: 1.9.0
 Summary: QueueWorker is asynchronous Task Queue implementation built on top of Asyncio.Can you spawn distributed workers to run functions inside workers.
 Home-page: https://github.com/phenobarbital/qworker
 Author: Jesus Lara
 Author-email: jesuslara@phenobarbital.info
 License: MIT
 Project-URL: Source, https://github.com/phenobarbital/qworker
 Project-URL: Funding, https://paypal.me/phenobarbital
```

## Comparing `qworker-1.8.8.dist-info/LICENSE` & `qworker-1.9.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `qworker-1.8.8.dist-info/RECORD` & `qworker-1.9.0.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-qworker-1.8.8.dist-info/METADATA,sha256=7MhusvmcusYgaNrYdRvMQG3PeDtMuC66RjonMcFpRQ0,3170
-qworker-1.8.8.dist-info/entry_points.txt,sha256=ooHTYYyEjHI9Rj79mHQmU_s4HP-PTvD3g3xSa26vIGc,40
-qworker-1.8.8.dist-info/LICENSE,sha256=EW8vB8vWRFvBxGC3soG2HCxkuNrQpOGUgNFd81h7u54,1070
-qworker-1.8.8.dist-info/top_level.txt,sha256=2NxbFCeI_G1kzzf628VnbDchanCX6rcTJORdENY-rHI,3
-qworker-1.8.8.dist-info/WHEEL,sha256=G1-Tt_WO10x9oXjmzv4wXxMLbyVnQaK1ig1QydJbPuA,217
-qworker-1.8.8.dist-info/RECORD,,
-qw/conf.py,sha256=K7JRS16jsVinw0cJM4MbTUtCWNg2a5tHTJMnnjpwSkw,2946
-qw/version.py,sha256=1nYRvnaS5sV75ByMOt8G_oBfZMkNDc8vwy_oyyQ5Ngc,622
+qworker-1.9.0.dist-info/METADATA,sha256=v394oZunkjRUIaobuadJ4nZ8mayt0BmQGbuI5vci_Q8,3170
+qworker-1.9.0.dist-info/entry_points.txt,sha256=ooHTYYyEjHI9Rj79mHQmU_s4HP-PTvD3g3xSa26vIGc,40
+qworker-1.9.0.dist-info/LICENSE,sha256=EW8vB8vWRFvBxGC3soG2HCxkuNrQpOGUgNFd81h7u54,1070
+qworker-1.9.0.dist-info/top_level.txt,sha256=2NxbFCeI_G1kzzf628VnbDchanCX6rcTJORdENY-rHI,3
+qworker-1.9.0.dist-info/WHEEL,sha256=G1-Tt_WO10x9oXjmzv4wXxMLbyVnQaK1ig1QydJbPuA,217
+qworker-1.9.0.dist-info/RECORD,,
+qw/conf.py,sha256=Fxx8-LNyeuN4PfQEiWlcAYzyqJXGNw8fsif2L_Qc2hE,3025
+qw/version.py,sha256=ULnBV6-p9unqXZecahhR9M43cBhH_iUpW6bru53jtyw,622
 qw/__init__.py,sha256=awMNjg7WGznbrNuJLoEJV6BbnREk4BPD8k9My8BD7Uo,137
 qw/protocols.py,sha256=I57MsY72OlVlGFT1-ggZCm4nWAd89m4m5YEwGUWGAmg,4129
-qw/client.py,sha256=-JRd9E6v0bUeUuLH-2a2WLfLujkQW6AWnEEbYoTZWPc,16261
+qw/client.py,sha256=-JG_lIY02VU7zIVDHaCt4DybQvi7BBXT8dJ655Zci1k,18156
 qw/discovery.py,sha256=l_Lb3Bmni6WTTu5fxzj4-9KquiRak1rq8k9I70f_hSI,2160
 qw/process.py,sha256=TnA7JKqMmWaLQCKQEFY3A8Q1poC0cMtpsHuB412OJgw,8058
-qw/server.py,sha256=dcvINYJZM6eyfWlxpQNhSI2iFMXZEWx7quodtdBfsg0,19258
+qw/server.py,sha256=Q2ZfvMMuVeu9QuQQ2jKi459Qq0baC_Y9UoECMHzgqoU,22119
 qw/decorators.py,sha256=lL5CN9a7gUi8iDEfOI7kSpABkScu5cE99yj9eYGn69w,380
 qw/exceptions.cpython-39-x86_64-linux-gnu.so,sha256=IYBQ4BGCKA3duo8PEQylQhSS4BTYgAJ_rNjMOWU2nf0,568544
 qw/__main__.py,sha256=H52Hv_ll_8Nwnbj4UAhGfe3bfCbAhYAdH6f5Y3kbpFo,2472
 qw/executor/__init__.py,sha256=3ggAsCVXZnimnhOcs5aLUHJZRV-A7vqNd46ScvmM-mY,4265
 qw/utils/__init__.py,sha256=bYf_I4ymTf8vsqMjK00NJi2-A-lVijPTwN6HDOVjcjQ,46
 qw/utils/functions.py,sha256=9iXVvYLtQzOK0tRecOV2Oqrl-2raxAHwBCNilLui1xQ,512
 qw/utils/json.cpython-39-x86_64-linux-gnu.so,sha256=jHsLujozPguK1eL9t-Grw2KYTVlZvo4k7rLtF8a6YuU,434800
```

