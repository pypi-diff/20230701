# Comparing `tmp/tensordict_nightly-2023.6.8-py39-none-any.whl.zip` & `tmp/tensordict_nightly-2023.7.1-cp39-cp39-macosx_11_0_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,29 +1,30 @@
-Zip file size: 133833 bytes, number of entries: 27
--rw-r--r--  2.0 unx     1088 b- defN 23-Jun-08 11:18 tensordict/__init__.py
--rw-r--r--  2.0 unx     6000 b- defN 23-Jun-08 11:18 tensordict/_contextlib.py
--rw-r--r--  2.0 unx    30428 b- defN 23-Jun-08 11:18 tensordict/memmap.py
--rw-r--r--  2.0 unx    33829 b- defN 23-Jun-08 11:18 tensordict/persistent.py
--rw-r--r--  2.0 unx    30420 b- defN 23-Jun-08 11:18 tensordict/tensorclass.py
--rw-r--r--  2.0 unx   275863 b- defN 23-Jun-08 11:18 tensordict/tensordict.py
--rw-r--r--  2.0 unx    28907 b- defN 23-Jun-08 11:18 tensordict/utils.py
--rw-r--r--  2.0 unx       84 b- defN 23-Jun-08 11:19 tensordict/version.py
--rw-r--r--  2.0 unx     1314 b- defN 23-Jun-08 11:18 tensordict/nn/__init__.py
--rw-r--r--  2.0 unx    39699 b- defN 23-Jun-08 11:18 tensordict/nn/common.py
--rw-r--r--  2.0 unx    19059 b- defN 23-Jun-08 11:18 tensordict/nn/functional_modules.py
--rw-r--r--  2.0 unx    22301 b- defN 23-Jun-08 11:18 tensordict/nn/probabilistic.py
--rw-r--r--  2.0 unx    19564 b- defN 23-Jun-08 11:18 tensordict/nn/sequence.py
--rw-r--r--  2.0 unx    10621 b- defN 23-Jun-08 11:18 tensordict/nn/utils.py
--rw-r--r--  2.0 unx      499 b- defN 23-Jun-08 11:18 tensordict/nn/distributions/__init__.py
--rw-r--r--  2.0 unx     7073 b- defN 23-Jun-08 11:18 tensordict/nn/distributions/continuous.py
--rw-r--r--  2.0 unx     2580 b- defN 23-Jun-08 11:18 tensordict/nn/distributions/discrete.py
--rw-r--r--  2.0 unx     6504 b- defN 23-Jun-08 11:18 tensordict/nn/distributions/truncated_normal.py
--rw-r--r--  2.0 unx     1226 b- defN 23-Jun-08 11:18 tensordict/nn/distributions/utils.py
--rw-r--r--  2.0 unx      381 b- defN 23-Jun-08 11:18 tensordict/prototype/__init__.py
--rw-r--r--  2.0 unx     7507 b- defN 23-Jun-08 11:18 tensordict/prototype/fx.py
--rw-r--r--  2.0 unx      739 b- defN 23-Jun-08 11:18 tensordict/prototype/tensorclass.py
--rw-r--r--  2.0 unx     1098 b- defN 23-Jun-08 11:19 tensordict_nightly-2023.6.8.dist-info/LICENSE
--rw-r--r--  2.0 unx    15366 b- defN 23-Jun-08 11:19 tensordict_nightly-2023.6.8.dist-info/METADATA
--rw-r--r--  2.0 unx       93 b- defN 23-Jun-08 11:19 tensordict_nightly-2023.6.8.dist-info/WHEEL
--rw-r--r--  2.0 unx       11 b- defN 23-Jun-08 11:19 tensordict_nightly-2023.6.8.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2352 b- defN 23-Jun-08 11:19 tensordict_nightly-2023.6.8.dist-info/RECORD
-27 files, 564606 bytes uncompressed, 130031 bytes compressed:  77.0%
+Zip file size: 198542 bytes, number of entries: 28
+-rw-r--r--  2.0 unx     1094 b- defN 23-Jul-01 13:49 tensordict/__init__.py
+-rw-r--r--  2.0 unx     6000 b- defN 23-Jul-01 13:49 tensordict/_contextlib.py
+-rwxr-xr-x  2.0 unx   180648 b- defN 23-Jul-01 13:50 tensordict/_tensordict.so
+-rw-r--r--  2.0 unx    30443 b- defN 23-Jul-01 13:49 tensordict/memmap.py
+-rw-r--r--  2.0 unx    34579 b- defN 23-Jul-01 13:49 tensordict/persistent.py
+-rw-r--r--  2.0 unx    32306 b- defN 23-Jul-01 13:49 tensordict/tensorclass.py
+-rw-r--r--  2.0 unx   306446 b- defN 23-Jul-01 13:49 tensordict/tensordict.py
+-rw-r--r--  2.0 unx    39422 b- defN 23-Jul-01 13:49 tensordict/utils.py
+-rw-r--r--  2.0 unx       84 b- defN 23-Jul-01 13:50 tensordict/version.py
+-rw-r--r--  2.0 unx     1314 b- defN 23-Jul-01 13:49 tensordict/nn/__init__.py
+-rw-r--r--  2.0 unx    48896 b- defN 23-Jul-01 13:49 tensordict/nn/common.py
+-rw-r--r--  2.0 unx    24262 b- defN 23-Jul-01 13:49 tensordict/nn/functional_modules.py
+-rw-r--r--  2.0 unx    22301 b- defN 23-Jul-01 13:49 tensordict/nn/probabilistic.py
+-rw-r--r--  2.0 unx    19564 b- defN 23-Jul-01 13:49 tensordict/nn/sequence.py
+-rw-r--r--  2.0 unx    10621 b- defN 23-Jul-01 13:49 tensordict/nn/utils.py
+-rw-r--r--  2.0 unx      499 b- defN 23-Jul-01 13:49 tensordict/nn/distributions/__init__.py
+-rw-r--r--  2.0 unx     7073 b- defN 23-Jul-01 13:49 tensordict/nn/distributions/continuous.py
+-rw-r--r--  2.0 unx     2580 b- defN 23-Jul-01 13:49 tensordict/nn/distributions/discrete.py
+-rw-r--r--  2.0 unx     6504 b- defN 23-Jul-01 13:49 tensordict/nn/distributions/truncated_normal.py
+-rw-r--r--  2.0 unx     1226 b- defN 23-Jul-01 13:49 tensordict/nn/distributions/utils.py
+-rw-r--r--  2.0 unx      381 b- defN 23-Jul-01 13:49 tensordict/prototype/__init__.py
+-rw-r--r--  2.0 unx     7507 b- defN 23-Jul-01 13:49 tensordict/prototype/fx.py
+-rw-r--r--  2.0 unx      739 b- defN 23-Jul-01 13:49 tensordict/prototype/tensorclass.py
+-rw-r--r--  2.0 unx     1098 b- defN 23-Jul-01 13:50 tensordict_nightly-2023.7.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx    15403 b- defN 23-Jul-01 13:50 tensordict_nightly-2023.7.1.dist-info/METADATA
+-rw-r--r--  2.0 unx      109 b- defN 23-Jul-01 13:50 tensordict_nightly-2023.7.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       11 b- defN 23-Jul-01 13:50 tensordict_nightly-2023.7.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2437 b- defN 23-Jul-01 13:50 tensordict_nightly-2023.7.1.dist-info/RECORD
+28 files, 803547 bytes uncompressed, 194614 bytes compressed:  75.8%
```

## zipnote {}

```diff
@@ -1,13 +1,16 @@
 Filename: tensordict/__init__.py
 Comment: 
 
 Filename: tensordict/_contextlib.py
 Comment: 
 
+Filename: tensordict/_tensordict.so
+Comment: 
+
 Filename: tensordict/memmap.py
 Comment: 
 
 Filename: tensordict/persistent.py
 Comment: 
 
 Filename: tensordict/tensorclass.py
@@ -60,23 +63,23 @@
 
 Filename: tensordict/prototype/fx.py
 Comment: 
 
 Filename: tensordict/prototype/tensorclass.py
 Comment: 
 
-Filename: tensordict_nightly-2023.6.8.dist-info/LICENSE
+Filename: tensordict_nightly-2023.7.1.dist-info/LICENSE
 Comment: 
 
-Filename: tensordict_nightly-2023.6.8.dist-info/METADATA
+Filename: tensordict_nightly-2023.7.1.dist-info/METADATA
 Comment: 
 
-Filename: tensordict_nightly-2023.6.8.dist-info/WHEEL
+Filename: tensordict_nightly-2023.7.1.dist-info/WHEEL
 Comment: 
 
-Filename: tensordict_nightly-2023.6.8.dist-info/top_level.txt
+Filename: tensordict_nightly-2023.7.1.dist-info/top_level.txt
 Comment: 
 
-Filename: tensordict_nightly-2023.6.8.dist-info/RECORD
+Filename: tensordict_nightly-2023.7.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tensordict/__init__.py

```diff
@@ -1,28 +1,29 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
 from tensordict.memmap import MemmapTensor, set_transfer_ownership
 from tensordict.persistent import PersistentTensorDict
-from tensordict.tensorclass import is_tensorclass, tensorclass
+from tensordict.tensorclass import tensorclass
 from tensordict.tensordict import (
     is_batchedtensor,
     is_memmap,
     is_tensor_collection,
     LazyStackedTensorDict,
     make_tensordict,
     merge_tensordicts,
     pad,
     pad_sequence,
     SubTensorDict,
     TensorDict,
     TensorDictBase,
 )
+from tensordict.utils import is_tensorclass
 
 try:
     from tensordict.version import __version__
 except ImportError:
     __version__ = None
 
 
@@ -37,10 +38,9 @@
     "pad_sequence",
     "make_tensordict",
     "is_memmap",
     "is_batchedtensor",
     "is_tensor_collection",
     "pad",
     "PersistentTensorDict",
-    "is_tensorclass",
     "tensorclass",
 ]
```

## tensordict/memmap.py

```diff
@@ -280,17 +280,17 @@
         memmap_copy = copy(memmap_tensor)
         if memmap_copy._index is None:
             memmap_copy._index = []
         else:
             # avoid extending someone else's index
             memmap_copy._index = deepcopy(memmap_copy._index)
         memmap_copy._index.append(index)
-        # memmap_copy.transfer_ownership = False
         memmap_copy._shape_indexed = None
         memmap_copy.file = memmap_tensor.file
+        memmap_copy._memmap_array = memmap_tensor._memmap_array
 
         return memmap_copy
 
     def __iter__(self):
         for i in range(self.shape[0]):
             yield self[i]
```

## tensordict/persistent.py

```diff
@@ -7,14 +7,16 @@
 from __future__ import annotations
 
 import tempfile
 import warnings
 from pathlib import Path
 from typing import Any
 
+from tensordict._tensordict import unravel_keys
+
 H5_ERR = None
 try:
     import h5py
 
     _has_h5 = True
 except ModuleNotFoundError as err:
     H5_ERR = err
@@ -31,14 +33,15 @@
     NO_DEFAULT,
     TensorDict,
     TensorDictBase,
 )
 from tensordict.utils import (
     _maybe_unravel_keys_silent,
     _shape,
+    cache,
     DeviceType,
     expand_right,
     IndexType,
     NestedKey,
     NUMPY_TO_TORCH_DTYPE_DICT,
 )
 
@@ -117,28 +120,30 @@
         ...     data = PersistentTensorDict(file=f, batch_size=[3], mode="w")
         ...     data["a", "b"] = torch.randn(3, 4)
         ...     print(data)
 
     """
 
     def __new__(cls, *args, **kwargs):
-        cls._names = None
+        cls._td_dim_names = None
         return super().__new__(cls, *args, **kwargs)
 
     def __init__(
         self,
         *,
         batch_size,
         filename=None,
         group=None,
         mode="r",
         backend="h5",
         device=None,
         **kwargs,
     ):
+        self._locked_tensordicts = []
+        self._lock_id = set()
         if not _has_h5:
             raise ModuleNotFoundError("Could not load h5py.") from H5_ERR
         super().__init__()
         self.filename = filename
         self.mode = mode
         if backend != "h5":
             raise NotImplementedError
@@ -218,14 +223,15 @@
         return out
 
     def close(self):
         """Closes the persistent tensordict."""
         self.file.close()
 
     def _process_key(self, key):
+        key = unravel_keys(key)
         if isinstance(key, str):
             return key
         else:
             return "/".join(key)
 
     def _check_batch_size(self, batch_size) -> None:
         for key in self.keys(include_nested=True, leaves_only=True):
@@ -242,38 +248,45 @@
             array = self.file[key]
             return array
         except KeyError:
             if default is not NO_DEFAULT:
                 return default
             raise KeyError(f"key {key} not found in PersistentTensorDict {self}")
 
-    def get(self, key, default=NO_DEFAULT):
-        array = self._get_array(key, default)
+    def _process_array(self, key, array):
         if isinstance(array, (h5py.Dataset,)):
             if self.device is not None:
                 device = self.device
             else:
                 device = torch.device("cpu")
             # we convert to an array first to avoid "Creating a tensor from a list of numpy.ndarrays is extremely slow."
             array = array[:]
             out = torch.as_tensor(array, device=device)
             if self._pin_mem:
                 return out.pin_memory()
             return out
-        elif array is not default:
+        else:
             out = self._nested_tensordicts.get(key, None)
             if out is None:
                 out = self._nested_tensordicts[key] = PersistentTensorDict(
                     group=array,
                     batch_size=self.batch_size,
                     device=self.device,
                 )
             return out
-        else:
-            return default
+
+    @cache  # noqa: B019
+    def get(self, key, default=NO_DEFAULT):
+        array = self._get_array(key, default)
+        if array is default:
+            return array
+        return self._process_array(key, array)
+
+    _get_str = get
+    _get_tuple = get
 
     def get_at(
         self, key: str, idx: IndexType, default: CompatibleType = NO_DEFAULT
     ) -> CompatibleType:
         array = self._get_array(key, default)
         if isinstance(array, (h5py.Dataset,)):
             if self.device is not None:
@@ -390,14 +403,15 @@
                 if err_set_batch_size is not None:
                     raise err from err_set_batch_size
                 raise RuntimeError(
                     f"Cannot broadcast the tensordict {value} to the shape of the indexed persistent tensordict {self}[{index}]."
                 ) from err
         sub_td.update(value, inplace=True)
 
+    # @cache  # noqa: B019
     def keys(
         self, include_nested: bool = False, leaves_only: bool = False
     ) -> _PersistentTDKeysView:
         return _PersistentTDKeysView(
             tensordict=self,
             include_nested=include_nested,
             leaves_only=leaves_only,
@@ -594,14 +608,19 @@
             array[:] = value
         else:
             nested = self.get(key)
             for subkey in nested.keys():
                 nested.fill_(subkey, value)
         return self
 
+    def _create_nested_str(self, key):
+        self.file.create_group(key)
+        target_td = self._get_str(key)
+        return target_td
+
     def select(
         self, *keys: str, inplace: bool = False, strict: bool = True
     ) -> PersistentTensorDict:
         raise NotImplementedError(
             "Cannot call select on a PersistentTensorDict. "
             "Create a regular tensordict first using the `to_tensordict` method."
         )
@@ -686,18 +705,18 @@
             raise RuntimeError("Cannot pass an index to _set when inplace=False.")
         # shortcut set if we're placing a tensordict
         if is_tensor_collection(value):
             if isinstance(key, tuple):
                 key, subkey = key[0], key[1:]
             else:
                 key, subkey = key, []
-            target_td = self.get(key, default=None)
+            target_td = self._get_str(key, default=None)
             if target_td is None:
                 self.file.create_group(key)
-                target_td = self.get(key)
+                target_td = self._get_str(key)
                 target_td.batch_size = value.batch_size
             elif not is_tensor_collection(target_td):
                 raise RuntimeError(
                     f"cannot set a tensor collection in place of a non-tensor collection in {self.__class__.__name__}. "
                     f"Got self.get({key})={target_td} and value={value}."
                 )
             if idx is None:
@@ -805,15 +824,15 @@
         for key, td in orig_metadata_container._nested_tensordicts.items():
             array = self._get_array(key)
             self._nested_tensordicts[key] = PersistentTensorDict(
                 group=array,
                 batch_size=td.batch_size,
                 device=td.device,
             )
-            self._nested_tensordicts[key].names = td._names
+            self._nested_tensordicts[key].names = td._td_dim_names
             self._nested_tensordicts[key]._set_metadata(td)
 
     def clone(self, recurse: bool = True, newfile=None) -> PersistentTensorDict:
         if recurse:
             # this should clone the h5 to a new location indicated by newfile
             if newfile is None:
                 warnings.warn(
@@ -832,15 +851,15 @@
                 # f_src.copy(f_src[key],  f_dest[key], "DataSet")
             # create a non-recursive copy and update the file
             # this way, we can keep the batch-size of every nested tensordict
             clone = self.clone(False)
             clone.file = f_dest
             clone.filename = newfile
             clone._pin_mem = False
-            clone.names = self._names
+            clone.names = self._td_dim_names
             clone._nested_tensordicts = {}
             clone._set_metadata(self)
             return clone
         else:
             # we need to keep the batch-size of nested tds, which we do manually
             nested_tds = {
                 key: td.clone(False) for key, td in self._nested_tensordicts.items()
@@ -853,15 +872,15 @@
                 mode=self.mode,
                 backend="h5",
                 device=self.device,
                 batch_size=self.batch_size,
             )
             clone._nested_tensordicts = nested_tds
             clone._pin_mem = False
-            clone.names = self._names
+            clone.names = self._td_dim_names
             return clone
 
     def __getstate__(self):
         state = self.__dict__.copy()
         filename = state["file"].file.filename
         group_name = state["file"].name
         state["file"] = None
@@ -872,14 +891,21 @@
     def __setstate__(self, state):
         state["file"] = h5py.File(state["filename"], mode=state["mode"])
         if state["group_name"] != "/":
             state["file"] = state["file"][state["group_name"]]
         del state["group_name"]
         self.__dict__.update(state)
 
+    def _add_batch_dim(self, *, in_dim, vmap_level):
+        raise RuntimeError("Persistent tensordicts cannot be used with vmap.")
+
+    def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
+        # not accessible
+        ...
+
 
 def _set_max_batch_size(source: PersistentTensorDict):
     """Updates a tensordict with its maximium batch size."""
     tensor_data = list(source._items_metadata())
     for key, val in tensor_data:
         if not val["array"]:
             _set_max_batch_size(source.get(key))
```

## tensordict/tensorclass.py

```diff
@@ -16,25 +16,26 @@
 from dataclasses import dataclass
 from textwrap import indent
 from typing import Any, Callable, Sequence, TypeVar
 
 import tensordict as tensordict_lib
 
 import torch
-
+from tensordict._tensordict import unravel_keys
 from tensordict.memmap import MemmapTensor
 from tensordict.tensordict import (
     _get_repr,
     is_tensor_collection,
     NO_DEFAULT,
     TD_HANDLED_FUNCTIONS,
     TensorDict,
     TensorDictBase,
 )
-from tensordict.utils import DeviceType, IndexType, NestedKey
+
+from tensordict.utils import DeviceType, IndexType, is_tensorclass, NestedKey
 from torch import Tensor
 
 T = TypeVar("T", bound=TensorDictBase)
 PY37 = sys.version_info < (3, 8)
 
 # Regex precompiled patterns
 OPTIONAL_PATTERN = re.compile(r"Optional\[(.*?)\]")
@@ -56,24 +57,14 @@
     torch.split,
     torch.stack,
     torch.cat,
     torch.gather,
 }
 
 
-def is_tensorclass(obj: type | Any) -> bool:
-    """Returns True if obj is either a tensorclass or an instance of a tensorclass."""
-    cls = obj if isinstance(obj, type) else type(obj)
-    return (
-        dataclasses.is_dataclass(cls)
-        and "to_tensordict" in cls.__dict__
-        and "_from_tensordict" in cls.__dict__
-    )
-
-
 def tensorclass(cls: T) -> T:
     """A decorator to create :obj:`tensorclass` classes.
 
     :obj:`tensorclass` classes are specialized :obj:`dataclass` instances that
     can execute some pre-defined tensor operations out of the box, such as
     indexing, item assignment, reshaping, casting to device or storage and many
     others.
@@ -165,14 +156,15 @@
         if attr in dir(TensorDict):
             raise AttributeError(
                 f"Attribute name {attr} can't be used with @tensorclass"
             )
 
     cls.__init__ = _init_wrapper(cls.__init__)
     cls._from_tensordict = classmethod(_from_tensordict_wrapper(expected_keys))
+    cls.from_tensordict = cls._from_tensordict
     cls.__torch_function__ = classmethod(__torch_function__)
     cls.__getstate__ = _getstate
     cls.__setstate__ = _setstate
     cls.__getattribute__ = _getattribute_wrapper(cls.__getattribute__)
     cls.__setattr__ = _setattr_wrapper(cls.__setattr__, expected_keys)
     cls.__getattr__ = _getattr
     cls.__getitem__ = _getitem
@@ -185,21 +177,31 @@
     cls.set = _set
     cls.set_at_ = _set_at_
     cls.get = _get
     cls.get_at = _get_at
     cls.state_dict = _state_dict
     cls.load_state_dict = _load_state_dict
 
+    for attr in TensorDict.__dict__.keys():
+        func = getattr(TensorDict, attr)
+        if (
+            inspect.ismethod(func) and func.__self__ is TensorDict
+        ):  # detects classmethods
+            setattr(cls, attr, _wrap_classmethod(cls, func))
+
     cls.to_tensordict = _to_tensordict
     cls.device = property(_device, _device_setter)
     cls.batch_size = property(_batch_size, _batch_size_setter)
 
     cls.__doc__ = f"{cls.__name__}{inspect.signature(cls)}"
 
-    tensordict_lib.tensordict._ACCEPTED_CLASSES += [cls]
+    tensordict_lib.tensordict._ACCEPTED_CLASSES = (
+        *tensordict_lib.tensordict._ACCEPTED_CLASSES,
+        cls,
+    )
     return cls
 
 
 def _arg_to_tensordict(arg):
     # if arg is a tensorclass or sequence of tensorclasses, extract the underlying
     # tensordicts and return those instead
     if is_tensorclass(arg):
@@ -364,79 +366,59 @@
     @functools.wraps(getattribute)
     def wrapper(self, item: str) -> Any:
         if not item.startswith("__"):
             if (
                 "_tensordict" in self.__dict__
                 and item in self.__dict__["_tensordict"].keys()
             ):
-                out = self._tensordict[item]
+                out = self._tensordict.get(item)
                 return out
             elif (
                 "_non_tensordict" in self.__dict__
                 and item in self.__dict__["_non_tensordict"]
             ):
                 out = self._non_tensordict[item]
                 return out
         return getattribute(self, item)
 
     return wrapper
 
 
+SET_ATTRIBUTES = ("batch_size", "device", "_locked_tensordicts")
+
+
 def _setattr_wrapper(setattr_: Callable, expected_keys: set[str]) -> Callable:
     @functools.wraps(setattr_)
     def wrapper(self, key: str, value: Any) -> None:  # noqa: D417
         """Set the value of an attribute for the tensor class object.
 
         Args:
             key (str): the name of the attribute to set
             value (any): the value to set for the attribute
 
         """
+        __dict__ = self.__dict__
         if (
-            "_tensordict" not in self.__dict__
-            or "_non_tensordict" not in self.__dict__
-            or key in ("batch_size", "device")
+            "_tensordict" not in __dict__
+            or "_non_tensordict" not in __dict__
+            or key in SET_ATTRIBUTES
         ):
             return setattr_(self, key, value)
-        if key not in expected_keys:
-            raise AttributeError(
-                f"Cannot set the attribute '{key}', expected attributes are {expected_keys}."
-            )
 
-        if isinstance(value, tuple(tensordict_lib.tensordict._ACCEPTED_CLASSES)):
-            # Avoiding key clash, honoring the user input to assign tensor type data to the key
-            if key in self._non_tensordict.keys():
-                del self._non_tensordict[key]
-            self._tensordict[key] = value
-        else:
-            # Avoiding key clash, honoring the user input to assign non-tensor data to the key
-            if key in self._tensordict.keys():
-                del self._tensordict[key]
-            # Saving all non-tensor attributes
-            self._non_tensordict[key] = value
-        return None
+        out = self.set(key, value)
+        if out is not self:
+            raise RuntimeError(
+                "Cannot set attribute on a locked tensorclass, even if "
+                "clone_on_set is set to True. Use my_obj.set(...) instead."
+            )
 
     return wrapper
 
 
-def _getattr(self, attr: str) -> Any:
-    """Retrieve the value of an object's attribute, or a method output if attr is callable.
-
-    Args:
-        attr: name of the attribute to retrieve or function to compute
-
-    Returns:
-        value of the attribute, or a method output applied on the instance
-
-    """
-    res = getattr(self._tensordict, attr)
-    if not callable(res):
-        return res
-    func = res
-
+def _wrap_method(self, attr, func):
     @functools.wraps(func)
     def wrapped_func(*args, **kwargs):
         args = tuple(_arg_to_tensordict(arg) for arg in args)
         kwargs = {key: _arg_to_tensordict(value) for key, value in kwargs.items()}
         res = func(*args, **kwargs)
         if isinstance(res, TensorDictBase):
             if attr.endswith("_"):
@@ -451,14 +433,44 @@
             # create a new tensorclass from res and copy the metadata from self
             return self._from_tensordict(res, copy(self._non_tensordict))
         return res
 
     return wrapped_func
 
 
+def _wrap_classmethod(cls, func):
+    @functools.wraps(func)
+    def wrapped_func(*args, **kwargs):
+        res = func.__get__(cls)(*args, **kwargs)
+        # res = func(*args, **kwargs)
+        if isinstance(res, TensorDictBase):
+            # create a new tensorclass from res and copy the metadata from self
+            return cls._from_tensordict(res)
+        return res
+
+    return wrapped_func
+
+
+def _getattr(self, attr: str) -> Any:
+    """Retrieve the value of an object's attribute, or a method output if attr is callable.
+
+    Args:
+        attr: name of the attribute to retrieve or function to compute
+
+    Returns:
+        value of the attribute, or a method output applied on the instance
+
+    """
+    res = getattr(self._tensordict, attr)
+    if not callable(res):
+        return res
+    func = res
+    return _wrap_method(self, attr, func)
+
+
 def _getitem(self, item: NestedKey) -> Any:
     """Retrieve the class object at the given index. Indexing will happen for nested tensors as well.
 
     Args:
        item (int or any other valid index type): index of the object to retrieve
 
     Returns:
@@ -585,34 +597,67 @@
         "device cannot be set using tensorclass.device = device, "
         "because device cannot be updated in-place. To update device, use "
         "tensorclass.to(new_device), which will return a new tensorclass "
         "on the new device."
     )
 
 
-def _set(self, key: NestedKey, value: Any):
+def _set(self, key: NestedKey, value: Any, inplace: bool = False):
     """Sets a new key-value pair.
 
     Args:
         key (str, tuple of str): name of the key to be set.
            If tuple of str it is equivalent to chained calls of getattr
            followed by a final setattr.
         value (Any): value to be stored in the tensorclass
+        inplace (bool, optional): if ``True``, set will tentatively try to
+            update the value in-place. If ``False`` or if the key isn't present,
+            the value will be simply written at its destination.
 
     Returns:
         self
 
     """
     if isinstance(key, str):
-        key = (key,)
+        __dict__ = self.__dict__
+        if __dict__["_tensordict"].is_locked:
+            raise RuntimeError(TensorDictBase.LOCK_ERROR)
+        expected_keys = self.__dataclass_fields__
+        if key not in expected_keys:
+            raise AttributeError(
+                f"Cannot set the attribute '{key}', expected attributes are {expected_keys}."
+            )
+
+        if isinstance(value, tuple(tensordict_lib.tensordict._ACCEPTED_CLASSES)):
+            # Avoiding key clash, honoring the user input to assign tensor type data to the key
+            if key in self._non_tensordict.keys():
+                if inplace:
+                    raise RuntimeError(
+                        f"Cannot update an existing entry of type {type(self._non_tensordict.get(key))} with a value of type {type(value)}."
+                    )
+                del self._non_tensordict[key]
+            self._tensordict.set(key, value, inplace=inplace)
+        else:
+            # Avoiding key clash, honoring the user input to assign non-tensor data to the key
+            if key in self._tensordict.keys():
+                if inplace:
+                    raise RuntimeError(
+                        f"Cannot update an existing entry of type {type(self._tensordict.get(key))} with a value of type {type(value)}."
+                    )
+                self._tensordict.del_(key)
+            # Saving all non-tensor attributes
+            self._non_tensordict[key] = value
+        return self
 
-    if key and isinstance(key, tuple):
+    if isinstance(key, tuple) and len(key):
+        key = unravel_keys(key)
         if len(key) > 1:
-            return getattr(self, key[0]).set(key[1:], value)
-        return setattr(self, key[0], value)
+            return self.set(key[0], getattr(self, key[0]).set(key[1:], value))
+        out = self.set(key[0], value)
+        return out
     raise ValueError(
         f"Supported type for key are str and tuple, got {key} of type {type(key)}"
     )
 
 
 def _set_at_(self, key: NestedKey, value: Any, idx: IndexType):
     if key in self._non_tensordict:
```

## tensordict/tensordict.py

```diff
@@ -5,14 +5,15 @@
 
 from __future__ import annotations
 
 import abc
 import collections
 import functools
 import numbers
+import os
 import re
 import textwrap
 import warnings
 from collections import defaultdict
 from collections.abc import MutableMapping
 from copy import copy, deepcopy
 from numbers import Number
@@ -29,53 +30,66 @@
     Union,
 )
 from warnings import warn
 
 import numpy as np
 
 import torch
+from tensordict._tensordict import unravel_keys
 from tensordict.memmap import memmap_tensor_as_tensor, MemmapTensor
 from tensordict.utils import (
     _device,
     _dtype,
     _get_item,
     _getitem_batch_size,
     _is_shared,
+    _is_tensorclass,
     _maybe_unravel_keys_silent,
     _set_item,
     _shape,
+    _StringOnlyDict,
     _sub_index,
+    as_decorator,
+    cache,
     convert_ellipsis_to_idx,
     DeviceType,
+    erase_cache,
     expand_as_right,
     expand_right,
     IndexType,
     int_generator,
+    is_tensorclass,
+    lock_blocked,
     NestedKey,
+    NON_STR_KEY,
+    NON_STR_KEY_TUPLE,
     prod,
-    unravel_keys,
+    # unravel_keys,
 )
 from torch import distributed as dist, Tensor
 from torch.utils._pytree import tree_map
 
-
 try:
     from torch.jit._shape_functions import infer_size_impl
 except ImportError:
     from tensordict.utils import infer_size_impl
 
 # from torch.utils._pytree import _register_pytree_node
 
 
 _has_functorch = False
 try:
     try:
         from functorch._C import is_batchedtensor
     except ImportError:
-        from torch._C._functorch import is_batchedtensor
+        from torch._C._functorch import (
+            _add_batch_dim,
+            _remove_batch_dim,
+            is_batchedtensor,
+        )
 
     _has_functorch = True
 except ImportError:
     _has_functorch = False
 
     def is_batchedtensor(tensor: Tensor) -> bool:
         """Placeholder for the functorch function."""
@@ -109,14 +123,31 @@
     CompatibleType = Union[
         Tensor,
         MemmapTensor,
         KeyedJaggedTensor,
     ]
 _STR_MIXED_INDEX_ERROR = "Received a mixed string-non string index. Only string-only or string-free indices are supported."
 
+_HEURISTIC_EXCLUDED = (Tensor, tuple, list, set, dict, np.ndarray)
+
+_TENSOR_COLLECTION_MEMO = {}
+
+
+def _is_tensor_collection(datatype):
+    out = _TENSOR_COLLECTION_MEMO.get(datatype, None)
+    if out is None:
+        if issubclass(datatype, TensorDictBase):
+            out = True
+        elif _is_tensorclass(datatype):
+            out = True
+        else:
+            out = False
+        _TENSOR_COLLECTION_MEMO[datatype] = out
+    return out
+
 
 def is_tensor_collection(datatype: type | Any) -> bool:
     """Checks if a data object or a type is a tensor container from the tensordict lib.
 
     Returns:
         ``True`` if the input is a TensorDictBase subclass, a tensorclass or an istance of these.
         ``False`` otherwise.
@@ -128,21 +159,18 @@
         ... class MyClass:
         ...     pass
         ...
         >>> is_tensor_collection(MyClass)  # True
         >>> is_tensor_collection(MyClass(batch_size=[]))  # True
 
     """
-    from tensordict.tensorclass import is_tensorclass
-
-    return (
-        issubclass(datatype, TensorDictBase)
-        if isinstance(datatype, type)
-        else isinstance(datatype, TensorDictBase)
-    ) or is_tensorclass(datatype)
+    # memoizing is 2x faster
+    if not isinstance(datatype, type):
+        datatype = type(datatype)
+    return _is_tensor_collection(datatype)
 
 
 def is_memmap(datatype: type | Any) -> bool:
     """Returns ``True`` if the class is a subclass of :class:`~.MemmapTensor` or the object an instance of it."""
     return (
         issubclass(datatype, MemmapTensor)
         if isinstance(datatype, type)
@@ -185,58 +213,56 @@
         self.leaves_only = leaves_only
 
     def __iter__(self) -> Iterable[str] | Iterable[tuple[str, ...]]:
         if not self.include_nested:
             if self.leaves_only:
                 for key in self._keys():
                     target_class = self.tensordict.entry_class(key)
-                    if is_tensor_collection(target_class):
+                    if _is_tensor_collection(target_class):
                         continue
                     yield key
             else:
                 yield from self._keys()
         else:
             yield from self._iter_helper(self.tensordict)
 
     def _iter_helper(
         self, tensordict: TensorDictBase, prefix: str | None = None
     ) -> Iterable[str] | Iterable[tuple[str, ...]]:
         items_iter = self._items(tensordict)
 
         for key, value in items_iter:
             full_key = self._combine_keys(prefix, key)
-            if (
-                is_tensor_collection(value)
-                or isinstance(value, (KeyedJaggedTensor,))
-                and self.include_nested
+            cls = value.__class__
+            if self.include_nested and (
+                _is_tensor_collection(cls) or issubclass(cls, KeyedJaggedTensor)
             ):
                 subkeys = tuple(
                     self._iter_helper(
                         value,
                         full_key if isinstance(full_key, tuple) else (full_key,),
                     )
                 )
                 yield from subkeys
-            if not (is_tensor_collection(value) and self.leaves_only):
+            if not self.leaves_only or not _is_tensor_collection(cls):
                 yield full_key
 
     def _combine_keys(self, prefix: str | None, key: NestedKey) -> NestedKey:
         if prefix is not None:
             if isinstance(key, tuple):
                 return prefix + key
-            return (*prefix, key)
+            return prefix + (key,)
         return key
 
     def __len__(self) -> int:
         return sum(1 for _ in self)
 
     def _items(
         self, tensordict: TensorDict | None = None
     ) -> Iterable[tuple[NestedKey, CompatibleType]]:
-        from tensordict.tensorclass import is_tensorclass
 
         if tensordict is None:
             tensordict = self.tensordict
         if isinstance(tensordict, TensorDict) or is_tensorclass(tensordict):
             return tensordict._tensordict.items()
         elif isinstance(tensordict, LazyStackedTensorDict):
             return _iter_items_lazystack(tensordict)
@@ -248,61 +274,54 @@
             # be careful to not rely on tensordict._tensordict existing.
             return ((key, tensordict.get(key)) for key in tensordict._source.keys())
 
     def _keys(self) -> _TensorDictKeysView:
         return self.tensordict._tensordict.keys()
 
     def __contains__(self, key: NestedKey) -> bool:
+        try:
+            key = unravel_keys(key)
+        except Exception as err:
+            raise TypeError(NON_STR_KEY) from err
+
         if isinstance(key, str):
             if key in self._keys():
                 if self.leaves_only:
-                    return not is_tensor_collection(self.tensordict.entry_class(key))
+                    return not _is_tensor_collection(self.tensordict.entry_class(key))
                 return True
             return False
-
-        elif isinstance(key, tuple):
+        else:
+            # thanks to unravel_keys we know the key is a tuple
             if len(key) == 1:
-                return key[0] in self
-            elif len(key) > 1 and self.include_nested:
+                return key[0] in self._keys()
+            elif self.include_nested:
                 if key[0] in self._keys():
                     entry_type = self.tensordict.entry_class(key[0])
-                    is_tensor = entry_type is Tensor
-                    is_kjt = not is_tensor and entry_type is KeyedJaggedTensor
-                    _is_tensordict = (
-                        not is_tensor
-                        and not is_kjt
-                        and is_tensor_collection(entry_type)
-                    )
-
-                    # TODO: SavedTensorDict currently doesn't support nested membership checks
-                    _tensordict_nested = _is_tensordict and key[
-                        1:
-                    ] in self.tensordict.get(key[0]).keys(
-                        include_nested=self.include_nested
-                    )
-                    if _tensordict_nested:
-                        return True
-                    _kjt = (
-                        is_kjt
-                        and len(key) == 2
-                        and key[1] in self.tensordict.get(key[0]).keys()
-                    )
-                    return _kjt
-
+                    if entry_type in (Tensor, MemmapTensor):
+                        return False
+                    if entry_type is KeyedJaggedTensor:
+                        if len(key) > 2:
+                            return False
+                        return key[1] in self.tensordict.get(key[0]).keys()
+                    _is_tensordict = _is_tensor_collection(entry_type)
+                    if _is_tensordict:
+                        # # this will call unravel_keys many times
+                        # return key[1:] in self.tensordict._get_str(key[0], NO_DEFAULT).keys(include_nested=self.include_nested)
+                        # this won't call unravel_keys but requires to get the default which can be suboptimal
+                        leaf_td = self.tensordict._get_tuple(key[:-1], None)
+                        if leaf_td is None or (
+                            not _is_tensor_collection(leaf_td.__class__)
+                            and not isinstance(leaf_td, KeyedJaggedTensor)
+                        ):
+                            return False
+                        return key[-1] in leaf_td.keys()
                 return False
+            # this is reached whenever there is more than one key but include_nested is False
             if all(isinstance(subkey, str) for subkey in key):
-                raise TypeError(
-                    "Nested membership checks with tuples of strings is only supported "
-                    "when setting `include_nested=True`."
-                )
-
-        raise TypeError(
-            "TensorDict keys are always strings. Membership checks are only supported "
-            "for strings or non-empty tuples of strings (for nested TensorDicts)"
-        )
+                raise TypeError(NON_STR_KEY_TUPLE)
 
     def __repr__(self):
         include_nested = f"include_nested={self.include_nested}"
         leaves_only = f"leaves_only={self.leaves_only}"
         return f"{self.__class__.__name__}({list(self)},\n{indent(include_nested, 4*' ')},\n{indent(leaves_only, 4*' ')})"
 
 
@@ -321,29 +340,58 @@
 
     LOCK_ERROR = (
         "Cannot modify locked TensorDict. For in-place modification, consider "
         "using the `set_()` method and make sure the key is present."
     )
 
     def __new__(cls, *args: Any, **kwargs: Any) -> TensorDictBase:
-        cls._safe = kwargs.get("_safe", False)
-        cls._lazy = kwargs.get("_lazy", False)
-        cls._inplace_set = kwargs.get("_inplace_set", False)
-        cls.is_meta = kwargs.get("is_meta", False)
-        cls._is_locked = kwargs.get("_is_locked", False)
-        cls._sorted_keys = None
-        return super().__new__(cls)
+        self = super().__new__(cls)
+        self._safe = kwargs.get("_safe", False)
+        self._lazy = kwargs.get("_lazy", False)
+        self._inplace_set = kwargs.get("_inplace_set", False)
+        self.is_meta = kwargs.get("is_meta", False)
+        self._is_locked = kwargs.get("_is_locked", False)
+        self._sorted_keys = None
+        self._cache = None
+        self._last_op = None
+        self._last_op_queue = collections.deque()
+        return self
 
     def __getstate__(self) -> dict[str, Any]:
         state = self.__dict__.copy()
         return state
 
     def __setstate__(self, state: dict[str, Any]) -> dict[str, Any]:
         self.__dict__.update(state)
 
+    @staticmethod
+    def from_module(module):
+        """Copies the params and buffers of a module in a tensordict.
+
+        Examples:
+            >>> from torch import nn
+            >>> module = nn.TransformerDecoder(
+            ...     decoder_layer=nn.TransformerDecoderLayer(nhead=4, d_model=4),
+            ...     num_layers=1)
+            >>> params = TensorDict.from_module(module)
+            >>> print(params["layers", "0", "linear1"])
+            TensorDict(
+                fields={
+                    bias: Parameter(shape=torch.Size([2048]), device=cpu, dtype=torch.float32, is_shared=False),
+                    weight: Parameter(shape=torch.Size([2048, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+        """
+        td = TensorDict(dict(module.named_parameters()), [])
+        td.update(dict(module.named_buffers()))
+        td = td.unflatten_keys(".")
+        td.lock_()
+        return td
+
     @property
     def shape(self) -> torch.Size:
         """See :obj:`TensorDictBase.batch_size`."""
         return self.batch_size
 
     @property
     @abc.abstractmethod
@@ -358,53 +406,59 @@
 
         Returns:
             a torch.Size object describing the TensorDict batch size.
 
         """
         raise NotImplementedError
 
+    def _erase_cache(self):
+        self._cache = None
+
     @property
     def names(self):
-        names = self._names
+        names = self._td_dim_names
         if names is None:
             return [None for _ in range(self.batch_dims)]
         return names
 
+    def _erase_names(self):
+        self._td_dim_names = None
+
     @names.setter
     def names(self, value):
         # we don't run checks on types for efficiency purposes
         if value is None:
-            self._names = None
+            self._erase_names()
             return
         num_none = sum(v is None for v in value)
         if num_none:
             num_none -= 1
         if len(set(value)) != len(value) - num_none:
             raise ValueError(f"Some dimension names are non-unique: {value}.")
         if len(value) != self.batch_dims:
             raise ValueError(
                 "the length of the dimension names must equate the tensordict batch_dims attribute. "
                 f"Got {value} for batch_dims {self.batch_dims}."
             )
         self._rename_subtds(value)
-        self._names = list(value)
+        self._td_dim_names = list(value)
 
     @abc.abstractmethod
     def _rename_subtds(self, value):
         # renames all the sub-tensordicts dimension according to value.
         # If value has less dimensions than the TD, the rest is just assumed to be None
         raise NotImplementedError
 
     def _check_dim_name(self, name):
         if name is None:
             return False
-        if self._names is not None and name in self._names:
+        if self._td_dim_names is not None and name in self._td_dim_names:
             return True
         for key in self.keys():
-            if is_tensor_collection(self.entry_class(key)):
+            if _is_tensor_collection(self.entry_class(key)):
                 if self.get(key)._check_dim_name(name):
                     return True
         else:
             return False
 
     def refine_names(self, *names):
         """Refines the dimension names of self according to names.
@@ -448,15 +502,15 @@
                     continue
                 else:
                     raise RuntimeError(
                         f"refine_names: cannot coerce TensorDict names {self.names} with {names_copy}."
                     )
         self.names = names
         # we also need to rename the sub-tensordicts
-        self._rename_subtds(self.names)
+        # self._rename_subtds(self.names)
         return self
 
     def rename(self, *names, **rename_map):
         clone = self.clone(recurse=False)
         if len(names) == 1 and names[0] is None:
             clone.names = None
         if rename_map and names:
@@ -468,15 +522,15 @@
                 "Neither a name map nor a name list was passed. "
                 "Only one is accepted."
             )
         elif rename_map:
             for i, name in enumerate(clone.names):
                 new_name = rename_map.pop(name, NO_DEFAULT)
                 if new_name is not NO_DEFAULT:
-                    clone._names[i] = new_name
+                    clone._td_dim_names[i] = new_name
             if rename_map:
                 raise ValueError(
                     f"Some names to be renamed were not part of the tensordict names: {rename_map.keys()} vs {self.names}."
                 )
         else:
             clone.names = names
         return clone
@@ -490,24 +544,24 @@
             )
         elif not rename_map and not names and self.batch_dims:
             raise ValueError(
                 "Neither a name map nor a name list was passed. "
                 "Only one is accepted."
             )
         elif rename_map:
-            _names = copy(self.names)
-            for i, name in enumerate(_names):
+            _td_dim_names = copy(self.names)
+            for i, name in enumerate(_td_dim_names):
                 new_name = rename_map.pop(name, NO_DEFAULT)
                 if new_name is not NO_DEFAULT:
-                    _names[i] = new_name
+                    _td_dim_names[i] = new_name
             if rename_map:
                 raise ValueError(
                     f"Some names to be renamed were not part of the tensordict names: {rename_map.keys()} vs {self.names}."
                 )
-            self.names = _names
+            self.names = _td_dim_names
         else:
             self.names = names
         return self
 
     def size(self, dim: int | None = None) -> torch.Size | int:
         """Returns the size of the dimension indicated by :obj:`dim`.
 
@@ -531,29 +585,29 @@
                 "tensordict is not permitted. Consider instantiating the "
                 "tensordict first by calling `td = td.to_tensordict()` before "
                 "resetting the batch size."
             )
         if not isinstance(new_batch_size, torch.Size):
             new_batch_size = torch.Size(new_batch_size)
         for key in self.keys():
-            if is_tensor_collection(self.entry_class(key)):
+            if _is_tensor_collection(self.entry_class(key)):
                 tensordict = self.get(key)
                 if len(tensordict.batch_size) < len(new_batch_size):
                     # document as edge case
                     tensordict.batch_size = new_batch_size
                     self._set(key, tensordict)
         self._check_new_batch_size(new_batch_size)
         self._change_batch_size(new_batch_size)
-        if self._names is not None:
-            if len(self._names) < len(new_batch_size):
-                self.names = self._names + [None] * (
-                    len(new_batch_size) - len(self._names)
+        if self._td_dim_names is not None:
+            if len(self._td_dim_names) < len(new_batch_size):
+                self.names = self._td_dim_names + [None] * (
+                    len(new_batch_size) - len(self._td_dim_names)
                 )
             else:
-                self.names = self._names[: self.batch_dims]
+                self.names = self._td_dim_names[: self.batch_dims]
 
     @property
     def batch_dims(self) -> int:
         """Length of the tensordict batch size.
 
         Returns:
             int describing the number of dimensions of the tensordict.
@@ -596,15 +650,15 @@
         """Clears the device of the tensordict.
 
         Returns: self
 
         """
         self._device = None
         for value in self.values():
-            if is_tensor_collection(value):
+            if _is_tensor_collection(value.__class__):
                 value.clear_device_()
         return self
 
     clear_device = _renamed_inplace_method(clear_device_)
 
     def is_shared(self) -> bool:
         """Checks if tensordict is in shared memory.
@@ -624,15 +678,17 @@
         if self.device and not self._is_memmap:
             return self.device.type == "cuda" or self._is_shared
         return self._is_shared
 
     def state_dict(self) -> OrderedDict[str, Any]:
         out = collections.OrderedDict()
         for key, item in self.apply(memmap_tensor_as_tensor).items():
-            out[key] = item if not is_tensor_collection(item) else item.state_dict()
+            out[key] = (
+                item if not _is_tensor_collection(item.__class__) else item.state_dict()
+            )
         if "__batch_size" in out:
             raise KeyError(
                 "Cannot retrieve the state_dict of a TensorDict with `'__batch_size'` key"
             )
         if "__device" in out:
             raise KeyError(
                 "Cannot retrieve the state_dict of a TensorDict with `'__batch_size'` key"
@@ -890,15 +946,15 @@
         self._send(dst, _tag=init_tag - 1, pseudo_rand=pseudo_rand)
 
     def _send(self, dst: int, _tag: int = -1, pseudo_rand: bool = False) -> int:
         for key in self.sorted_keys:
             value = self.get(key)
             if isinstance(value, Tensor):
                 pass
-            elif is_tensor_collection(value):
+            elif _is_tensor_collection(value.__class__):
                 _tag = value._send(dst, _tag=_tag, pseudo_rand=pseudo_rand)
                 continue
             elif isinstance(value, MemmapTensor):
                 value = value.as_tensor()
             else:
                 raise NotImplementedError(f"Type {type(value)} is not supported.")
             if not pseudo_rand:
@@ -929,15 +985,15 @@
         return self._recv(src, _tag=init_tag - 1, pseudo_rand=pseudo_rand)
 
     def _recv(self, src: int, _tag: int = -1, pseudo_rand: bool = False) -> int:
         for key in self.sorted_keys:
             value = self.get(key)
             if isinstance(value, Tensor):
                 pass
-            elif is_tensor_collection(value):
+            elif _is_tensor_collection(value.__class__):
                 _tag = value._recv(src, _tag=_tag, pseudo_rand=pseudo_rand)
                 continue
             elif isinstance(value, MemmapTensor):
                 value = value.as_tensor()
             else:
                 raise NotImplementedError(f"Type {type(value)} is not supported.")
             if not pseudo_rand:
@@ -1038,15 +1094,15 @@
     ) -> int:
         root = False
         if _futures is None:
             root = True
             _futures = []
         for key in self.sorted_keys:
             value = self.get(key)
-            if is_tensor_collection(value):
+            if _is_tensor_collection(value.__class__):
                 _tag = value._isend(
                     dst, _tag=_tag, pseudo_rand=pseudo_rand, _futures=_futures
                 )
                 continue
             elif isinstance(value, Tensor):
                 pass
             elif isinstance(value, MemmapTensor):
@@ -1108,15 +1164,15 @@
         root = False
         if _future_list is None:
             _future_list = []
             root = True
 
         for key in self.sorted_keys:
             value = self.get(key)
-            if is_tensor_collection(value):
+            if _is_tensor_collection(value.__class__):
                 _tag, _future_list = value._irecv(
                     src,
                     _tag=_tag,
                     _future_list=_future_list,
                     pseudo_rand=pseudo_rand,
                 )
                 continue
@@ -1158,15 +1214,15 @@
     ):
         root = False
         if _future_list is None:
             _future_list = []
             root = True
         for key in self.sorted_keys:
             value = self.get(key)
-            if is_tensor_collection(value):
+            if _is_tensor_collection(value.__class__):
                 _future_list = value._reduce(
                     dst=dst,
                     op=op,
                     async_op=async_op,
                     _future_list=_future_list,
                 )
                 continue
@@ -1209,27 +1265,38 @@
         else:
             # raise KeyError
             raise KeyError(
                 f'key "{key}" not found in {self.__class__.__name__} with '
                 f"keys {sorted(self.keys())}"
             )
 
-    @abc.abstractmethod
     def get(
         self, key: NestedKey, default: str | CompatibleType = NO_DEFAULT
     ) -> CompatibleType:
         """Gets the value stored with the input key.
 
         Args:
             key (str, tuple of str): key to be queried. If tuple of str it is
                 equivalent to chained calls of getattr.
             default: default value if the key is not found in the tensordict.
 
         """
-        raise NotImplementedError(f"{self.__class__.__name__}")
+        key = unravel_keys(key)
+        if isinstance(key, str):
+            return self._get_str(key, default=default)
+        else:
+            return self._get_tuple(key, default=default)
+
+    @abc.abstractmethod
+    def _get_str(self, key, default):
+        ...
+
+    @abc.abstractmethod
+    def _get_tuple(self, key, default):
+        ...
 
     def get_item_shape(self, key: NestedKey):
         """Returns the shape of the entry."""
         return self.get(key).shape
 
     def pop(
         self, key: NestedKey, default: str | CompatibleType = NO_DEFAULT
@@ -1325,26 +1392,26 @@
                 **constructor_kwargs,
             )
         else:
             out = TensorDict(
                 {},
                 batch_size=self.batch_size,
                 device=self.device if not device else device,
-                names=self._names,
+                names=self._td_dim_names,
                 _run_checks=False,
                 **constructor_kwargs,
             )
 
         is_locked = out.is_locked
         if not inplace and is_locked:
             out.unlock_()
 
         for key, item in self.items():
             _others = [_other.get(key) for _other in others]
-            if is_tensor_collection(item):
+            if _is_tensor_collection(item.__class__):
                 item_trsf = item.apply(
                     fn,
                     *_others,
                     inplace=inplace,
                     batch_size=batch_size,
                     device=device,
                     **constructor_kwargs,
@@ -1359,14 +1426,52 @@
                 else:
                     out._set(key, item_trsf, inplace=inplace)
 
         if not inplace and is_locked:
             out.lock_()
         return out
 
+    @cache  # noqa: B019
+    def _add_batch_dim(self, *, in_dim, vmap_level):
+        if self.is_memmap():
+            td = self.cpu().as_tensor()
+        else:
+            td = self
+        out = TensorDict(
+            {
+                key: value._add_batch_dim(in_dim=in_dim, vmap_level=vmap_level)
+                if is_tensor_collection(value)
+                else _add_batch_dim(value, in_dim, vmap_level)
+                for key, value in td.items()
+            },
+            batch_size=[b for i, b in enumerate(td.batch_size) if i != in_dim],
+            names=[name for i, name in enumerate(td.names) if i != in_dim],
+        )
+        return out
+
+    @cache  # noqa: B019
+    def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
+        new_batch_size = list(self.batch_size)
+        new_batch_size.insert(out_dim, batch_size)
+        new_names = list(self.names)
+        new_names.insert(out_dim, None)
+        out = TensorDict(
+            {
+                key: value._remove_batch_dim(
+                    vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim
+                )
+                if is_tensor_collection(value)
+                else _remove_batch_dim(value, vmap_level, batch_size, out_dim)
+                for key, value in self.items()
+            },
+            batch_size=new_batch_size,
+            names=new_names,
+        )
+        return out
+
     def as_tensor(self):
         """Calls as_tensor on all the tensors contained in the object.
 
         This is reserved to classes that contain exclusively MemmapTensors,
         and will raise an exception in all other cases.
 
         """
@@ -1401,31 +1506,44 @@
             self
 
         """
         if input_dict_or_td is self:
             # no op
             return self
         keys = set(self.keys(False))
-        for key, value in input_dict_or_td.items():
+        for key, value in list(input_dict_or_td.items()):
             if clone and hasattr(value, "clone"):
                 value = value.clone()
             if isinstance(key, tuple):
                 key, subkey = key[0], key[1:]
             else:
                 subkey = []
             # the key must be a string by now. Let's check if it is present
             if key in keys:
                 target_type = self.entry_class(key)
-                if is_tensor_collection(target_type):
+                if _is_tensor_collection(target_type):
                     target = self.get(key)
                     if len(subkey):
                         target.update({subkey: value})
                         continue
-                    elif isinstance(value, (dict,)) or is_tensor_collection(value):
-                        target.update(value)
+                    elif isinstance(value, (dict,)) or _is_tensor_collection(
+                        value.__class__
+                    ):
+                        if isinstance(value, LazyStackedTensorDict) and not isinstance(
+                            target, LazyStackedTensorDict
+                        ):
+                            self.set(
+                                key,
+                                LazyStackedTensorDict(
+                                    *target.unbind(value.stack_dim),
+                                    stack_dim=value.stack_dim,
+                                ).update(value),
+                            )
+                        else:
+                            target.update(value)
                         continue
             if len(subkey):
                 self.set((key, *subkey), value, inplace=inplace)
             else:
                 self.set(key, value, inplace=inplace)
         return self
 
@@ -1499,27 +1617,29 @@
                     b: Tensor(torch.Size([3, 4, 10]), dtype=torch.float32)},
                 batch_size=torch.Size([3, 4]),
                 device=None,
                 is_shared=False)
 
         """
         for key, value in input_dict_or_td.items():
-            if not isinstance(value, tuple(_ACCEPTED_CLASSES)):
+            if not isinstance(value, _ACCEPTED_CLASSES):
                 raise TypeError(
                     f"Expected value to be one of types {_ACCEPTED_CLASSES} "
                     f"but got {type(value)}"
                 )
             if clone:
                 value = value.clone()
             self.set_at_(key, value, idx)
         return self
 
     def _convert_to_tensor(self, array: np.ndarray) -> Tensor | MemmapTensor:
         if isinstance(array, np.bool_):
             array = array.item()
+        if isinstance(array, list):
+            array = np.asarray(array)
         return torch.as_tensor(array, device=self.device)
 
     def _convert_to_tensordict(self, dict_value: dict[str, Any]) -> TensorDictBase:
         return TensorDict(
             dict_value,
             batch_size=self.batch_size,
             device=self.device,
@@ -1536,77 +1656,132 @@
         return key
 
     def _validate_value(
         self,
         value: CompatibleType | dict[str, CompatibleType],
         check_shape: bool = True,
     ) -> CompatibleType | dict[str, CompatibleType]:
-        if isinstance(value, tuple(_ACCEPTED_CLASSES)):
+        cls = value.__class__
+        is_tc = _is_tensor_collection(cls)
+        if is_tc or issubclass(cls, _ACCEPTED_CLASSES):
             pass
-        elif isinstance(value, dict):
+        elif issubclass(cls, dict):
             value = self._convert_to_tensordict(value)
         else:
             try:
                 value = self._convert_to_tensor(value)
-            except ValueError:
+            except ValueError as err:
                 raise ValueError(
-                    f"we only supports tensorclasses, tensordicts,"
+                    f"TensorDict conversion only supports tensorclasses, tensordicts,"
                     f" numeric scalars and tensors. Got {type(value)}"
-                )
-
-        if check_shape and _shape(value)[: self.batch_dims] != self.batch_size:
+                ) from err
+        bs = self.batch_size
+        if check_shape and bs and _shape(value)[: len(bs)] != bs:
             # if TensorDict, let's try to map it to the desired shape
-            if is_tensor_collection(value):
+            if is_tc:
                 value = value.clone(recurse=False)
                 value.batch_size = self.batch_size
             else:
                 raise RuntimeError(
                     f"batch dimension mismatch, got self.batch_size"
                     f"={self.batch_size} and value.shape[:self.batch_dims]"
                     f"={_shape(value)[: self.batch_dims]} with value {value}"
                 )
-
-        if self.device is not None:
-            value = value.to(self.device, non_blocking=True)
-
+        device = self.device
+        if device is not None and value.device != device:
+            value = value.to(device, non_blocking=True)
         if (
-            self._names is not None
-            and is_tensor_collection(value)
+            self._td_dim_names is not None
+            and is_tc
             and check_shape
             and value.names[: self.ndim] != self.names
         ):
             value = value.clone(False).refine_names(*self.names)
         elif (
-            self._names is None
+            self._td_dim_names is None
             and check_shape
-            and is_tensor_collection(value)
-            and value._names is not None
+            and is_tc
+            and value._td_dim_names is not None
         ):
             self.names = value.names[: self.batch_dims]
 
         return value
 
     @abc.abstractmethod
     def pin_memory(self) -> TensorDictBase:
         """Calls :obj:`pin_memory` on the stored tensors."""
         raise NotImplementedError(f"{self.__class__.__name__}")
 
     def items(
         self, include_nested: bool = False, leaves_only: bool = False
     ) -> Iterator[tuple[str, CompatibleType]]:
         """Returns a generator of key-value pairs for the tensordict."""
-        for k in self.keys(include_nested=include_nested, leaves_only=leaves_only):
-            yield k, self.get(k)
+        # check the conditions once only
+        if include_nested and leaves_only:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                if _is_tensor_collection(val.__class__):
+                    yield from (
+                        (unravel_keys((k, _key)), _val)
+                        for _key, _val in val.items(
+                            include_nested=include_nested, leaves_only=leaves_only
+                        )
+                    )
+                else:
+                    yield k, val
+        elif include_nested:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                yield k, val
+                if _is_tensor_collection(val.__class__):
+                    yield from (
+                        (unravel_keys((k, _key)), _val)
+                        for _key, _val in val.items(
+                            include_nested=include_nested, leaves_only=leaves_only
+                        )
+                    )
+        elif leaves_only:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                if not _is_tensor_collection(val.__class__):
+                    yield k, val
+        else:
+            for k in self.keys():
+                yield k, self._get_str(k, NO_DEFAULT)
 
     def values(
         self, include_nested: bool = False, leaves_only: bool = False
     ) -> Iterator[CompatibleType]:
         """Returns a generator representing the values for the tensordict."""
-        for k in self.keys(include_nested=include_nested, leaves_only=leaves_only):
-            yield self.get(k)
+        # check the conditions once only
+        if include_nested and leaves_only:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                if _is_tensor_collection(val.__class__):
+                    yield from val.values(
+                        include_nested=include_nested, leaves_only=leaves_only
+                    )
+                else:
+                    yield val
+        elif include_nested:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                yield val
+                if _is_tensor_collection(val.__class__):
+                    yield from val.values(
+                        include_nested=include_nested, leaves_only=leaves_only
+                    )
+        elif leaves_only:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                if not _is_tensor_collection(val.__class__):
+                    yield val
+        else:
+            for k in self.keys():
+                yield self._get_str(k, NO_DEFAULT)
 
     @abc.abstractmethod
     def keys(
         self, include_nested: bool = False, leaves_only: bool = False
     ) -> _TensorDictKeysView:
         """Returns a generator of tensordict keys."""
         raise NotImplementedError(f"{self.__class__.__name__}")
@@ -1731,18 +1906,18 @@
                 list(self.batch_size)[:start_dim]
                 + [nelt]
                 + list(self.batch_size[end_dim + 1 :])
             )
         else:
             batch_size = [nelt] + list(self.batch_size[end_dim + 1 :])
         out = self.apply(flatten, batch_size=batch_size)
-        if self._names is not None:
+        if self._td_dim_names is not None:
             names = [
                 name
-                for i, name in enumerate(self._names)
+                for i, name in enumerate(self._td_dim_names)
                 if (i < start_dim or i > end_dim)
             ]
             names.insert(start_dim, None)
             out.names = names
         return out
 
     def unflatten(self, dim, unflattened_size):
@@ -1777,21 +1952,37 @@
                 list(self.batch_size)[:dim]
                 + list(unflattened_size)
                 + list(self.batch_size[dim + 1 :])
             )
         else:
             batch_size = list(unflattened_size) + list(self.batch_size[1:])
         out = self.apply(unflatten, batch_size=batch_size)
-        if self._names is not None:
-            names = copy(self._names)
+        if self._td_dim_names is not None:
+            names = copy(self._td_dim_names)
             for _ in range(len(unflattened_size) - 1):
                 names.insert(dim, None)
             out.names = names
         return out
 
+    def __enter__(self):
+        self._last_op_queue.append(self._last_op)
+        return self
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        _last_op = self._last_op_queue.pop()
+        if _last_op is not None:
+            last_op, (args, kwargs) = _last_op
+            if last_op is self.__class__.lock_.__name__:
+                return self.unlock_()
+            elif last_op is self.__class__.unlock_.__name__:
+                return self.lock_()
+            else:
+                raise NotImplementedError(f"Unrecognised function {last_op}.")
+        return self
+
     def __bool__(self) -> bool:
         raise ValueError("Converting a tensordict to boolean value is not permitted")
 
     def __ne__(self, other: object) -> TensorDictBase:
         """XOR operation over two tensordicts, for evey key.
 
         The two tensordicts must have the same key set.
@@ -1800,20 +1991,17 @@
             other (TensorDictBase, dict, or float): the value to compare against.
 
         Returns:
             a new TensorDict instance with all tensors are boolean
             tensors of the same shape as the original tensors.
 
         """
-        # avoiding circular imports
-        from tensordict.tensorclass import is_tensorclass
-
-        if is_tensorclass(other):
+        if _is_tensorclass(other.__class__):
             return other != self
-        if isinstance(other, (dict,)) or is_tensor_collection(other):
+        if isinstance(other, (dict,)) or _is_tensor_collection(other.__class__):
             keys1 = set(self.keys())
             keys2 = set(other.keys())
             if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
                 raise KeyError(
                     f"keys in {self} and {other} mismatch, got {keys1} and {keys2}"
                 )
             d = {}
@@ -1832,20 +2020,17 @@
         """Compares two tensordicts against each other, for every key. The two tensordicts must have the same key set.
 
         Returns:
             a new TensorDict instance with all tensors are boolean
             tensors of the same shape as the original tensors.
 
         """
-        # avoiding circular imports
-        from tensordict.tensorclass import is_tensorclass
-
         if is_tensorclass(other):
             return other == self
-        if isinstance(other, (dict,)) or is_tensor_collection(other):
+        if isinstance(other, (dict,)) or _is_tensor_collection(other.__class__):
             keys1 = set(self.keys())
             keys2 = set(other.keys())
             if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
                 raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
             d = {}
             for key, item1 in self.items():
                 d[key] = item1 == other.get(key)
@@ -1942,21 +2127,32 @@
             default (torch.Tensor): default value to return if the key is
                 not present in the tensordict.
 
         Returns:
             indexed tensor.
 
         """
-        # TODO: this is NOT explicitely tested. Make a test
-        try:
-            return self.get(key, NO_DEFAULT)[idx]
-        except KeyError:
-            if default is NO_DEFAULT:
-                raise
-            return default
+        key = unravel_keys(key)
+        if isinstance(key, str):
+            return self._get_at_str(key, idx, default)
+        else:
+            # must be a tuple
+            return self._get_at_tuple(key, idx, default)
+
+    def _get_at_str(self, key, idx, default):
+        out = self._get_str(key, default)
+        if out is default:
+            return out
+        return out[idx]
+
+    def _get_at_tuple(self, key, idx, default):
+        out = self._get_tuple(key, default)
+        if out is default:
+            return out
+        return out[idx]
 
     @abc.abstractmethod
     def share_memory_(self) -> TensorDictBase:
         """Places all the tensors in shared memory.
 
         The TensorDict is then locked, meaning that the only writing operations that
         can be executed must be done in-place.
@@ -2012,32 +2208,32 @@
         Returns:
             a new ``TensorDict`` instance with data stored as memory-mapped tensors.
 
         """
         if prefix is not None:
             prefix = Path(prefix)
             if not prefix.exists():
-                prefix.mkdir(exist_ok=True)
+                os.makedirs(prefix, exist_ok=True)
             torch.save(
                 {"batch_size": self.batch_size, "device": self.device},
                 prefix / "meta.pt",
             )
         if not self.keys():
             raise Exception(
                 "memmap_like() must be called when the TensorDict is (partially) "
                 "populated. Set a tensor first."
             )
         tensordict = TensorDict(
-            {}, self.batch_size, device=self.device, names=self._names
+            {}, self.batch_size, device=self.device, names=self._td_dim_names
         )
         for key, value in self.items():
-            if is_tensor_collection(value):
+            if _is_tensor_collection(value.__class__):
                 if prefix is not None:
                     # ensure subdirectory exists
-                    (prefix / key).mkdir(exist_ok=True)
+                    os.makedirs(prefix / key, exist_ok=True)
                     tensordict[key] = value.memmap_like(
                         prefix=prefix / key,
                     )
                     torch.save(
                         {"batch_size": value.batch_size, "device": value.device},
                         prefix / key / "meta.pt",
                     )
@@ -2077,21 +2273,15 @@
     def detach(self) -> TensorDictBase:
         """Detach the tensors in the tensordict.
 
         Returns:
             a new tensordict with no tensor requiring gradient.
 
         """
-        return TensorDict(
-            {key: item.detach() for key, item in self.items()},
-            batch_size=self.batch_size,
-            device=self.device,
-            names=self._names,
-            _run_checks=False,
-        )
+        return self.apply(lambda x: x.detach())
 
     def to_h5(
         self,
         filename,
         **kwargs,
     ):
         """Converts a tensordict to a PersistentTensorDict with the h5 backend.
@@ -2137,35 +2327,35 @@
         from .persistent import PersistentTensorDict
 
         out = PersistentTensorDict.from_dict(
             self,
             filename=filename,
             **kwargs,
         )
-        if self._names is not None:
-            out.names = self._names
+        if self._td_dim_names is not None:
+            out.names = self._td_dim_names
         return out
 
     def to_tensordict(self):
         """Returns a regular TensorDict instance from the TensorDictBase.
 
         Returns:
             a new TensorDict object containing the same values.
 
         """
         return TensorDict(
             {
                 key: value.clone()
-                if not is_tensor_collection(value)
+                if not _is_tensor_collection(value.__class__)
                 else value.to_tensordict()
                 for key, value in self.items()
             },
             device=self.device,
             batch_size=self.batch_size,
-            names=self._names,
+            names=self._td_dim_names,
         )
 
     def zero_(self) -> TensorDictBase:
         """Zeros all tensors in the tensordict in-place."""
         for key in self.keys():
             self.fill_(key, 0)
         return self
@@ -2179,15 +2369,15 @@
         idx = [
             ((*tuple(slice(None) for _ in range(dim)), i))
             for i in range(self.shape[dim])
         ]
         if dim < 0:
             dim = self.batch_dims + dim
         batch_size = torch.Size([s for i, s in enumerate(self.batch_size) if i != dim])
-        names = self._names
+        names = self._td_dim_names
         if names is not None:
             names = copy(names)
             names = [name for i, name in enumerate(names) if i != dim]
         out = []
         for _idx in idx:
             out.append(
                 self.apply(
@@ -2307,14 +2497,43 @@
 
     def cuda(self, device: int = None) -> TensorDictBase:
         """Casts a tensordict to a cuda device (if not already on it)."""
         if device is None:
             return self.to(torch.device("cuda"))
         return self.to(f"cuda:{device}")
 
+    def _create_nested_str(self, key):
+        self.set(key, self.select())
+
+    def _create_nested_tuple(self, key):
+        self._create_nested_str(key[0])
+        if len(key) > 1:
+            td = self._get_str(key[0], NO_DEFAULT)
+            td._create_nested_tuple(key[1:])
+
+    @lock_blocked
+    def create_nested(self, key):
+        """Creates a nested tensordict of the same shape, device and dim names as the current tensordict.
+
+        If the value already exists, it will be overwritten by this operation.
+        This operation is blocked in locked tensordicts.
+
+        Examples:
+            >>> data = TensorDict({}, [3, 4, 5])
+            >>> data.create_nested("root")
+            >>> data.create_nested(("some", "nested", "value"))
+            >>> nested = data.get(("some", "nested", "value"))
+        """
+        key = unravel_keys(key)
+        if isinstance(key, str):
+            self._create_nested_str(key)
+        else:
+            self._create_nested_tuple(key)
+        return self
+
     @abc.abstractmethod
     def masked_fill_(self, mask: Tensor, value: float | bool) -> TensorDictBase:
         """Fills the values corresponding to the mask with the desired value.
 
         Args:
             mask (boolean torch.Tensor): mask of values to be filled. Shape
                 must match tensordict shape.
@@ -2399,15 +2618,15 @@
     def contiguous(self) -> TensorDictBase:
         """Returns a new tensordict of the same type with contiguous values (or self if values are already contiguous)."""
         raise NotImplementedError
 
     def to_dict(self) -> dict[str, Any]:
         """Returns a dictionary with key-value pairs matching those of the tensordict."""
         return {
-            key: value.to_dict() if is_tensor_collection(value) else value
+            key: value.to_dict() if _is_tensor_collection(value.__class__) else value
             for key, value in self.items()
         }
 
     def unsqueeze(self, dim: int) -> TensorDictBase:
         """Unsqueeze all tensors for a dimension comprised in between `-td.batch_dims` and `td.batch_dims` and returns them in a new tensordict.
 
         Args:
@@ -2571,15 +2790,15 @@
                 "split(): argument 'split_size' must be int or list of ints"
             )
         dictionaries = [{} for _ in range(len(batch_sizes))]
         for key, item in self.items():
             split_tensors = torch.split(item, split_size, dim)
             for idx, split_tensor in enumerate(split_tensors):
                 dictionaries[idx][key] = split_tensor
-        names = self._names
+        names = self._td_dim_names
         if names is not None:
             names = copy(names)
         return [
             TensorDict(
                 dictionaries[i],
                 batch_sizes[i],
                 device=self.device,
@@ -2674,14 +2893,48 @@
             source=self,
             custom_op="view",
             inv_op="view",
             custom_op_kwargs={"size": shape},
             inv_op_kwargs={"size": self.batch_size},
         )
 
+    def transpose(self, dim0, dim1):
+        """Returns a tensordit that is a transposed version of input. The given dimensions ``dim0`` and ``dim1`` are swapped.
+
+        In-place or out-place modifications of the transposed tensordict will
+        impact the original tensordict too as the memory is shared and the operations
+        are mapped back on the original tensordict.
+
+        Examples:
+            >>> tensordict = TensorDict({"a": torch.randn(3, 4, 5)}, [3, 4])
+            >>> tensordict_transpose = tensordict.transpose(0, 1)
+            >>> print(tensordict_transpose.shape)
+            torch.Size([4, 3])
+            >>> tensordict_transpose.set("b",, torch.randn(4, 3))
+            >>> print(tensordict.get("b").shape)
+            torch.Size([3, 4])
+        """
+        if dim0 < 0:
+            dim0 = self.ndim + dim0
+        if dim1 < 0:
+            dim1 = self.ndim + dim1
+        if any((dim0 < 0, dim1 < 0)):
+            raise ValueError(
+                "The provided dimensions are incompatible with the tensordict batch-size."
+            )
+        if dim0 == dim1:
+            return self
+        return _TransposedTensorDict(
+            source=self,
+            custom_op="transpose",
+            inv_op="transpose",
+            custom_op_kwargs={"dim0": dim0, "dim1": dim1},
+            inv_op_kwargs={"dim0": dim0, "dim1": dim1},
+        )
+
     def permute(
         self,
         *dims_list: int,
         dims: list[int] | None = None,
     ) -> TensorDictBase:
         """Returns a view of a tensordict with the batch dimensions permuted according to dims.
 
@@ -2780,15 +3033,15 @@
                 "dim must be greater than or equal to -tensordict.batch_dims and "
                 "smaller than tensordict.batch_dims"
             )
         if dim is not None:
             if dim < 0:
                 dim = self.batch_dims + dim
 
-            names = self._names
+            names = self._td_dim_names
             if names is not None:
                 names = copy(names)
                 names = [name for i, name in enumerate(names) if i != dim]
 
             return TensorDict(
                 source={key: value.all(dim=dim) for key, value in self.items()},
                 batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
@@ -2813,15 +3066,15 @@
                 "dim must be greater than or equal to -tensordict.batch_dims and "
                 "smaller than tensordict.batch_dims"
             )
         if dim is not None:
             if dim < 0:
                 dim = self.batch_dims + dim
 
-            names = self._names
+            names = self._td_dim_names
             if names is not None:
                 names = copy(names)
                 names = [name for i, name in enumerate(names) if i != dim]
 
             return TensorDict(
                 source={key: value.any(dim=dim) for key, value in self.items()},
                 batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
@@ -2837,27 +3090,28 @@
     def __iter__(self) -> Generator:
         if not self.batch_dims:
             raise StopIteration
         length = self.batch_size[0]
         for i in range(length):
             yield self[i]
 
+    @cache  # noqa: B019
     def flatten_keys(
         self, separator: str = ".", inplace: bool = False
     ) -> TensorDictBase:
         to_flatten = []
         existing_keys = self.keys(include_nested=True)
         for key, value in self.items():
             key_split = tuple(key.split(separator))
             if isinstance(value, TensorDictBase):
                 to_flatten.append(key)
             elif (
                 separator in key
                 and key_split in existing_keys
-                and not is_tensor_collection(self.entry_class(key_split))
+                and not _is_tensor_collection(self.entry_class(key_split))
             ):
                 raise KeyError(
                     f"Flattening keys in tensordict collides with existing key '{key}'"
                 )
 
         if inplace:
             for key in to_flatten:
@@ -2885,14 +3139,15 @@
                     )
                     for inner_key, inner_item in inner_tensordict.items():
                         tensordict_out.set(separator.join([key, inner_key]), inner_item)
                 else:
                     tensordict_out.set(key, value)
             return tensordict_out
 
+    @cache  # noqa: B019
     def unflatten_keys(
         self, separator: str = ".", inplace: bool = False
     ) -> TensorDictBase:
         to_unflatten = defaultdict(list)
         for key in self.keys():
             if separator in key[1:-1]:
                 split_key = key.split(separator)
@@ -2946,28 +3201,28 @@
         raise NotImplementedError(
             "TensorDict does not support membership checks with the `in` keyword. If "
             "you want to check if a particular key is in your TensorDict, please use "
             "`key in tensordict.keys()` instead."
         )
 
     def _get_names_idx(self, idx):
-        if self._names is None:
+        if self._td_dim_names is None:
             names = None
         else:
 
             def is_boolean(idx):
                 if isinstance(idx, tuple) and len(idx) == 1:
                     return is_boolean(idx[0])
                 if hasattr(idx, "dtype") and idx.dtype is torch.bool:
                     return idx.ndim
                 return None
 
             num_boolean_dim = is_boolean(idx)
             if num_boolean_dim:
-                names = [None] + self._names[num_boolean_dim:]
+                names = [None] + self._td_dim_names[num_boolean_dim:]
             else:
 
                 def is_int(subidx):
                     if isinstance(subidx, Number):
                         return True
                     if isinstance(subidx, Tensor) and len(subidx.shape) == 0:
                         return True
@@ -2985,15 +3240,18 @@
                     if _idx is None:
                         idx_to_take.append(None)
                     elif _is_number(_idx):
                         count += 1
                     else:
                         idx_to_take.append(count)
                         count += 1
-                names = [self._names[i] if i is not None else None for i in idx_to_take]
+                names = [
+                    self._td_dim_names[i] if i is not None else None
+                    for i in idx_to_take
+                ]
         return names
 
     def _index_tensordict(self, idx: IndexType) -> TensorDictBase:
         names = self._get_names_idx(idx)
         return TensorDict(
             source={key: _get_item(item, idx) for key, item in self.items()},
             batch_size=_getitem_batch_size(self.batch_size, idx),
@@ -3187,15 +3445,15 @@
             value (Number, bool): value to use for the filling
 
         Returns:
             self
 
         """
         target_class = self.entry_class(key)
-        if is_tensor_collection(target_class):
+        if _is_tensor_collection(target_class):
             tensordict = self.get(key)
             tensordict.apply_(lambda x: x.fill_(value))
             self._set(key, tensordict, inplace=True)
         else:
             tensor = torch.full_like(self.get(key), value)
             self._set(key, tensor, inplace=True)
         return self
@@ -3228,46 +3486,97 @@
         """
         if key not in self.keys(include_nested=isinstance(key, tuple)):
             self.set(key, default, inplace=inplace)
         return self.get(key)
 
     @property
     def is_locked(self) -> bool:
-        if "_is_locked" not in self.__dict__:
-            self._is_locked = False
         return self._is_locked
 
     @is_locked.setter
     def is_locked(self, value: bool) -> None:
         if value:
             self.lock_()
         else:
             self.unlock_()
 
-    def lock_(self) -> TensorDictBase:
+    def _lock_propagate(self, lock_ids=None):
+        """Registers the parent tensordict that handles the lock."""
         self._is_locked = True
+        is_root = lock_ids is None
+        if is_root:
+            lock_ids = set()
+        self._lock_id = self._lock_id.union(lock_ids)
+        lock_ids = lock_ids.union({id(self)})
+        _locked_tensordicts = []
         for key in self.keys():
-            if is_tensor_collection(self.entry_class(key)):
-                self.get(key).lock_()
+            if _is_tensor_collection(self.entry_class(key)):
+                dest = self.get(key)
+                dest._lock_propagate(lock_ids)
+                _locked_tensordicts.append(dest)
+        if is_root:
+            self._locked_tensordicts = _locked_tensordicts
+        else:
+            self._locked_tensordicts += _locked_tensordicts
+
+    @as_decorator("is_locked")
+    def lock_(self) -> TensorDictBase:
+        if self.is_locked:
+            return self
+        self._lock_propagate()
         return self
 
     lock = _renamed_inplace_method(lock_)
 
-    def unlock_(self) -> TensorDictBase:
+    def _remove_lock(self, lock_id):
+        self._lock_id = self._lock_id - {lock_id}
+        if self._locked_tensordicts:
+            for td in self._locked_tensordicts:
+                td._remove_lock(lock_id)
+
+    @erase_cache
+    def _propagate_unlock(self, lock_ids=None):
+        if lock_ids is not None:
+            self._lock_id.difference_update(lock_ids)
+        else:
+            lock_ids = set()
         self._is_locked = False
+
+        unlocked_tds = [self]
+        lock_ids.add(id(self))
+        for key in self.keys():
+            if _is_tensor_collection(self.entry_class(key)):
+                dest = self.get(key)
+                unlocked_tds.extend(dest._propagate_unlock(lock_ids))
+        self._locked_tensordicts = []
+
         self._is_shared = False
         self._is_memmap = False
         self._sorted_keys = None
-        for key in self.keys():
-            if is_tensor_collection(self.entry_class(key)):
-                self.get(key).unlock_()
+        return unlocked_tds
+
+    @as_decorator("is_locked")
+    def unlock_(self) -> TensorDictBase:
+        unlock_tds = self._propagate_unlock()
+        for td in unlock_tds:
+            if len(td._lock_id):
+                self.lock_()
+                raise RuntimeError(
+                    "Cannot unlock a tensordict that is part of a locked graph. "
+                    "Unlock the root tensordict first. If the tensordict is part of multiple graphs, "
+                    "group the graphs under a common tensordict an unlock this root. "
+                )
         return self
 
     unlock = _renamed_inplace_method(unlock_)
 
+    def __del__(self):
+        for td in self._locked_tensordicts:
+            td._remove_lock(id(self))
+
     def is_floating_point(self):
         for item in self.values(include_nested=True, leaves_only=True):
             if not item.is_floating_point():
                 return False
         else:
             return True
 
@@ -3301,14 +3610,24 @@
         Args:
             dst_type (type or string): the desired type
 
         """
         return self.apply(lambda x: x.type(dst_type))
 
 
+_ACCEPTED_CLASSES = [
+    Tensor,
+    MemmapTensor,
+    TensorDictBase,
+]
+if _has_torchrec:
+    _ACCEPTED_CLASSES += [KeyedJaggedTensor]
+_ACCEPTED_CLASSES = tuple(_ACCEPTED_CLASSES)
+
+
 class TensorDict(TensorDictBase):
     """A batched dictionary of tensors.
 
     TensorDict is a tensor container where all tensors are stored in a
     key-value pair fashion and where each element shares at least the
     following features:
     - memory location (shared, memory-mapped array, ...);
@@ -3395,86 +3714,96 @@
     __slots__ = (
         "_tensordict",
         "_batch_size",
         "_is_shared",
         "_is_memmap",
         "_device",
         "_is_locked",
-        "_names",
+        "_td_dim_names",
+        "_lock_id",
+        "_locked_tensordicts",
+        "_cache",
+        "_last_op",
+        "_last_op_queue",
     )
 
     def __new__(cls, *args: Any, **kwargs: Any) -> TensorDict:
         cls._is_shared = False
         cls._is_memmap = False
-        cls._names = None
+        cls._td_dim_names = None
         return super().__new__(cls, *args, _safe=True, _lazy=False, **kwargs)
 
     def __init__(
         self,
         source: TensorDictBase | dict[str, CompatibleType],
         batch_size: Sequence[int] | torch.Size | int | None = None,
         device: DeviceType | None = None,
         names: Sequence[str] | None = None,
         _run_checks: bool = True,
         _is_shared: bool | None = False,
         _is_memmap: bool | None = False,
     ) -> None:
+        self._lock_id = set()
+        self._locked_tensordicts = []
+
         self._is_shared = _is_shared
         self._is_memmap = _is_memmap
         if device is not None:
             device = torch.device(device)
         self._device = device
 
         if not _run_checks:
-            self._tensordict: dict = dict(source)
+            _tensordict: dict = _StringOnlyDict()
             self._batch_size = batch_size
-            upd_dict = {}
-            for key, value in self._tensordict.items():
+            for key, value in source.items():
                 if isinstance(value, dict):
                     value = TensorDict(
                         value,
                         batch_size=self._batch_size,
                         device=self._device,
                         _run_checks=_run_checks,
                         _is_shared=_is_shared,
                         _is_memmap=_is_memmap,
                     )
-                    upd_dict[key] = value
-            if upd_dict:
-                self._tensordict.update(upd_dict)
-            self._names = names
+                _tensordict[key] = value
+            self._tensordict = _tensordict
+            self._td_dim_names = names
         else:
-            self._tensordict = {}
+            self._tensordict = _StringOnlyDict()
             if not isinstance(source, (TensorDictBase, dict)):
                 raise ValueError(
                     "A TensorDict source is expected to be a TensorDictBase "
                     f"sub-type or a dictionary, found type(source)={type(source)}."
                 )
             self._batch_size = self._parse_batch_size(source, batch_size)
 
             self.names = names
 
             if source is not None:
                 for key, value in source.items():
                     self.set(key, value)
 
     @classmethod
-    def from_dict(cls, input_dict, batch_size=None, device=None):
+    def from_dict(cls, input_dict, batch_size=None, device=None, batch_dims=None):
         """Returns a TensorDict created from a dictionary or another :class:`TensorDict`.
 
         If ``batch_size`` is not specified, returns the maximum batch size possible.
 
         This function works on nested dictionaries too, or can be used to determine the
         batch-size of a nested tensordict.
 
         Args:
             input_dict (dictionary, optional): a dictionary to use as a data source
                 (nested keys compatible).
             batch_size (iterable of int, optional): a batch size for the tensordict.
             device (torch.device or compatible type, optional): a device for the TensorDict.
+            batch_dims (int, optional): the ``batch_dims`` (ie number of leading dimensions
+                to be considered for ``batch_size``). Exclusinve with ``batch_size``.
+                Note that this is the __maximum__ number of batch dims of the tensordict,
+                a smaller number is tolerated.
 
         Examples:
             >>> input_dict = {"a": torch.randn(3, 4), "b": torch.randn(3)}
             >>> print(TensorDict.from_dict(input_dict))
             TensorDict(
                 fields={
                     a: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
@@ -3511,26 +3840,35 @@
                         device=None,
                         is_shared=False)},
                 batch_size=torch.Size([3]),
                 device=None,
                 is_shared=False)
 
         """
+        if batch_dims is not None and batch_size is not None:
+            raise ValueError(
+                "Cannot pass both batch_size and batch_dims to `from_dict`."
+            )
+
         batch_size_set = [] if batch_size is None else batch_size
         for key, value in list(input_dict.items()):
             if isinstance(value, (dict,)):
-                input_dict[key] = TensorDict(value, batch_size_set, device=device)
+                # we don't know if another tensor of smaller size is coming
+                # so we can't be sure that the batch-size will still be valid later
+                input_dict[key] = TensorDict.from_dict(
+                    value, batch_size=[], device=device, batch_dims=None
+                )
         # _run_checks=False breaks because a tensor may have the same batch-size as the tensordict
         out = cls(
             input_dict,
             batch_size=batch_size_set,
             device=device,
         )
         if batch_size is None:
-            _set_max_batch_size(out)
+            _set_max_batch_size(out, batch_dims)
         else:
             out.batch_size = batch_size
         return out
 
     @staticmethod
     def _parse_batch_size(
         source: TensorDictBase | dict,
@@ -3557,17 +3895,20 @@
         raise RuntimeError(
             f"Setting batch dims on {self.__class__.__name__} instances is "
             f"not allowed."
         )
 
     def _rename_subtds(self, names):
         if names is None:
-            names = [None] * self.ndim
+            for item in self._tensordict.values():
+                if _is_tensor_collection(item.__class__):
+                    item._erase_names()
+            return
         for item in self._tensordict.values():
-            if is_tensor_collection(item):
+            if _is_tensor_collection(item.__class__):
                 item_names = item.names
                 td_names = list(names) + item_names[len(names) :]
                 item.rename_(*td_names)
 
     @property
     def device(self) -> torch.device | None:
         """Device of the tensordict.
@@ -3695,15 +4036,15 @@
                 d[key] = value.expand(*shape)
         out = TensorDict(
             source=d,
             batch_size=torch.Size(shape),
             device=self.device,
             _run_checks=False,
         )
-        if self._names is not None:
+        if self._td_dim_names is not None:
             out.refine_names(..., *self.names)
         return out
 
     def _set(self, key: str, value, inplace: bool = False) -> TensorDictBase:
         if isinstance(key, tuple):
             td, subkey = _get_leaf_tensordict(
                 self, key, _default_hook if not inplace else None
@@ -3779,14 +4120,15 @@
             td, subkey = _get_leaf_tensordict(self, key)
             del td[subkey]
             return self
 
         del self._tensordict[key]
         return self
 
+    @lock_blocked
     def rename_key_(
         self, old_key: str, new_key: str, safe: bool = False
     ) -> TensorDictBase:
         # these checks are not perfect, tuples that are not tuples of strings or empty
         # tuples could go through but (1) it will raise an error anyway and (2)
         # those checks are expensive when repeated often.
         if not isinstance(old_key, (str, tuple)):
@@ -3795,16 +4137,14 @@
             )
         if not isinstance(new_key, (str, tuple)):
             raise TypeError(
                 f"Expected new_name to be a string or a tuple of strings but found {type(new_key)}"
             )
         if safe and (new_key in self.keys(include_nested=True)):
             raise KeyError(f"key {new_key} already present in TensorDict.")
-        if self.is_locked:
-            raise RuntimeError(TensorDictBase.LOCK_ERROR)
 
         if isinstance(new_key, tuple):
             td, subkey = _get_leaf_tensordict(self, new_key)
         else:
             td, subkey = self, new_key
         td._set(subkey, self.get(old_key))
         self.del_(old_key)
@@ -3864,55 +4204,55 @@
             tensor_in = _sub_index(tensor_in, idx)
             tensor_in.copy_(value)
         else:
             _set_item(tensor_in, idx, value)
 
         return self
 
-    def get(
-        self, key: NestedKey, default: str | CompatibleType = NO_DEFAULT
-    ) -> CompatibleType:
-        key = unravel_keys(key)
+    def _get_str(self, key, default):
+        first_key = key
+        out = self._tensordict.get(first_key, None)
+        if out is None:
+            return self._default_get(first_key, default)
+        return out
 
+    def _get_tuple(self, key, default):
+        first = self._get_str(key[0], None)
+        if first is None:
+            return self._default_get(key[0], default)
+        if len(key) == 1:
+            return first
         try:
-            if isinstance(key, tuple):
-                if len(key) > 1:
-                    first_lev = self.get(key[0])
-                    if len(key) == 2 and isinstance(first_lev, KeyedJaggedTensor):
-                        return first_lev[key[1]]
-                    try:
-                        return first_lev.get(key[1:])
-                    except AttributeError as err:
-                        if "has no attribute" in str(err):
-                            raise ValueError(
-                                f"Expected a TensorDictBase instance but got {type(first_lev)} instead"
-                                f" for key '{key[0]}' and subkeys {key[1:]} in tensordict:\n{self}."
-                            )
-                return self.get(key[0])
-            return self._tensordict[key]
-        except KeyError:
-            # this is slower than a if / else but (1) it allows to avoid checking
-            # that the key is present and (2) it should be used less frequently than
-            # the regular get()
-            return self._default_get(key, default)
+            if isinstance(first, KeyedJaggedTensor):
+                if len(key) != 2:
+                    raise ValueError(f"Got too many keys for a KJT: {key}.")
+                return first[key[1]]
+            else:
+                return first._get_tuple(key[1:], default=default)
+        except AttributeError as err:
+            if "has no attribute" in str(err):
+                raise ValueError(
+                    f"Expected a TensorDictBase instance but got {type(first)} instead"
+                    f" for key '{key[1:]}' in tensordict:\n{self}."
+                )
 
     def share_memory_(self) -> TensorDictBase:
         if self.is_memmap():
             raise RuntimeError(
                 "memmap and shared memory are mutually exclusive features."
             )
         if self.device is not None and self.device.type == "cuda":
             # cuda tensors are shared by default
             return self
         for value in self.values():
             # no need to consider MemmapTensors here as we have checked that this is not a memmap-tensordict
             if (
                 isinstance(value, Tensor)
                 and value.device.type == "cpu"
-                or is_tensor_collection(value)
+                or _is_tensor_collection(value.__class__)
             ):
                 value.share_memory_()
         self._is_shared = True
         self.lock_()
         return self
 
     def detach_(self) -> TensorDictBase:
@@ -3924,15 +4264,15 @@
         self,
         prefix: str | None = None,
         copy_existing: bool = False,
     ) -> TensorDictBase:
         if prefix is not None:
             prefix = Path(prefix)
             if not prefix.exists():
-                prefix.mkdir(exist_ok=True)
+                os.makedirs(prefix, exist_ok=True)
             torch.save(
                 {"batch_size": self.batch_size, "device": self.device},
                 prefix / "meta.pt",
             )
         if self.is_shared() and self.device.type == "cpu":
             raise RuntimeError(
                 "memmap and shared memory are mutually exclusive features."
@@ -3943,18 +4283,18 @@
                 "populated. Set a tensor first."
             )
         for key, value in self.items():
             if value.requires_grad:
                 raise Exception(
                     "memmap is not compatible with gradients, one of Tensors has requires_grad equals True"
                 )
-            if is_tensor_collection(value):
+            if _is_tensor_collection(value.__class__):
                 if prefix is not None:
                     # ensure subdirectory exists
-                    (prefix / key).mkdir(exist_ok=True)
+                    os.makedirs(prefix / key, exist_ok=True)
                     self._tensordict[key] = value.memmap_(
                         prefix=prefix / key, copy_existing=copy_existing
                     )
                     torch.save(
                         {"batch_size": value.batch_size, "device": value.device},
                         prefix / key / "meta.pt",
                     )
@@ -4014,15 +4354,17 @@
                 if path == prefix / "meta.pt":
                     # skip prefix / "meta.pt" as we've already read it
                     continue
                 key = key[:-1]  # drop "meta.pt" from key
                 metadata = torch.load(path)
                 if key in out.keys(include_nested=True):
                     out[key].batch_size = metadata["batch_size"]
-                    out[key] = out[key].to(metadata["device"])
+                    device = metadata["device"]
+                    if device is not None:
+                        out[key] = out[key].to(device)
                 else:
                     out[key] = cls(
                         {}, batch_size=metadata["batch_size"], device=metadata["device"]
                     )
             else:
                 leaf, *_ = key[-1].rsplit(".", 2)  # remove .meta.pt suffix
                 key = (*key[:-1], leaf)
@@ -4037,30 +4379,32 @@
         return out
 
     def to(self, dest: DeviceType | torch.Size | type, **kwargs: Any) -> TensorDictBase:
         if isinstance(dest, type) and issubclass(dest, TensorDictBase):
             if isinstance(self, dest):
                 return self
             td = dest(source=self, **kwargs)
-            if self._names is not None:
-                td.names = self._names
+            if self._td_dim_names is not None:
+                td.names = self._td_dim_names
             return td
         elif isinstance(dest, (torch.device, str, int)):
             # must be device
             dest = torch.device(dest)
             if self.device is not None and dest == self.device:
                 return self
 
             def to(tensor):
                 return tensor.to(dest, **kwargs)
 
             return self.apply(to, device=dest)
         elif isinstance(dest, torch.Size):
             self.batch_size = dest
             return self
+        elif dest is None:
+            return self
         else:
             raise NotImplementedError(
                 f"dest must be a string, torch.device or a TensorDict "
                 f"instance, {dest} not allowed"
             )
 
     def masked_fill_(self, mask: Tensor, value: float | int | bool) -> TensorDictBase:
@@ -4077,15 +4421,15 @@
         return all([value.is_contiguous() for _, value in self.items()])
 
     def clone(self, recurse: bool = True) -> TensorDictBase:
         return TensorDict(
             source={key: _clone_value(value, recurse) for key, value in self.items()},
             batch_size=self.batch_size,
             device=self.device,
-            names=copy(self._names),
+            names=copy(self._td_dim_names),
             _run_checks=False,
             _is_shared=self.is_shared() if not recurse else False,
             _is_memmap=self.is_memmap() if not recurse else False,
         )
 
     def contiguous(self) -> TensorDictBase:
         if not self.is_contiguous():
@@ -4121,37 +4465,73 @@
                         *val, strict=strict, inplace=inplace
                     )
 
         out = TensorDict(
             device=self.device,
             batch_size=self.batch_size,
             source=source,
-            names=self._names,
+            names=self._td_dim_names,
             _run_checks=False,
             _is_memmap=self._is_memmap,
             _is_shared=self._is_shared,
         )
         if inplace:
             self._tensordict = out._tensordict
             return self
         return out
 
     def keys(
         self, include_nested: bool = False, leaves_only: bool = False
     ) -> _TensorDictKeysView:
+        if not include_nested and not leaves_only:
+            return self._tensordict.keys()
+        else:
+            return self._nested_keys(
+                include_nested=include_nested, leaves_only=leaves_only
+            )
+
+    # @cache  # noqa: B019
+    def _nested_keys(
+        self, include_nested: bool = False, leaves_only: bool = False
+    ) -> _TensorDictKeysView:
         return _TensorDictKeysView(
             self, include_nested=include_nested, leaves_only=leaves_only
         )
 
     def __getstate__(self):
-        return {slot: getattr(self, slot) for slot in self.__slots__}
+        return {
+            slot: getattr(self, slot)
+            for slot in self.__slots__
+            if slot not in ("_last_op", "_cache", "_last_op_queue")
+        }
 
     def __setstate__(self, state):
         for slot, value in state.items():
             setattr(self, slot, value)
+        self._cache = None
+        self._last_op = collections.deque()
+
+    # some custom methods for efficiency
+    def items(
+        self, include_nested: bool = False, leaves_only: bool = False
+    ) -> Iterator[tuple[str, CompatibleType]]:
+        if not include_nested and not leaves_only:
+            return self._tensordict.items()
+        else:
+            return super().items(include_nested=include_nested, leaves_only=leaves_only)
+
+    def values(
+        self, include_nested: bool = False, leaves_only: bool = False
+    ) -> Iterator[tuple[str, CompatibleType]]:
+        if not include_nested and not leaves_only:
+            return self._tensordict.values()
+        else:
+            return super().values(
+                include_nested=include_nested, leaves_only=leaves_only
+            )
 
 
 class _ErrorInteceptor:
     """Context manager for catching errors and modifying message.
 
     Intended for use with stacking / concatenation operations applied to TensorDicts.
 
@@ -4207,14 +4587,19 @@
         elif prefix:
             yield (*prefix, key)
         else:
             yield key
 
 
 def _default_hook(td: TensorDictBase, k: tuple[str, ...]) -> None:
+    """Used to populate a tensordict.
+
+    For example, ``td.set(("a", "b"))`` may require to create ``"a"``.
+
+    """
     out = td.get(k[0], None)
     if out is None:
         out = td.select()
         if td.is_locked:
             raise RuntimeError(TensorDictBase.LOCK_ERROR)
         td._set(k[0], out)
     return out
@@ -4251,29 +4636,31 @@
     expected: TensorDictBase,
     rtol: float | None = None,
     atol: float | None = None,
     equal_nan: bool = True,
     msg: str = "",
 ) -> bool:
     """Compares two tensordicts and raise an exception if their content does not match exactly."""
-    if not is_tensor_collection(actual) or not is_tensor_collection(expected):
+    if not _is_tensor_collection(actual.__class__) or not _is_tensor_collection(
+        expected.__class__
+    ):
         raise TypeError("assert_allclose inputs must be of TensorDict type")
     set1 = set(actual.keys())
     set2 = set(expected.keys())
     if not (len(set1.difference(set2)) == 0 and len(set2) == len(set1)):
         raise KeyError(
             "actual and expected tensordict keys mismatch, "
             f"keys {(set1 - set2).union(set2 - set1)} appear in one but not "
             f"the other."
         )
     keys = sorted(actual.keys(), key=str)
     for key in keys:
         input1 = actual.get(key)
         input2 = expected.get(key)
-        if is_tensor_collection(input1):
+        if _is_tensor_collection(input1.__class__):
             assert_allclose_td(input1, input2, rtol=rtol, atol=atol)
             continue
 
         mse = (input1.to(torch.float) - input2.to(torch.float)).pow(2).sum()
         mse = mse.div(input1.numel()).sqrt().item()
 
         default_msg = f"key {key} does not match, got mse = {mse:4.4f}"
@@ -4327,15 +4714,15 @@
         target_shape[dim] = index_expand.shape[dim]
         index_expand = index_expand.expand(target_shape)
         out = torch.gather(tensor, dim, index_expand, out=dest)
         return out
 
     if out is None:
 
-        names = input._names
+        names = input._td_dim_names
 
         return TensorDict(
             {key: _gather_tensor(value) for key, value in input.items()},
             batch_size=index.shape,
             names=names,
         )
     TensorDict(
@@ -4360,17 +4747,15 @@
             f"supported with full_like with TensorDict"
         )
     return td_clone
 
 
 @implements_for_td(torch.zeros_like)
 def _zeros_like(td: TensorDictBase, **kwargs: Any) -> TensorDictBase:
-    td_clone = td.clone()
-    for key in td_clone.keys():
-        td_clone.fill_(key, 0.0)
+    td_clone = td.apply(torch.zeros_like)
     if "dtype" in kwargs:
         raise ValueError("Cannot pass dtype to full_like with TensorDict")
     if "device" in kwargs:
         td_clone = td_clone.to(kwargs.pop("device"))
     if len(kwargs):
         raise RuntimeError(
             f"keyword arguments {list(kwargs.keys())} are not "
@@ -4467,15 +4852,15 @@
             device = list_of_tensordicts[0].device
             for td in list_of_tensordicts[1:]:
                 if device == td.device:
                     continue
                 else:
                     device = None
                     break
-        names = list_of_tensordicts[0]._names
+        names = list_of_tensordicts[0]._td_dim_names
         return TensorDict(
             out, device=device, batch_size=batch_size, _run_checks=False, names=names
         )
     else:
         if out.batch_size != batch_size:
             raise RuntimeError(
                 "out.batch_size and cat batch size must match, "
@@ -4662,15 +5047,15 @@
         {}, torch.Size(new_batch_size), device=tensordict.device, _run_checks=False
     )
     for key, tensor in tensordict.items():
         cur_pad = reverse_pad
         if len(pad_size) < len(_shape(tensor)) * 2:
             cur_pad = [0] * (len(_shape(tensor)) * 2 - len(pad_size)) + reverse_pad
 
-        if is_tensor_collection(tensor):
+        if _is_tensor_collection(tensor.__class__):
             padded = pad(tensor, pad_size, value)
         else:
             padded = torch.nn.functional.pad(tensor, cur_pad, value=value)
         out.set(key, padded)
 
     return out
 
@@ -4828,15 +5213,15 @@
 
     def __init__(
         self,
         source: TensorDictBase,
         idx: IndexType,
         batch_size: Sequence[int] | None = None,
     ) -> None:
-        if not isinstance(source, TensorDictBase):
+        if not _is_tensor_collection(source.__class__):
             raise TypeError(
                 f"Expected source to be a subclass of TensorDictBase, "
                 f"got {type(source)}"
             )
         self._source = source
         idx = (
             (idx,)
@@ -4879,23 +5264,15 @@
                     new_idx.append(_idx)
             return tuple(new_idx)
         return idx
 
     def exclude(self, *keys: str, inplace: bool = False) -> TensorDictBase:
         if inplace:
             return super().exclude(*keys, inplace=True)
-        return TensorDict(
-            {key: value for key, value in self.items()},
-            batch_size=self.batch_size,
-            device=self.device,
-            names=self._names,
-            _run_checks=False,
-            _is_memmap=self.is_memmap(),
-            _is_shared=self.is_shared(),
-        ).exclude(*keys, inplace=True)
+        return self.to_tensordict().exclude(*keys, inplace=True)
 
     @property
     def batch_size(self) -> torch.Size:
         return self._batch_size
 
     @batch_size.setter
     def batch_size(self, new_size: torch.Size) -> None:
@@ -4905,26 +5282,26 @@
     def names(self):
         names = self._source._get_names_idx(self.idx)
         if names is None:
             return [None] * self.batch_dims
         return names
 
     @property
-    def _names(self):
+    def _td_dim_names(self):
         return self.names
 
     @names.setter
     def names(self, value):
         raise RuntimeError(
             "Names of a subtensordict cannot be modified. Instantiate the tensordict first."
         )
 
     def _rename_subtds(self, names):
         for key in self.keys():
-            if is_tensor_collection(self.entry_class(key)):
+            if _is_tensor_collection(self.entry_class(key)):
                 raise RuntimeError("Cannot rename nested sub-tensordict dimensions.")
 
     @property
     def device(self) -> None | torch.device:
         return self._source.device
 
     @device.setter
@@ -4985,20 +5362,24 @@
         self,
         key: NestedKey,
         tensor: dict[str, CompatibleType] | CompatibleType,
         inplace: bool = False,
     ) -> TensorDictBase:
         key = self._validate_key(key)
 
-        if isinstance(key, tuple):
-            parent = self.get_parent_tensordict()
-            subparent, subkey = _get_leaf_tensordict(parent, key, _default_hook)
-            subparent.get_sub_tensordict(self.idx).set(subkey, tensor, inplace=inplace)
+        if isinstance(key, tuple) and len(key) > 1:
+            source = self._source
+            td = source._get_str(key[0], None)
+            if td is None:
+                source.create_nested(key[0])
+            td = self._get_str(key[0], NO_DEFAULT)
+            td.set(key[1:], tensor, inplace)
             return self
-
+        elif isinstance(key, tuple):
+            key = key[0]
         key_present = key in self.keys()
         inplace = inplace and key_present
         if not inplace:
             if self.is_locked:
                 raise RuntimeError(TensorDictBase.LOCK_ERROR)
             if key_present:
                 raise RuntimeError(
@@ -5030,22 +5411,23 @@
             raise KeyError(
                 f'key "{key}" not found in tensordict, '
                 f'call td.set("{key}", value) for populating tensordict with '
                 f"new key-value pair"
             ) from e
         return self
 
+    # @cache  # noqa: B019
     def keys(
         self, include_nested: bool = False, leaves_only: bool = False
     ) -> _TensorDictKeysView:
         return self._source.keys(include_nested=include_nested, leaves_only=leaves_only)
 
     def entry_class(self, key: NestedKey) -> type:
         source_type = type(self._source.get(key))
-        if is_tensor_collection(source_type):
+        if _is_tensor_collection(source_type):
             return self.__class__
         return source_type
 
     def _stack_onto_(
         self, key: str, list_item: list[CompatibleType], dim: int
     ) -> SubTensorDict:
         self._source._stack_onto_at_(key, list_item, dim=dim, idx=self.idx)
@@ -5054,29 +5436,31 @@
     def to(self, dest: DeviceType | torch.Size | type, **kwargs: Any) -> TensorDictBase:
         if isinstance(dest, type) and issubclass(dest, TensorDictBase):
             if isinstance(self, dest):
                 return self
             out = dest(
                 source=self.clone(),
             )
-            if self._names is not None:
-                out.names = self._names
+            if self._td_dim_names is not None:
+                out.names = self._td_dim_names
             return out
         elif isinstance(dest, (torch.device, str, int)):
             dest = torch.device(dest)
             # try:
             if self.device is not None and dest == self.device:
                 return self
             td = self.to_tensordict().to(dest, **kwargs)
             # must be device
             return td
 
         elif isinstance(dest, torch.Size):
             self.batch_size = dest
             return self
+        elif dest is None:
+            return self
         else:
             raise NotImplementedError(
                 f"dest must be a string, torch.device or a TensorDict "
                 f"instance, {dest} not allowed"
             )
 
     def _change_batch_size(self, new_size: torch.Size) -> None:
@@ -5089,14 +5473,22 @@
     def get(
         self,
         key: NestedKey,
         default: Tensor | str | None = NO_DEFAULT,
     ) -> CompatibleType:
         return self._source.get_at(key, self.idx, default=default)
 
+    def _get_str(self, key, default):
+        if key in self.keys() and _is_tensor_collection(self.entry_class(key)):
+            return SubTensorDict(self._source._get_str(key, NO_DEFAULT), self.idx)
+        return self._source._get_at_str(key, self.idx, default=default)
+
+    def _get_tuple(self, key, default):
+        return self._source._get_tuple(key, self.idx, default=default)
+
     def set_at_(
         self,
         key: NestedKey,
         value: dict[str, CompatibleType] | CompatibleType,
         idx: IndexType,
         discard_idx_attr: bool = False,
     ) -> SubTensorDict:
@@ -5108,31 +5500,14 @@
             self._source.set_at_(key, value, idx)
         else:
             tensor = self._source.get_at(key, self.idx)
             tensor[idx] = value
             self._source.set_at_(key, tensor, self.idx)
         return self
 
-    def get_at(
-        self,
-        key: str,
-        idx: IndexType,
-        discard_idx_attr: bool = False,
-        default: Tensor | str | None = NO_DEFAULT,
-    ) -> CompatibleType:
-        if not isinstance(idx, tuple):
-            idx = (idx,)
-        if discard_idx_attr:
-            return self._source.get_at(key, idx, default=default)
-        else:
-            out = self._source.get_at(key, self.idx, default=default)
-            if out is default:
-                return out
-            return out[idx]
-
     def update(
         self,
         input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
         clone: bool = False,
         inplace: bool = False,
         **kwargs,
     ) -> SubTensorDict:
@@ -5148,15 +5523,15 @@
             if isinstance(key, tuple):
                 key, subkey = key[0], key[1:]
             else:
                 subkey = []
             # the key must be a string by now. Let's check if it is present
             if key in keys:
                 target_class = self.entry_class(key)
-                if is_tensor_collection(target_class):
+                if _is_tensor_collection(target_class):
                     target = self._source.get(key).get_sub_tensordict(self.idx)
                     if len(subkey):
                         target.update({subkey: value})
                         continue
                     elif isinstance(value, (dict, TensorDictBase)):
                         target.update(value)
                         continue
@@ -5331,14 +5706,59 @@
         )
 
     def share_memory_(self) -> TensorDictBase:
         raise RuntimeError(
             "Casting a sub-tensordict values to shared memory cannot be done."
         )
 
+    @property
+    def is_locked(self) -> bool:
+        return self._source.is_locked
+
+    @is_locked.setter
+    def is_locked(self, value) -> bool:
+        if value:
+            self.lock_()
+        else:
+            self.unlock_()
+
+    @as_decorator("is_locked")
+    def lock_(self) -> TensorDictBase:
+        # we can't lock sub-tensordicts because that would mean that the
+        # parent tensordict cannot be modified either.
+        if not self.is_locked:
+            raise RuntimeError(
+                "Cannot lock a SubTensorDict. Lock the parent tensordict instead."
+            )
+        return self
+
+    @as_decorator("is_locked")
+    def unlock_(self) -> TensorDictBase:
+        if self.is_locked:
+            raise RuntimeError(
+                "Cannot unlock a SubTensorDict. Unlock the parent tensordict instead."
+            )
+        return self
+
+    def _remove_lock(self, lock_id):
+        raise RuntimeError(
+            "Cannot unlock a SubTensorDict. Unlock the parent tensordict instead."
+        )
+
+    def _lock_propagate(self, lock_ids=None):
+        raise RuntimeError(
+            "Cannot lock a SubTensorDict. Lock the parent tensordict instead."
+        )
+
+    lock = _renamed_inplace_method(lock_)
+    unlock = _renamed_inplace_method(unlock_)
+
+    def __del__(self):
+        pass
+
 
 def merge_tensordicts(*tensordicts: TensorDictBase) -> TensorDictBase:
     """Merges tensordicts together."""
     if len(tensordicts) < 2:
         raise RuntimeError(
             f"at least 2 tensordicts must be provided, got" f" {len(tensordicts)}"
         )
@@ -5354,28 +5774,50 @@
 class _LazyStackedTensorDictKeysView(_TensorDictKeysView):
     def __len__(self) -> int:
         return len(self.tensordict.valid_keys)
 
     def _keys(self) -> list[str]:
         return self.tensordict.valid_keys
 
+    def __contains__(self, item):
+        item = unravel_keys(item)
+        if isinstance(item, str):
+            if item in self._keys():
+                if self.leaves_only:
+                    return not _is_tensor_collection(self.tensordict.entry_class(item))
+                return True
+        elif len(item) == 1:
+            if item[0] in self._keys():
+                if self.leaves_only:
+                    return not _is_tensor_collection(
+                        self.tensordict.entry_class(item[0])
+                    )
+                return True
+        # otherwise take the long way
+        return all(
+            item in tensordict.keys(self.include_nested, self.leaves_only)
+            for tensordict in self.tensordict.tensordicts
+        )
+
 
 class LazyStackedTensorDict(TensorDictBase):
     """A Lazy stack of TensorDicts.
 
     When stacking TensorDicts together, the default behaviour is to put them
     in a stack that is not instantiated.
     This allows to seamlessly work with stacks of tensordicts with operations
     that will affect the original tensordicts.
 
     Args:
          *tensordicts (TensorDict instances): a list of tensordict with
             same batch size.
          stack_dim (int): a dimension (between `-td.ndimension()` and
             `td.ndimension()-1` along which the stack should be performed.
+         hook_out (callable, optional): a callable to execute after :meth:`~.get`.
+         hook_in (callable, optional): a callable to execute before :meth:`~.set`.
 
     Examples:
         >>> from tensordict import TensorDict
         >>> import torch
         >>> tds = [TensorDict({'a': torch.randn(3, 4)}, batch_size=[3])
         ...     for _ in range(10)]
         >>> td_stack = torch.stack(tds, -1)
@@ -5385,25 +5827,28 @@
         torch.Size([3, 10, 4])
         >>> print(td_stack[:, 0] is tds[0])
         True
 
     """
 
     def __new__(cls, *args: Any, **kwargs: Any) -> LazyStackedTensorDict:
-        cls._names = None
+        cls._td_dim_names = None
         return super().__new__(cls, *args, _safe=False, _lazy=True, **kwargs)
 
     def __init__(
         self,
         *tensordicts: TensorDictBase,
         stack_dim: int = 0,
+        hook_out: callable | None = None,
+        hook_in: callable | None = None,
         batch_size: Sequence[int] | None = None,  # TODO: remove
     ) -> None:
         self._is_shared = False
         self._is_memmap = False
+        self._is_locked = None
 
         # sanity check
         N = len(tensordicts)
         if not N:
             raise RuntimeError(
                 "at least one tensordict must be provided to "
                 "StackedTensorDict to be instantiated"
@@ -5436,14 +5881,16 @@
                     f"cannot be created. Got td[0].batch_size={_batch_size} "
                     f"and td[i].batch_size={_bs} "
                 )
         self.tensordicts: list[TensorDictBase] = list(tensordicts)
         self.stack_dim = stack_dim
         self._batch_size = self._compute_batch_size(_batch_size, stack_dim, N)
         self._update_valid_keys()
+        self.hook_out = hook_out
+        self.hook_in = hook_in
         if batch_size is not None and batch_size != self.batch_size:
             raise RuntimeError("batch_size does not match self.batch_size.")
 
     @property
     def device(self) -> torch.device | None:
         # devices might have changed, so we check that they're all the same
         device_set = {td.device for td in self.tensordicts}
@@ -5465,29 +5912,30 @@
 
     @batch_size.setter
     def batch_size(self, new_size: torch.Size) -> None:
         return self._batch_size_setter(new_size)
 
     @property
     def names(self):
-        if self._names is None:
+        if self._td_dim_names is None:
             names = copy(self.tensordicts[0].names)
             names.insert(self.stack_dim, None)
-            self._names = names
-        return self._names
+            self._td_dim_names = names
+        return self._td_dim_names
 
     @names.setter
+    @erase_cache  # a nested lazy stacked tensordict is not apparent to the root
     def names(self, value):
         if value is None:
             for td in self.tensordicts:
                 td.names = None
-            self._names = None
+            self._td_dim_names = None
         else:
             names_c = list(value)
-            self._names = copy(names_c)
+            self._td_dim_names = copy(names_c)
             name = names_c[self.stack_dim]
             names_c = list(names_c)
             del names_c[self.stack_dim]
             for td in self.tensordicts:
                 if td._check_dim_name(name):
                     raise ValueError(f"The dimension name {name} is already taken.")
                 td.rename_(*names_c)
@@ -5595,14 +6043,16 @@
     ) -> TensorDictBase:
         key = self._validate_key(key)
         # we don't need this as locked lazy stacks have locked nested tds so the error will be captured in the loop
         # if self.is_locked:
         #     raise RuntimeError(TensorDictBase.LOCK_ERROR)
 
         tensor = self._validate_value(tensor)
+        if self.hook_in is not None:
+            tensor = self.hook_in(tensor)
         for td, _item in zip(self.tensordicts, tensor.unbind(self.stack_dim)):
             td.set(key, _item, inplace=inplace)
 
         first_key = key if (isinstance(key, str)) else key[0]
         if key not in self._valid_keys:
             self._valid_keys = sorted([*self._valid_keys, first_key], key=str)
 
@@ -5618,14 +6068,72 @@
         except KeyError as e:
             raise KeyError(
                 "setting a value in-place on a stack of TensorDict is only "
                 "permitted if all members of the stack have this key in "
                 "their register."
             ) from e
 
+    def unsqueeze(self, dim: int) -> TensorDictBase:
+        if dim < 0:
+            dim = self.batch_dims + dim + 1
+
+        if (dim > self.batch_dims) or (dim < 0):
+            raise RuntimeError(
+                f"unsqueezing is allowed for dims comprised between "
+                f"`-td.batch_dims` and `td.batch_dims` only. Got "
+                f"dim={dim} with a batch size of {self.batch_size}."
+            )
+        if dim <= self.stack_dim:
+            stack_dim = self.stack_dim + 1
+        else:
+            dim = dim - 1
+            stack_dim = self.stack_dim
+        return LazyStackedTensorDict(
+            *(tensordict.unsqueeze(dim) for tensordict in self.tensordicts),
+            stack_dim=stack_dim,
+        )
+
+    def squeeze(self, dim: int | None = None) -> TensorDictBase:
+        """Squeezes all tensors for a dimension comprised in between `-td.batch_dims+1` and `td.batch_dims-1` and returns them in a new tensordict.
+
+        Args:
+            dim (Optional[int]): dimension along which to squeeze. If dim is None, all singleton dimensions will be squeezed. dim is None by default.
+
+        """
+        if dim is None:
+            size = self.size()
+            if len(self.size()) == 1 or size.count(1) == 0:
+                return self
+            first_singleton_dim = size.index(1)
+            return self.squeeze(first_singleton_dim).squeeze()
+
+        if dim < 0:
+            dim = self.batch_dims + dim
+
+        if self.batch_dims and (dim >= self.batch_dims or dim < 0):
+            raise RuntimeError(
+                f"squeezing is allowed for dims comprised between 0 and "
+                f"td.batch_dims only. Got dim={dim} and batch_size"
+                f"={self.batch_size}."
+            )
+
+        if dim >= self.batch_dims or self.batch_size[dim] != 1:
+            return self
+        if dim == self.stack_dim:
+            return self.tensordicts[0]
+        elif dim < self.stack_dim:
+            stack_dim = self.stack_dim - 1
+        else:
+            dim = dim - 1
+            stack_dim = self.stack_dim
+        return LazyStackedTensorDict(
+            *(tensordict.squeeze(dim) for tensordict in self.tensordicts),
+            stack_dim=stack_dim,
+        )
+
     def unbind(self, dim: int) -> tuple[TensorDictBase, ...]:
         if dim < 0:
             dim = self.batch_dims + dim
         if dim == self.stack_dim:
             return tuple(self.tensordicts)
         else:
             return super().unbind(dim)
@@ -5649,48 +6157,50 @@
             for source, tensordict_dest in zip(list_item, self.tensordicts):
                 tensordict_dest.set_(key, source)
         else:
             # we must stack and unbind, there is no way to make it more efficient
             self.set_(key, torch.stack(list_item, dim))
         return self
 
-    def get(
+    @cache  # noqa: B019
+    def _get_str(
         self,
         key: NestedKey,
         default: str | CompatibleType = NO_DEFAULT,
     ) -> CompatibleType:
         # TODO: the stacking logic below works for nested keys, but the key in
         # self.valid_keys check will fail and we'll return the default instead.
         # For now we'll advise user that nested keys aren't supported, but it should be
         # fairly easy to add support if we could add nested keys to valid_keys.
 
         # we can handle the case where the key is a tuple of length 1
-        if (isinstance(key, tuple)) and len(key) == 1:
-            key = key[0]
-        elif isinstance(key, tuple):
-            try:
-                tensordict, key = _get_leaf_tensordict(self, key)
-            except KeyError:
-                return self._default_get(key, default)
-            return tensordict.get(key, default=default)
-
         keys = self.valid_keys
         if key not in keys:
             # first, let's try to update the valid keys
             self._update_valid_keys()
             keys = self.valid_keys
 
         if key not in keys:
             return self._default_get(key, default)
 
         tensors = [td.get(key, default=default) for td in self.tensordicts]
         try:
             out = torch.stack(tensors, self.stack_dim)
-            if is_tensor_collection(out) and self._names is not None:
-                out.refine_names(*self.names, *out.names[self.ndim :])
+            if _is_tensor_collection(out.__class__):
+                if self._td_dim_names is not None:
+                    out.refine_names(*self.names, *out.names[self.ndim :])
+                if isinstance(out, TensorDictBase):
+                    out.hook_out = self.hook_out
+                    out.hook_in = self.hook_in
+                else:
+                    # then it's a tensorclass
+                    out._tensordict.hook_out = self.hook_out
+                    out._tensordict.hook_in = self.hook_in
+            elif self.hook_out is not None:
+                out = self.hook_out(out)
             return out
         except RuntimeError as err:
             if "stack expects each tensor to be equal size" in str(err):
                 shapes = {_shape(tensor) for tensor in tensors}
                 raise RuntimeError(
                     f"Found more than one unique shape in the tensors to be "
                     f"stacked ({shapes}). This is likely due to a modification "
@@ -5698,22 +6208,130 @@
                     f"updated/created with an uncompatible shape. If the entries "
                     f"are intended to have a different shape, use the get_nestedtensor "
                     f"method instead."
                 )
             else:
                 raise err
 
-    def get_at(self, key, index, default=NO_DEFAULT):
-        item = self.get(key, default=default)
-        if item is default and default is not NO_DEFAULT:
-            return item
-        if isinstance(item, TensorDictBase):
-            return SubTensorDict(item, index)
+    def _get_tuple(self, key, default):
+        first = self._get_str(key[0], None)
+        if first is None:
+            return self._default_get(first, default)
+        if len(key) == 1:
+            return first
+        try:
+            if isinstance(first, KeyedJaggedTensor):
+                if len(key) != 2:
+                    raise ValueError(f"Got too many keys for a KJT: {key}.")
+                return first[key[-1]]
+            else:
+                return first._get_tuple(key[1:], default=default)
+        except AttributeError as err:
+            if "has no attribute" in str(err):
+                raise ValueError(
+                    f"Expected a TensorDictBase instance but got {type(first)} instead"
+                    f" for key '{key[1:]}' in tensordict:\n{self}."
+                )
+
+    @cache  # noqa: B019
+    def _add_batch_dim(self, *, in_dim, vmap_level):
+        if self.is_memmap():
+            td = torch.stack([td.cpu().as_tensor() for td in self.tensordicts], 0)
+        else:
+            td = self
+        if in_dim < 0:
+            in_dim = self.ndim + in_dim
+        if in_dim == self.stack_dim:
+            return self._cached_add_batch_dims(td, in_dim=in_dim, vmap_level=vmap_level)
+        if in_dim < td.stack_dim:
+            # then we'll stack along a dim before
+            stack_dim = td.stack_dim - 1
+        else:
+            in_dim = in_dim - 1
+            stack_dim = td.stack_dim
+        tds = [
+            td.apply(
+                lambda _arg: _add_batch_dim(_arg, in_dim, vmap_level),
+                batch_size=[b for i, b in enumerate(td.batch_size) if i != in_dim],
+                names=[name for i, name in enumerate(td.names) if i != in_dim],
+            )
+            for td in td.tensordicts
+        ]
+        return LazyStackedTensorDict(*tds, stack_dim=stack_dim)
+
+    @classmethod
+    def _cached_add_batch_dims(cls, td, in_dim, vmap_level):
+        # we return a stack with hook_out, and hack the batch_size and names
+        # Per se it is still a LazyStack but the stacking dim is "hidden" from
+        # the outside
+        out = td.clone(False)
+
+        def hook_out(tensor, in_dim=in_dim, vmap_level=vmap_level):
+            return _add_batch_dim(tensor, in_dim, vmap_level)
+
+        n = len(td.tensordicts)
+
+        def hook_in(
+            tensor,
+            out_dim=in_dim,
+            batch_size=n,
+            vmap_level=vmap_level,
+        ):
+            return _remove_batch_dim(tensor, vmap_level, batch_size, out_dim)
+
+        out.hook_out = hook_out
+        out.hook_in = hook_in
+        out._batch_size = torch.Size(
+            [dim for i, dim in enumerate(out._batch_size) if i != out.stack_dim]
+        )
+        if out._td_dim_names is not None:
+            out._td_dim_names = [
+                name for i, name in enumerate(out._td_dim_names) if i != out.stack_dim
+            ]
         else:
-            return item[index]
+            out._td_dim_names = [None] * out.ndim
+        return out
+
+    @cache  # noqa: B019
+    def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
+        if self.hook_out is not None:
+            # this is the hacked version. We just need to remove the hook_out and
+            # reset a proper batch size
+            return LazyStackedTensorDict(
+                *self.tensordicts,
+                stack_dim=out_dim,
+            )
+            # return self._cache_remove_batch_dim(vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim)
+        else:
+            # we must call _remove_batch_dim on all tensordicts
+            # batch_size: size of the batch when we unhide it.
+            # out_dim: dimension where the output will be found
+            new_batch_size = list(self.batch_size)
+            new_batch_size.insert(out_dim, batch_size)
+            new_names = list(self.names)
+            new_names.insert(out_dim, None)
+            # rebuild the lazy stack
+            # the stack dim is the same if the out_dim is past it, but it
+            # must be incremented by one otherwise.
+            # In the first case, the out_dim must be decremented by one
+            if out_dim > self.stack_dim:
+                stack_dim = self.stack_dim
+                out_dim = out_dim - 1
+            else:
+                stack_dim = self.stack_dim + 1
+            out = LazyStackedTensorDict(
+                *[
+                    td._remove_batch_dim(
+                        vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim
+                    )
+                    for td in self.tensordicts
+                ],
+                stack_dim=stack_dim,
+            )
+        return out
 
     def get_nestedtensor(
         self,
         key: NestedKey,
         default: str | CompatibleType = NO_DEFAULT,
     ) -> CompatibleType:
         # disallow getting nested tensor if the stacking dimension is not 0
@@ -5773,41 +6391,43 @@
                 stack_dim=self.stack_dim,
             )
         else:
             out = LazyStackedTensorDict(
                 *[td.clone(recurse=False) for td in self.tensordicts],
                 stack_dim=self.stack_dim,
             )
-        if self._names is not None:
+        if self._td_dim_names is not None:
             out.names = self.names
         return out
 
     def pin_memory(self) -> TensorDictBase:
         for td in self.tensordicts:
             td.pin_memory()
         return self
 
     def to(self, dest: DeviceType | type, **kwargs) -> TensorDictBase:
         if isinstance(dest, type) and issubclass(dest, TensorDictBase):
             if isinstance(self, dest):
                 return self
             kwargs.update({"batch_size": self.batch_size})
             out = dest(source=self, **kwargs)
-            if self._names is not None:
-                out.names = self._names
+            if self._td_dim_names is not None:
+                out.names = self._td_dim_names
             return out
         elif isinstance(dest, (torch.device, str, int)):
             dest = torch.device(dest)
             if self.device is not None and dest == self.device:
                 return self
             td = self.to_tensordict().to(dest, **kwargs)
             return td
 
         elif isinstance(dest, torch.Size):
             self.batch_size = dest
+        elif dest is None:
+            return self
         else:
             raise NotImplementedError(
                 f"dest must be a string, torch.device or a TensorDict "
                 f"instance, {dest} not allowed"
             )
 
     def _check_new_batch_size(self, new_size: torch.Size) -> None:
@@ -5822,14 +6442,15 @@
     def _change_batch_size(self, new_size: torch.Size) -> None:
         if not hasattr(self, "_orig_batch_size"):
             self._orig_batch_size = self.batch_size
         elif self._orig_batch_size == new_size:
             del self._orig_batch_size
         self._batch_size = new_size
 
+    # @cache  # noqa: B019
     def keys(
         self, include_nested: bool = False, leaves_only: bool = False
     ) -> _LazyStackedTensorDictKeysView:
         keys = _LazyStackedTensorDictKeysView(
             self, include_nested=include_nested, leaves_only=leaves_only
         )
         return keys
@@ -5838,15 +6459,15 @@
         valid_keys = set(self.tensordicts[0].keys())
         for td in self.tensordicts[1:]:
             valid_keys = valid_keys.intersection(td.keys())
         self._valid_keys = sorted(valid_keys)
 
     def entry_class(self, key: NestedKey) -> type:
         data_type = type(self.tensordicts[0].get(key))
-        if is_tensor_collection(data_type):
+        if _is_tensor_collection(data_type):
             return LazyStackedTensorDict
         return data_type
 
     def apply_(self, fn: Callable, *others):
         for i, td in enumerate(self.tensordicts):
             idx = (slice(None),) * self.stack_dim + (i,)
             td.apply_(fn, *[other[idx] for other in others])
@@ -5865,22 +6486,34 @@
         if inplace:
             if any(arg for arg in (batch_size, device, names, constructor_kwargs)):
                 raise ValueError(
                     "Cannot pass other arguments to LazyStackedTensorDict.apply when inplace=True."
                 )
             return self.apply_(fn, *others)
         else:
-            return super().apply(
-                fn,
-                *others,
-                batch_size=batch_size,
-                device=device,
-                names=names,
-                **constructor_kwargs,
+            if batch_size is not None:
+                return super().apply(
+                    fn,
+                    *others,
+                    batch_size=batch_size,
+                    device=device,
+                    names=names,
+                    **constructor_kwargs,
+                )
+            others = (other.unbind(self.stack_dim) for other in others)
+            out = LazyStackedTensorDict(
+                *(
+                    td.apply(fn, *oth, device=device)
+                    for td, *oth in zip(self.tensordicts, *others)
+                ),
+                stack_dim=self.stack_dim,
             )
+            if names is not None:
+                out.names = names
+            return out
 
     def select(
         self, *keys: str, inplace: bool = False, strict: bool = False
     ) -> LazyStackedTensorDict:
         # the following implementation keeps the hidden keys in the tensordicts
         tensordicts = [
             td.select(*keys, inplace=inplace, strict=strict) for td in self.tensordicts
@@ -5996,50 +6629,50 @@
         elif isinstance(index, (Tensor, list)) and self.stack_dim != 0:
             tds = [tensordict[index] for tensordict in self.tensordicts]
             dim_drop = self.tensordicts[0].ndim - tds[0].ndim
             out = LazyStackedTensorDict(
                 *tds,
                 stack_dim=self.stack_dim - dim_drop,
             )
-            if self._names is not None:
+            if self._td_dim_names is not None:
                 out.names = [
                     name if i != out.stack_dim else self.names[self.stack_dim]
                     for i, name in enumerate(out.names)
                 ]
             return out
         elif isinstance(index, slice) and self.stack_dim == 0:
             out = LazyStackedTensorDict(
                 *self.tensordicts[index], stack_dim=self.stack_dim
             )
-            if self._names is not None:
+            if self._td_dim_names is not None:
                 out.names = [
                     name if i != out.stack_dim else self.names[self.stack_dim]
                     for i, name in enumerate(out.names)
                 ]
             return out
         elif isinstance(index, slice) and self.stack_dim != 0:
             out = LazyStackedTensorDict(
                 *[tensordict[index] for tensordict in self.tensordicts],
                 stack_dim=self.stack_dim,
             )
-            if self._names is not None:
+            if self._td_dim_names is not None:
                 out.names = [
                     name if i != out.stack_dim else self.names[self.stack_dim]
                     for i, name in enumerate(out.names)
                 ]
             return out
         elif isinstance(index, (slice, Number)):
             new_stack_dim = (
                 self.stack_dim - 1 if isinstance(index, Number) else self.stack_dim
             )
             out = LazyStackedTensorDict(
                 *[td[index] for td in self.tensordicts],
                 stack_dim=new_stack_dim,
             )
-            if self._names is not None:
+            if self._td_dim_names is not None:
                 out.names = [
                     name if i != out.stack_dim else self.names[self.stack_dim]
                     for i, name in enumerate(out.names)
                 ]
             return out
         elif isinstance(index, tuple):
             for i, item in enumerate(index):
@@ -6083,33 +6716,31 @@
                 count += 1
                 if item is not None:
                     index_to_stack += [item]
             new_stack_dim = self.stack_dim - sum(
                 int(_is_number(_item)) - int(_item is None) for _item in index_to_stack
             )
             out = torch.stack(list(tensordicts), dim=new_stack_dim)
-            if self._names is not None:
+            if self._td_dim_names is not None:
                 out.names = [
                     name if i != out.stack_dim else self.names[self.stack_dim]
                     for i, name in enumerate(out.names)
                 ]
             return out
         else:
             raise NotImplementedError(
                 f"selecting StackedTensorDicts with type "
                 f"{index.__class__.__name__} is not supported yet"
             )
 
     def __eq__(self, other):
-        # avoiding circular imports
-        from tensordict.tensorclass import is_tensorclass
 
         if is_tensorclass(other):
             return other == self
-        if isinstance(other, (dict,)) or is_tensor_collection(other):
+        if isinstance(other, (dict,)) or _is_tensor_collection(other.__class__):
             if (
                 isinstance(other, LazyStackedTensorDict)
                 and other.stack_dim == self.stack_dim
             ):
                 if self.shape != other.shape:
                     raise RuntimeError(
                         "Cannot compare LazyStackedTensorDict instances of different shape."
@@ -6134,20 +6765,18 @@
             return torch.stack(
                 [td == other for td in self.tensordicts],
                 self.stack_dim,
             )
         return False
 
     def __ne__(self, other):
-        # avoiding circular imports
-        from tensordict.tensorclass import is_tensorclass
 
         if is_tensorclass(other):
             return other != self
-        if isinstance(other, (dict,)) or is_tensor_collection(other):
+        if isinstance(other, (dict,)) or _is_tensor_collection(other.__class__):
             if (
                 isinstance(other, LazyStackedTensorDict)
                 and other.stack_dim == self.stack_dim
             ):
                 if self.shape != other.shape:
                     raise RuntimeError(
                         "Cannot compare LazyStackedTensorDict instances of different shape."
@@ -6266,37 +6895,65 @@
             return _future_list
         else:
             for future in _future_list:
                 future.wait()
             return
 
     def del_(self, key: str, **kwargs: Any) -> TensorDictBase:
+        ids = set()
+        cur_len = len(ids)
+        is_deleted = False
+        error = None
         for td in self.tensordicts:
-            td.del_(key, **kwargs)
+            # checking that the td has not been processed yet.
+            # It could be that not all sub-tensordicts have the appropriate
+            # entry but one must have it (or an error is thrown).
+            tdid = id(td)
+            ids.add(tdid)
+            new_cur_len = len(ids)
+            if new_cur_len == cur_len:
+                continue
+            cur_len = new_cur_len
+            try:
+                td.del_(key, **kwargs)
+                is_deleted = True
+            except KeyError as err:
+                error = err
+                continue
+        if not is_deleted:
+            # we know err is defined because LazyStackedTensorDict cannot be empty
+            raise error
         self._valid_keys.remove(key)
         return self
 
     def pop(
         self, key: NestedKey, default: str | CompatibleType = NO_DEFAULT
     ) -> CompatibleType:
 
-        try:
-            # using try/except for get/del is suboptimal, but
-            # this is faster that checkink if key in self keys
-            out = self.get(key, default)
-            if key in self.valid_keys:
-                self._valid_keys.remove(key)
-        except KeyError as err:
-            # if default provided, 'out' value will return, else raise error
-            if default == NO_DEFAULT:
-                raise KeyError(
-                    f"You are trying to pop key `{key}` which is not in dict "
-                    f"without providing default value."
-                ) from err
-        return out
+        # using try/except for get/del is suboptimal, but
+        # this is faster that checkink if key in self keys
+        key = unravel_keys(key)
+        present = False
+        if isinstance(key, tuple):
+            if key in self.keys(True):
+                present = True
+                value = self._get_tuple(key, NO_DEFAULT)
+        elif key in self.keys():
+            present = True
+            value = self._get_str(key, NO_DEFAULT)
+        if present:
+            self.del_(key)
+        elif default is not NO_DEFAULT:
+            value = default
+        else:
+            raise KeyError(
+                f"You are trying to pop key `{key}` which is not in dict "
+                f"without providing default value."
+            )
+        return value
 
     def share_memory_(self) -> TensorDictBase:
         for td in self.tensordicts:
             td.share_memory_()
         self._is_shared = True
         self.lock_()
         return self
@@ -6308,15 +6965,15 @@
 
     def memmap_(
         self, prefix: str | None = None, copy_existing: bool = False
     ) -> TensorDictBase:
         if prefix is not None:
             prefix = Path(prefix)
             if not prefix.exists():
-                prefix.mkdir(exist_ok=True)
+                os.makedirs(prefix, exist_ok=True)
             torch.save({"stack_dim": self.stack_dim}, prefix / "meta.pt")
         for i, td in enumerate(self.tensordicts):
             td.memmap_(
                 prefix=(prefix / str(i)) if prefix is not None else None,
                 copy_existing=copy_existing,
             )
         self._is_memmap = True
@@ -6327,15 +6984,15 @@
         self,
         prefix: str | None = None,
     ) -> TensorDictBase:
         tds = []
         if prefix is not None:
             prefix = Path(prefix)
             if not prefix.exists():
-                prefix.mkdir(exist_ok=True)
+                os.makedirs(prefix, exist_ok=True)
             torch.save({"stack_dim": self.stack_dim}, prefix / "meta.pt")
         for i, td in enumerate(self.tensordicts):
             td_like = td.memmap_like(
                 prefix=(prefix / str(i)) if prefix is not None else None,
             )
             tds.append(td_like)
         td_out = torch.stack(tds, self.stack_dim)
@@ -6398,15 +7055,15 @@
             if isinstance(key, tuple):
                 key, subkey = key[0], key[1:]
             else:
                 subkey = ()
             # the key must be a string by now. Let's check if it is present
             if key in keys:
                 target_class = self.entry_class(key)
-                if is_tensor_collection(target_class):
+                if _is_tensor_collection(target_class):
                     if isinstance(value, dict):
                         value_unbind = TensorDict(
                             value, self.batch_size, _run_checks=False
                         ).unbind(self.stack_dim)
                     else:
                         value_unbind = value.unbind(self.stack_dim)
                     for t, _value in zip(self.tensordicts, value_unbind):
@@ -6479,14 +7136,15 @@
             td.masked_fill_(_mask, value)
         return self
 
     def masked_fill(self, mask: Tensor, value: float | bool) -> TensorDictBase:
         td_copy = self.clone()
         return td_copy.masked_fill_(mask, value)
 
+    @lock_blocked
     def insert(self, index: int, tensordict: TensorDictBase) -> None:
         """Insert a TensorDict into the stack at the specified index.
 
         Analogous to list.insert. The inserted TensorDict must have compatible
         batch_size and device. Insertion is in-place, nothing is returned.
 
         Args:
@@ -6519,58 +7177,111 @@
 
         self.tensordicts.insert(index, tensordict)
 
         N = len(self.tensordicts)
         self._batch_size = self._compute_batch_size(batch_size, self.stack_dim, N)
         self._update_valid_keys()
 
+    @lock_blocked
     def append(self, tensordict: TensorDictBase) -> None:
         """Append a TensorDict onto the stack.
 
         Analogous to list.append. The appended TensorDict must have compatible
         batch_size and device. The append operation is in-place, nothing is returned.
 
         Args:
             tensordict (TensorDictBase): The TensorDict to be appended onto the stack.
 
         """
         self.insert(len(self.tensordicts), tensordict)
 
     @property
     def is_locked(self) -> bool:
-        is_locked = self._is_locked
+        if self._is_locked is not None:
+            # if tensordicts have been locked through this Lazy stack, then we can
+            # trust this lazy stack to contain the info.
+            # In all other cases we must check
+            return self._is_locked
+        # If any of the tensordicts is not locked, we assume that the lazy stack
+        # is not locked either. Caching is then disabled and
         for td in self.tensordicts:
-            is_locked = is_locked or td.is_locked
-        self._is_locked = is_locked
-        return is_locked
+            if not td.is_locked:
+                return False
+        else:
+            # In this case, all tensordicts were locked before the lazy stack
+            # was created and they were not locked through the lazy stack.
+            # This means we cannot cache the value because this lazy stack
+            # if not part of the graph. We don't want it to be part of the graph
+            # because this object being locked is only a side-effect.
+            # Calling self.lock_() here could however speed things up.
+            return True
 
     @is_locked.setter
     def is_locked(self, value: bool) -> None:
         if value:
             self.lock_()
         else:
             self.unlock_()
 
-    def lock_(self) -> LazyStackedTensorDict:
+    @property
+    def _lock_id(self):
+        """Ids of all tensordicts that need to be unlocked for this to be unlocked."""
+        _lock_id = set()
+        for tensordict in self.tensordicts:
+            _lock_id = _lock_id.union(tensordict._lock_id)
+        _lock_id = _lock_id - {id(self)}
+        return _lock_id
+
+    def _lock_propagate(self, lock_ids=None):
+        """Registers the parent tensordict that handles the lock."""
         self._is_locked = True
+        _locked_tensordicts = []
+        is_root = lock_ids is None
+        if is_root:
+            lock_ids = set()
+        lock_ids = lock_ids.union({id(self)})
+        for dest in self.tensordicts:
+            dest._lock_propagate(lock_ids)
+            _locked_tensordicts.append(dest)
+
+    def _remove_lock(self, lock_id):
         for td in self.tensordicts:
-            td.lock_()
-        return self
+            td._remove_lock(lock_id)
 
-    lock = _renamed_inplace_method(lock_)
+    @erase_cache
+    def _propagate_unlock(self, lock_ids=None):
+        # we can't set _is_locked to False because after it's unlocked, anything
+        # can happen to a child tensordict.
+        self._is_locked = None
+        if lock_ids is None:
+            lock_ids = set()
+
+        unlocked_tds = [self]
+        lock_ids.add(id(self))
+        for dest in self.tensordicts:
+            unlocked_tds.extend(dest._propagate_unlock(lock_ids))
 
-    def unlock_(self) -> LazyStackedTensorDict:
-        self._is_locked = False
         self._is_shared = False
         self._is_memmap = False
         self._sorted_keys = None
+        return unlocked_tds
+
+    def __del__(self):
+        if self._is_locked is None:
+            # then we can reliably say that this lazy stack is not part of
+            # the tensordicts graphs
+            return
+        # this can be a perf bottleneck
         for td in self.tensordicts:
-            td.unlock_()
-        return self
+            td._remove_lock(id(self))
+
+    lock_ = TensorDictBase.lock_
+    lock = _renamed_inplace_method(lock_)
 
+    unlock_ = TensorDictBase.unlock_
     unlock = _renamed_inplace_method(unlock_)
 
 
 class _CustomOpTensorDict(TensorDictBase):
     """Encodes lazy operations on tensors contained in a TensorDict."""
 
     def __new__(cls, *args: Any, **kwargs: Any) -> _CustomOpTensorDict:
@@ -6653,53 +7364,42 @@
 
     @batch_size.setter
     def batch_size(self, new_size: torch.Size) -> None:
         self._batch_size_setter(new_size)
 
     def _rename_subtds(self, names):
         for key in self.keys():
-            if is_tensor_collection(self.entry_class(key)):
+            if _is_tensor_collection(self.entry_class(key)):
                 raise RuntimeError(
                     "Cannot rename dimensions of a lazy TensorDict with "
                     "nested collections. Convert the instance to a regular "
                     "tensordict by using the `to_tensordict()` method first."
                 )
 
     def _change_batch_size(self, new_size: torch.Size) -> None:
         if not hasattr(self, "_orig_batch_size"):
             self._orig_batch_size = self.batch_size
         elif self._orig_batch_size == new_size:
             del self._orig_batch_size
         self._batch_size = new_size
 
-    def get(
-        self,
-        key: NestedKey,
-        default: str | CompatibleType = NO_DEFAULT,
-        _return_original_tensor: bool = False,
-    ) -> CompatibleType:
-        # TODO: temporary hack while SavedTensorDict and LazyStackedTensorDict don't
-        # support nested iteration
-        include_nested = not isinstance(self._source, (LazyStackedTensorDict,))
-
-        if key in self._source.keys(include_nested=include_nested):
-            item = self._source.get(key)
-            transformed_tensor = getattr(item, self.custom_op)(
-                **self._update_custom_op_kwargs(item)
-            )
-            if not _return_original_tensor:
-                return transformed_tensor
-            return transformed_tensor, item
-        else:
-            if _return_original_tensor:
-                raise RuntimeError(
-                    "_return_original_tensor not compatible with get(..., "
-                    "default=smth)"
-                )
-            return self._default_get(key, default)
+    def _get_str(self, key, default):
+        tensor = self._source._get_str(key, default)
+        if tensor is default:
+            return tensor
+        return self._transform_value(tensor)
+
+    def _get_tuple(self, key, default):
+        tensor = self._source._get_tuple(key, default)
+        if tensor is default:
+            return tensor
+        return self._transform_value(tensor)
+
+    def _transform_value(self, item):
+        return getattr(item, self.custom_op)(**self._update_custom_op_kwargs(item))
 
     def _set(self, key, value, inplace: bool = False):
         value = getattr(value, self.inv_op)(**self._update_inv_op_kwargs(value))
         self._source._set(key, value, inplace=inplace)
         return self
 
     def set(
@@ -6711,27 +7411,32 @@
             raise Exception(
                 f"{self.__class__.__name__} does not support setting values. "
                 f"Consider calling .contiguous() before calling this method."
             )
         if self.is_locked:
             raise RuntimeError(TensorDictBase.LOCK_ERROR)
 
-        if isinstance(key, tuple):
-            subsource, subkey = _get_leaf_tensordict(self._source, key, _default_hook)
+        if isinstance(key, tuple) and len(key) > 1:
+            subsource = self._source._get_str(key[0], None)
+            if subsource is None:
+                self._source.create_nested(key[0])
+                subsource = self._source._get_str(key[0], NO_DEFAULT)
+
             td = self.__class__(
                 source=subsource,
                 custom_op=self.custom_op,
                 inv_op=self.inv_op,
                 custom_op_kwargs=self._update_custom_op_kwargs(subsource),
                 inv_op_kwargs=self._update_inv_op_kwargs(subsource),
             )
-            td.set(subkey, value, inplace=inplace)
+            td.set(key[1:], value, inplace=inplace)
             return self
+        elif isinstance(key, tuple):
+            key = key[0]
 
-        key = self._validate_key(key)
         value = self._validate_value(value)
         return self._set(key, value, inplace=inplace)
 
     def set_(self, key: str, value: dict | CompatibleType) -> _CustomOpTensorDict:
         if self.inv_op is None:
             raise Exception(
                 f"{self.__class__.__name__} does not support setting values. "
@@ -6741,17 +7446,15 @@
         key = self._validate_key(key)
         value = self._validate_value(value)
         return self._set(key, value, inplace=True)
 
     def set_at_(
         self, key: str, value: dict | CompatibleType, idx: IndexType
     ) -> _CustomOpTensorDict:
-        transformed_tensor, original_tensor = self.get(
-            key, _return_original_tensor=True
-        )
+        transformed_tensor, original_tensor = self.get(key), self._source.get(key)
         if transformed_tensor.data_ptr() != original_tensor.data_ptr():
             raise RuntimeError(
                 f"{self} original tensor and transformed_in do not point to the "
                 f"same storage. Setting values in place is not currently "
                 f"supported in this setting, consider calling "
                 f"`td.clone()` before `td.set_at_(...)`"
             )
@@ -6776,14 +7479,15 @@
         )
         indented_source = textwrap.indent(f"source={self._source}", "\t")
         return (
             f"{self.__class__.__name__}(\n{indented_source}, "
             f"\n\top={self.custom_op}({custom_op_kwargs_str}))"
         )
 
+    # @cache  # noqa: B019
     def keys(
         self, include_nested: bool = False, leaves_only: bool = False
     ) -> _TensorDictKeysView:
         return self._source.keys(include_nested=include_nested, leaves_only=leaves_only)
 
     def select(
         self, *keys: str, inplace: bool = False, strict: bool = True
@@ -6848,24 +7552,26 @@
         return self
 
     def to(self, dest: DeviceType | type, **kwargs) -> TensorDictBase:
         if isinstance(dest, type) and issubclass(dest, TensorDictBase):
             if isinstance(self, dest):
                 return self
             out = dest(source=self)
-            if self._names is not None:
-                out.names = self._names
+            if self._td_dim_names is not None:
+                out.names = self._td_dim_names
             return out
         elif isinstance(dest, (torch.device, str, int)):
             if self.device is not None and torch.device(dest) == self.device:
                 return self
             td = self._source.to(dest, **kwargs)
             self_copy = copy(self)
             self_copy._source = td
             return self_copy
+        elif dest is None:
+            return self
         else:
             raise NotImplementedError(
                 f"dest must be a string, torch.device or a TensorDict "
                 f"instance, {dest} not allowed"
             )
 
     def pin_memory(self) -> _CustomOpTensorDict:
@@ -6926,20 +7632,59 @@
     def share_memory_(self) -> _CustomOpTensorDict:
         self._source.share_memory_()
         self._is_shared = True
         self.lock_()
         return self
 
     @property
-    def _names(self):
-        # we also want for _names to be accurate
-        if self._source._names is None:
+    def _td_dim_names(self):
+        # we also want for _td_dim_names to be accurate
+        if self._source._td_dim_names is None:
             return None
         return self.names
 
+    @property
+    def is_locked(self) -> bool:
+        return self._source.is_locked
+
+    @is_locked.setter
+    def is_locked(self, value) -> bool:
+        if value:
+            self.lock_()
+        else:
+            self.unlock_()
+
+    @as_decorator("is_locked")
+    def lock_(self) -> TensorDictBase:
+        self._source.lock_()
+        return self
+
+    @erase_cache
+    @as_decorator("is_locked")
+    def unlock_(self) -> TensorDictBase:
+        self._source.unlock_()
+        return self
+
+    def _remove_lock(self, lock_id):
+        return self._source._remove_lock(lock_id)
+
+    @erase_cache
+    def _lock_propagate(self, lock_ids):
+        return self._source._lock_propagate(lock_ids)
+
+    lock = _renamed_inplace_method(lock_)
+    unlock = _renamed_inplace_method(unlock_)
+
+    def __del__(self):
+        pass
+
+    @property
+    def sorted_keys(self):
+        return self._source.sorted_keys
+
 
 class _UnsqueezedTensorDict(_CustomOpTensorDict):
     """A lazy view on an unsqueezed TensorDict.
 
     When calling `tensordict.unsqueeze(dim)`, a lazy view of this operation is
     returned such that the following code snippet works without raising an
     exception:
@@ -7077,14 +7822,95 @@
     @names.setter
     def names(self, value):
         raise RuntimeError(
             "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
         )
 
 
+class _TransposedTensorDict(_CustomOpTensorDict):
+    """A lazy view on a TensorDict with two batch dimensions transposed.
+
+    When calling `tensordict.permute(dims_list, dim)`, a lazy view of this operation is
+    returned such that the following code snippet works without raising an
+    exception:
+
+        >>> assert tensordict.transpose(dims_list, dim).transpose(dims_list, dim) is tensordict
+
+    """
+
+    def transpose(self, dim0, dim1) -> TensorDictBase:
+        if dim0 < 0:
+            dim0 = self.ndim + dim0
+        if dim1 < 0:
+            dim1 = self.ndim + dim1
+        if any((dim0 < 0, dim1 < 0)):
+            raise ValueError(
+                "The provided dimensions are incompatible with the tensordict batch-size."
+            )
+        if dim0 == dim1:
+            return self
+        dims = (self.inv_op_kwargs.get("dim0"), self.inv_op_kwargs.get("dim1"))
+        if dim0 in dims and dim1 in dims:
+            return self._source
+        return super().permute(dim0, dim1)
+
+    def add_missing_dims(
+        self, num_dims: int, batch_dims: tuple[int, ...]
+    ) -> tuple[int, ...]:
+        dim_diff = num_dims - len(batch_dims)
+        all_dims = list(range(num_dims))
+        for i, x in enumerate(batch_dims):
+            if x < 0:
+                x = x - dim_diff
+            all_dims[i] = x
+        return tuple(all_dims)
+
+    def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
+        return self.custom_op_kwargs
+
+    def _update_inv_op_kwargs(self, tensor: Tensor) -> dict[str, Any]:
+        return self.custom_op_kwargs
+
+    def _stack_onto_(
+        self,
+        key: str,
+        list_item: list[CompatibleType],
+        dim: int,
+    ) -> TensorDictBase:
+
+        trsp = self.custom_op_kwargs["dim0"], self.custom_op_kwargs["dim1"]
+        if dim == trsp[0]:
+            dim = trsp[1]
+        elif dim == trsp[1]:
+            dim = trsp[0]
+
+        list_permuted_items = []
+        for item in list_item:
+            list_permuted_items.append(item.transpose(*trsp))
+        self._source._stack_onto_(key, list_permuted_items, dim)
+        return self
+
+    @property
+    def names(self):
+        names = copy(self._source.names)
+        dim0 = self.custom_op_kwargs["dim0"]
+        dim1 = self.custom_op_kwargs["dim1"]
+        names = [
+            names[dim0] if i == dim1 else names[dim1] if i == dim0 else name
+            for i, name in enumerate(names)
+        ]
+        return names
+
+    @names.setter
+    def names(self, value):
+        raise RuntimeError(
+            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
+        )
+
+
 class _PermutedTensorDict(_CustomOpTensorDict):
     """A lazy view on a TensorDict with the batch dimensions permuted.
 
     When calling `tensordict.permute(dims_list, dim)`, a lazy view of this operation is
     returned such that the following code snippet works without raising an
     exception:
 
@@ -7122,14 +7948,15 @@
         if np.array_equal(np.argsort(dims_list), self.inv_op_kwargs.get("dims")):
             return self._source
         return super().permute(*dims_list)
 
     def add_missing_dims(
         self, num_dims: int, batch_dims: tuple[int, ...]
     ) -> tuple[int, ...]:
+        # Adds the feature dimensions to the permute dims
         dim_diff = num_dims - len(batch_dims)
         all_dims = list(range(num_dims))
         for i, x in enumerate(batch_dims):
             if x < 0:
                 x = x - dim_diff
             all_dims[i] = x
         return tuple(all_dims)
@@ -7205,15 +8032,15 @@
             f"is_shared={is_shared}",
         ]
     )
     return f"{cls.__name__}({s})"
 
 
 def _make_repr(key: str, item: CompatibleType, tensordict: TensorDictBase) -> str:
-    if is_tensor_collection(type(item)):
+    if _is_tensor_collection(type(item)):
         return f"{key}: {repr(tensordict.get(key))}"
     return f"{key}: {_get_repr(item)}"
 
 
 def _td_fields(td: TensorDictBase) -> str:
     strs = []
     for key in td.keys():
@@ -7222,21 +8049,24 @@
             strs.append(_make_repr(key, item, td))
         except RuntimeError as err:
             if re.match(r"Found more than one unique shape in the tensors", str(err)):
                 # we know td is lazy stacked and the key is a leaf
                 # so we can get the shape and escape the error
                 shape = td.get_item_shape(key)
                 tensor = td.tensordicts[0].get(key)
-                substr = _get_repr_custom(
-                    tensor.__class__,
-                    shape=shape,
-                    device=tensor.device,
-                    dtype=tensor.dtype,
-                    is_shared=tensor.is_shared(),
-                )
+                if isinstance(tensor, TensorDictBase):
+                    substr = _td_fields(tensor)
+                else:
+                    substr = _get_repr_custom(
+                        tensor.__class__,
+                        shape=shape,
+                        device=tensor.device,
+                        dtype=tensor.dtype,
+                        is_shared=tensor.is_shared(),
+                    )
                 strs.append(f"{key}: {substr}")
             else:
                 raise err
 
     return indent(
         "\n" + ",\n".join(sorted(strs)),
         4 * " ",
@@ -7271,23 +8101,14 @@
                     raise KeyError(
                         f"got keys {keys} and {set(td.keys())} which are "
                         f"incompatible"
                     )
     return keys
 
 
-_ACCEPTED_CLASSES = [
-    Tensor,
-    MemmapTensor,
-    TensorDictBase,
-]
-if _has_torchrec:
-    _ACCEPTED_CLASSES += [KeyedJaggedTensor]
-
-
 def _expand_to_match_shape(
     parent_batch_size: torch.Size,
     tensor: Tensor,
     self_batch_dims: int,
     self_device: DeviceType,
 ) -> Tensor | TensorDictBase:
     if hasattr(tensor, "dtype"):
@@ -7375,20 +8196,21 @@
             is_shared=False)
     """
     if input_dict is not None:
         kwargs.update(input_dict)
     return TensorDict.from_dict(kwargs, batch_size=batch_size, device=device)
 
 
-def _set_max_batch_size(source: TensorDictBase):
+def _set_max_batch_size(source: TensorDictBase, batch_dims=None):
     """Updates a tensordict with its maximium batch size."""
     tensor_data = list(source.values())
+
     for val in tensor_data:
-        if is_tensor_collection(val):
-            _set_max_batch_size(val)
+        if _is_tensor_collection(val.__class__):
+            _set_max_batch_size(val, batch_dims=batch_dims)
     batch_size = []
     if not tensor_data:  # when source is empty
         source.batch_size = batch_size
         return
     curr_dim = 0
     while True:
         if tensor_data[0].dim() > curr_dim:
@@ -7396,15 +8218,16 @@
         else:
             source.batch_size = batch_size
             return
         for tensor in tensor_data[1:]:
             if tensor.dim() <= curr_dim or tensor.size(curr_dim) != curr_dim_size:
                 source.batch_size = batch_size
                 return
-        batch_size.append(curr_dim_size)
+        if batch_dims is None or len(batch_size) < batch_dims:
+            batch_size.append(curr_dim_size)
         curr_dim += 1
 
 
 def _iter_items_lazystack(
     tensordict: LazyStackedTensorDict,
 ) -> Iterator[tuple[str, CompatibleType]]:
     for key in tensordict.valid_keys:
@@ -7414,15 +8237,15 @@
             tensordict._update_valid_keys()
             continue
 
 
 def _clone_value(value: CompatibleType, recurse: bool) -> CompatibleType:
     if recurse:
         return value.clone()
-    elif is_tensor_collection(value):
+    elif _is_tensor_collection(value.__class__):
         return value.clone(recurse=False)
     else:
         return value
 
 
 def _is_number(item):
     if isinstance(item, Number):
```

## tensordict/utils.py

```diff
@@ -1,24 +1,33 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
 from __future__ import annotations
 
+import dataclasses
+import inspect
 import math
 import time
 
 import warnings
+from collections import defaultdict
+from collections.abc import KeysView
+from copy import copy
 from functools import wraps
+from importlib import import_module
 from numbers import Number
-from typing import Any, List, Sequence, Tuple, TYPE_CHECKING, Union
+from typing import Any, Callable, List, Sequence, Tuple, TYPE_CHECKING, Union
 
 import numpy as np
 import torch
+
+from packaging.version import parse
+from tensordict._tensordict import unravel_keys
 from torch import Tensor
 
 if TYPE_CHECKING:
     from tensordict.memmap import MemmapTensor
     from tensordict.tensordict import TensorDictBase
 
 try:
@@ -451,16 +460,15 @@
 
 def _normalize_key(key: NestedKey) -> NestedKey:
     # normalises tuples of length one to their string contents
     return key if not isinstance(key, tuple) or len(key) > 1 else key[0]
 
 
 def index_keyedjaggedtensor(
-    kjt: KeyedJaggedTensor,
-    index: slice | range | list | torch.Tensor | np.ndarray,
+    kjt: KeyedJaggedTensor, index: slice | range | list | torch.Tensor | np.ndarray
 ) -> KeyedJaggedTensor:
     """Indexes a KeyedJaggedTensor along the batch dimension.
 
     Args:
         kjt (KeyedJaggedTensor): a KeyedJaggedTensor to index
         index (torch.Tensor or other indexing type): batch index to use.
             Indexing with an integer will result in an error.
@@ -504,18 +512,15 @@
         full_index < _offsets2.unsqueeze(-1)
     )
     sel = sel.any(0).any(0)
     full_index = full_index.squeeze()[sel]
     values = kjt._values[full_index]
     weights = kjt._weights[full_index]
     return KeyedJaggedTensor(
-        values=values,
-        keys=kjt.keys(),
-        weights=weights,
-        lengths=lengths,
+        values=values, keys=kjt.keys(), weights=weights, lengths=lengths
     )
 
 
 def setitem_keyedjaggedtensor(
     orig_tensor: KeyedJaggedTensor,
     index: slice | range | list | torch.Tensor | np.ndarray,
     other: KeyedJaggedTensor,
@@ -647,15 +652,15 @@
         return 1
     else:
         return tensor.ndimension()
 
 
 def _shape(tensor: torch.Tensor) -> torch.Size:
     try:
-        return torch.Size(tensor.shape)
+        return tensor.shape
     except AttributeError as err:
         if type(tensor) is KeyedJaggedTensor:
             return torch.Size([len(tensor.lengths()) // len(tensor.keys())])
         raise err
 
 
 def _device(tensor: torch.Tensor) -> torch.device:
@@ -665,14 +670,16 @@
         return tensor.device()
     else:
         return tensor.device
 
 
 def _is_shared(tensor: torch.Tensor) -> bool:
     if isinstance(tensor, torch.Tensor):
+        if torch._C._functorch.is_batchedtensor(tensor):
+            return None
         return tensor.is_shared()
     elif isinstance(tensor, KeyedJaggedTensor):
         return False
     else:
         return tensor.is_shared()
 
 
@@ -827,30 +834,14 @@
     if isinstance(index[0], bool):
         return True
     if isinstance(index[0], list):
         return _is_lis_of_list_of_bools(index[0], False)
     return False
 
 
-def unravel_keys(key):
-    """Unravels keys when one can be sure that they are keys."""
-    if isinstance(key, tuple):
-        newkey = []
-        for subkey in key:
-            if isinstance(subkey, str):
-                newkey.append(subkey)
-            else:
-                _key = unravel_keys(subkey)
-                newkey += _key
-        key = tuple(newkey)
-    elif not isinstance(key, str):
-        raise ValueError(f"key should be a Sequence[NestedKey]. Got {key}")
-    return key
-
-
 def _maybe_unravel_keys_silent(index):
     """Attemps to unravel keys.
 
     If not possible (not keys) return the original index.
     """
     if isinstance(index, tuple):
         newkey = []
@@ -862,7 +853,324 @@
                 if _key is key:
                     return index
                 newkey += _key
         newkey = tuple(newkey)
     else:
         return index
     return newkey
+
+
+def is_tensorclass(obj: type | Any) -> bool:
+    """Returns True if obj is either a tensorclass or an instance of a tensorclass."""
+    cls = obj if isinstance(obj, type) else type(obj)
+    return _is_tensorclass(cls)
+
+
+def _is_tensorclass(cls) -> bool:
+    return (
+        dataclasses.is_dataclass(cls)
+        and "to_tensordict" in cls.__dict__
+        and "_from_tensordict" in cls.__dict__
+    )
+
+
+class implement_for:
+    """A version decorator that checks the version in the environment and implements a function with the fitting one.
+
+    If specified module is missing or there is no fitting implementation, call of the decorated function
+    will lead to the explicit error.
+    In case of intersected ranges, last fitting implementation is used.
+
+    Args:
+        module_name (str or callable): version is checked for the module with this
+            name (e.g. "gym"). If a callable is provided, it should return the
+            module.
+        from_version: version from which implementation is compatible. Can be open (None).
+        to_version: version from which implementation is no longer compatible. Can be open (None).
+
+    Examples:
+        >>> @implement_for("torch", None, "1.13")
+        >>> def fun(self, x):
+        ...     # Older torch versions will return x + 1
+        ...     return x + 1
+        ...
+        >>> @implement_for("torch", "0.13", "2.0")
+        >>> def fun(self, x):
+        ...     # More recent torch versions will return x + 2
+        ...     return x + 2
+        ...
+        >>> @implement_for(lambda: import_module("torch"), "0.", None)
+        >>> def fun(self, x):
+        ...     # More recent gym versions will return x + 2
+        ...     return x + 2
+        ...
+        >>> @implement_for("gymnasium", "0.27", None)
+        >>> def fun(self, x):
+        ...     # If gymnasium is to be used instead of gym, x+3 will be returned
+        ...     return x + 3
+        ...
+
+        This indicates that the function is compatible with gym 0.13+, but doesn't with gym 0.14+.
+    """
+
+    # Stores pointers to fitting implementations: dict[func_name] = func_pointer
+    _implementations = {}
+    _setters = []
+
+    def __init__(
+        self,
+        module_name: Union[str, Callable],
+        from_version: str = None,
+        to_version: str = None,
+    ):
+        self.module_name = module_name
+        self.from_version = from_version
+        self.to_version = to_version
+        implement_for._setters.append(self)
+
+    @staticmethod
+    def check_version(version, from_version, to_version):
+        return (from_version is None or parse(version) >= parse(from_version)) and (
+            to_version is None or parse(version) < parse(to_version)
+        )
+
+    @staticmethod
+    def get_class_that_defined_method(f):
+        """Returns the class of a method, if it is defined, and None otherwise."""
+        return f.__globals__.get(f.__qualname__.split(".")[0], None)
+
+    @property
+    def func_name(self):
+        return self.fn.__name__
+
+    def module_set(self):
+        """Sets the function in its module, if it exists already."""
+        cls = self.get_class_that_defined_method(self.fn)
+        if cls is None:
+            # class not yet defined
+            return
+        if cls.__class__.__name__ == "function":
+            cls = inspect.getmodule(self.fn)
+        setattr(cls, self.fn.__name__, self.fn)
+
+    @staticmethod
+    def import_module(module_name: Union[Callable, str]) -> str:
+        """Imports module and returns its version."""
+        if not callable(module_name):
+            module = import_module(module_name)
+        else:
+            module = module_name()
+        return module.__version__
+
+    def __call__(self, fn):
+        self.fn = fn
+
+        # If the module is missing replace the function with the mock.
+        func_name = self.func_name
+        implementations = implement_for._implementations
+
+        @wraps(fn)
+        def unsupported(*args, **kwargs):
+            raise ModuleNotFoundError(
+                f"Supported version of '{func_name}' has not been found."
+            )
+
+        do_set = False
+        # Return fitting implementation if it was encountered before.
+        if func_name in implementations:
+            try:
+                # check that backends don't conflict
+                version = self.import_module(self.module_name)
+                if self.check_version(version, self.from_version, self.to_version):
+                    do_set = True
+                if not do_set:
+                    return implementations[func_name]
+            except ModuleNotFoundError:
+                # then it's ok, there is no conflict
+                return implementations[func_name]
+        else:
+            try:
+                version = self.import_module(self.module_name)
+                if self.check_version(version, self.from_version, self.to_version):
+                    do_set = True
+            except ModuleNotFoundError:
+                return unsupported
+        if do_set:
+            implementations[func_name] = fn
+            self.module_set()
+            return fn
+        return unsupported
+
+    @classmethod
+    def reset(cls, setters=None):
+        if setters is None:
+            setters = copy(cls._setters)
+        cls._setters = []
+        cls._implementations = {}
+        for setter in setters:
+            setter(setter.fn)
+            cls._setters.append(setter)
+
+
+def _unfold_sequence(seq):
+    for item in seq:
+        if isinstance(item, (list, tuple)):
+            yield tuple(_unfold_sequence(item))
+        else:
+            if isinstance(item, (str, int, slice)) or item is Ellipsis:
+                yield item
+            else:
+                yield id(item)
+
+
+def _make_cache_key(args, kwargs):
+    """Creats a key for the cache such that memory footprint is minimized."""
+    return (
+        tuple(_unfold_sequence(args)),
+        tuple(_unfold_sequence(sorted(kwargs.items()))),
+    )
+
+
+def cache(fun):
+    """A cache for TensorDictBase subclasses.
+
+    This decorator will cache the values returned by a method as long as the
+    input arguments match.
+    Leaves (tensors and such) are not cached.
+    The cache is stored within the tensordict such that it can be erased at any
+    point in time.
+
+    Examples:
+        >>> import timeit
+        >>> from tensordict import TensorDict
+        >>> class SomeOtherTd(TensorDict):
+        ...     @cache
+        ...     def all_keys(self):
+        ...         return set(self.keys(include_nested=True))
+        >>> td = SomeOtherTd({("a", "b", "c", "d", "e", "f", "g"): 1.0}, [])
+        >>> td.lock_()
+        >>> print(timeit.timeit("set(td.keys(True))", globals={'td': td}))
+        11.057
+        >>> print(timeit.timeit("set(td.all_keys())", globals={'td': td}))
+        0.88
+    """
+    from tensordict.memmap import MemmapTensor
+
+    @wraps(fun)
+    def newfun(_self: "TensorDictBase", *args, **kwargs):
+        if not _self.is_locked:
+            return fun(_self, *args, **kwargs)
+        cache = _self._cache
+        if cache is None:
+            cache = _self._cache = defaultdict(dict)
+        cache = cache[fun.__name__]
+        key = _make_cache_key(args, kwargs)
+        if key not in cache:
+            out = fun(_self, *args, **kwargs)
+            if not isinstance(out, (Tensor, MemmapTensor, KeyedJaggedTensor)):
+                # we don't cache tensors to avoid filling the mem and / or
+                # stacking them from their origin
+                cache[key] = out
+        else:
+            out = cache[key]
+        return out
+
+    return newfun
+
+
+def erase_cache(fun):
+    """A decorator to erase the cache at each call."""
+
+    @wraps(fun)
+    def new_fun(self, *args, **kwargs):
+        self._erase_cache()
+        return fun(self, *args, **kwargs)
+
+    return new_fun
+
+
+NON_STR_KEY_TUPLE = "Nested membership checks with tuples of strings is only supported when setting `include_nested=True`."
+NON_STR_KEY = "TensorDict keys are always strings. Membership checks are only supported for strings or non-empty tuples of strings (for nested TensorDicts)"
+
+
+class _StringKeys(KeysView):
+    """A key view where contains is restricted to strings."""
+
+    def __contains__(self, item):
+        if not isinstance(item, str):
+            # at this point, we don't care about efficiency anymore
+            is_tuple = False
+            try:
+                if isinstance(item, tuple) and all(
+                    isinstance(key, str) for key in unravel_keys(item)
+                ):
+                    is_tuple = True
+            except Exception:  # catch errors during unravel
+                raise TypeError(NON_STR_KEY)
+            if is_tuple:
+                raise TypeError(NON_STR_KEY_TUPLE)
+            raise TypeError(NON_STR_KEY)
+        return super().__contains__(item)
+
+
+class _StringOnlyDict(dict):
+    """A dict class where contains is restricted to strings."""
+
+    def __contains__(self, item):
+        if not isinstance(item, str):
+            # at this point, we don't care about efficiency anymore
+            is_tuple = False
+            try:
+                if isinstance(item, tuple) and all(
+                    isinstance(key, str) for key in unravel_keys(item)
+                ):
+                    is_tuple = True
+            except Exception:  # catch errors during unravel
+                raise TypeError(NON_STR_KEY)
+            if is_tuple:
+                raise TypeError(NON_STR_KEY_TUPLE)
+            raise TypeError(NON_STR_KEY)
+        return super().__contains__(item)
+
+    def keys(self):
+        return _StringKeys(self)
+
+
+def lock_blocked(func):
+    """Checks that the tensordict is unlocked before executing a function."""
+
+    @wraps(func)
+    def new_func(self, *args, **kwargs):
+        if self.is_locked:
+            raise RuntimeError(self.LOCK_ERROR)
+        return func(self, *args, **kwargs)
+
+    return new_func
+
+
+class as_decorator:
+    """Converts a method to a decorator.
+
+    Examples:
+        >>> from tensordict import TensorDict
+        >>> data = TensorDict({}, [])
+        >>> with data.lock_(): # lock_ is decorated
+        ...     assert data.is_locked
+        >>> assert not data.is_locked
+    """
+
+    def __init__(self, attr):
+        self.attr = attr
+
+    def __call__(self, func):
+        @wraps(func)
+        def new_func(_self, *args, **kwargs):
+            _attr_pre = getattr(_self, self.attr)
+            out = func(_self, *args, **kwargs)
+            _attr_post = getattr(_self, self.attr)
+            if _attr_post is not _attr_pre:
+                _self._last_op = (new_func.__name__, (args, kwargs))
+            else:
+                _self._last_op = None
+            return out
+
+        return new_func
```

## tensordict/version.py

```diff
@@ -1,2 +1,2 @@
-__version__ = '2023.06.08'
-git_version = '55e5be159389a88f9b77147ace17a6b5ffad0f88'
+__version__ = '2023.07.01'
+git_version = '128f42a8338223daeff16d539e97efb0c358c19b'
```

## tensordict/nn/common.py

```diff
@@ -9,20 +9,21 @@
 import inspect
 import warnings
 from textwrap import indent
 from typing import Any, Callable, Dict, Iterable, List, Sequence, Tuple, Union
 
 import torch
 from cloudpickle import dumps as cloudpickle_dumps, loads as cloudpickle_loads
+from tensordict._tensordict import unravel_keys
 
 from tensordict.nn.functional_modules import make_functional
 
 from tensordict.nn.utils import set_skip_existing
 from tensordict.tensordict import is_tensor_collection, make_tensordict, TensorDictBase
-from tensordict.utils import _normalize_key, NestedKey, unravel_keys
+from tensordict.utils import _normalize_key, implement_for, NestedKey
 from torch import nn, Tensor
 
 try:
     from functorch import FunctionalModule, FunctionalModuleWithBuffers
 
     _has_functorch = True
 except ImportError:
@@ -285,20 +286,27 @@
     def _init(self, module):
         if self._initialized:
             return
         self._initialized = True
         self.module = module
         module.out_keys = list(self.out_keys)
 
-    def __call__(
+    @implement_for("torch", None, "2.0")
+    def __call__(  # noqa: F811
         self,
         module: TensorDictModuleBase,
         tensordict_in: TensorDictBase,
         tensordict_out: TensorDictBase,
     ):
+        if not isinstance(tensordict_out, TensorDictBase):
+            raise RuntimeError(
+                "You are likely using tensordict.nn.dispatch with keyword arguments with an older (< 2.0) version of pytorch. "
+                "This is currently not supported. Please use unnamed arguments or upgrade pytorch."
+            )
+        print(tensordict_out)
         # detect dispatch calls
         in_keys = module.in_keys
         is_dispatched = self._detect_dispatch(tensordict_in, in_keys)
         out_keys = self.out_keys
         # if dispatch filtered the out keys as they should we're happy
         if is_dispatched:
             if (not isinstance(tensordict_out, tuple) and len(out_keys) == 1) or (
@@ -338,28 +346,107 @@
             return tensordict_out.select(
                 *in_keys,
                 *out_keys,
                 inplace=True,
                 strict=False,
             )
 
-    def _detect_dispatch(self, tensordict_in, in_keys):
+    @implement_for("torch", "2.0", None)
+    def __call__(  # noqa: F811
+        self,
+        module: TensorDictModuleBase,
+        tensordict_in: TensorDictBase,
+        kwargs: Dict,
+        tensordict_out: TensorDictBase,
+    ):
+        # detect dispatch calls
+        in_keys = module.in_keys
+        if not tensordict_in and kwargs.get("tensordict", None) is not None:
+            tensordict_in = kwargs.pop("tensordict")
+        is_dispatched = self._detect_dispatch(tensordict_in, kwargs, in_keys)
+        out_keys = self.out_keys
+        # if dispatch filtered the out keys as they should we're happy
+        if is_dispatched:
+            if (not isinstance(tensordict_out, tuple) and len(out_keys) == 1) or (
+                len(out_keys) == len(tensordict_out)
+            ):
+                return tensordict_out
+        self._init(module)
+        if is_dispatched:
+            # it might be the case that dispatch was not aware of what the out-keys were.
+            if isinstance(tensordict_out, tuple):
+                out = tuple(
+                    item
+                    for i, item in enumerate(tensordict_out)
+                    if module._out_keys[i] in module.out_keys
+                )
+                if len(out) == 1:
+                    return out[0]
+                return out
+            elif module._out_keys[0] in module.out_keys and len(module._out_keys) == 1:
+                return tensordict_out
+            elif (
+                module._out_keys[0] not in module.out_keys
+                and len(module._out_keys) == 1
+            ):
+                return ()
+            else:
+                raise RuntimeError(
+                    f"Selecting out-keys failed. Original out_keys: {module._out_keys}, selected: {module.out_keys}."
+                )
+        if tensordict_out is tensordict_in:
+            return tensordict_out.select(
+                *in_keys,
+                *out_keys,
+                inplace=True,
+            )
+        else:
+            return tensordict_out.select(
+                *in_keys,
+                *out_keys,
+                inplace=True,
+                strict=False,
+            )
+
+    @implement_for("torch", None, "2.0")
+    def _detect_dispatch(self, tensordict_in, in_keys):  # noqa: F811
         if isinstance(tensordict_in, TensorDictBase) and all(
             key in tensordict_in.keys() for key in in_keys
         ):
             return False
         elif isinstance(tensordict_in, tuple):
             if len(tensordict_in):
                 if isinstance(tensordict_in[0], TensorDictBase):
                     return self._detect_dispatch(tensordict_in[0], in_keys)
                 return True
             return not len(in_keys)
         # not a TDBase: must be True
         return True
 
+    @implement_for("torch", "2.0", None)
+    def _detect_dispatch(self, tensordict_in, kwargs, in_keys):  # noqa: F811
+        if isinstance(tensordict_in, TensorDictBase) and all(
+            key in tensordict_in.keys(include_nested=True) for key in in_keys
+        ):
+            return False
+        elif isinstance(tensordict_in, tuple):
+            if len(tensordict_in) or len(kwargs):
+                if len(tensordict_in) and isinstance(tensordict_in[0], TensorDictBase):
+                    return self._detect_dispatch(tensordict_in[0], kwargs, in_keys)
+                elif (
+                    not len(tensordict_in)
+                    and len(kwargs)
+                    and isinstance(kwargs.get("tensordict", None), TensorDictBase)
+                ):
+                    return self._detect_dispatch(kwargs["tensordict"], in_keys)
+                return True
+            return not len(in_keys)
+        # not a TDBase: must be True
+        return True
+
     def remove(self):
         # reset ground truth
         self.module.out_keys = self.module._out_keys
 
     def __del__(self):
         self.remove()
 
@@ -419,15 +506,16 @@
     @out_keys.setter
     def out_keys(self, value: List[Union[str, Tuple[str]]]):
         # the first time out_keys are set, they are marked as ground truth
         if not hasattr(self, "_out_keys"):
             self._out_keys = value
         self._out_keys_apparent = value
 
-    def select_out_keys(self, *out_keys):
+    @implement_for("torch", None, "2.0")
+    def select_out_keys(self, *out_keys):  # noqa: F811
         """Selects the keys that will be found in the output tensordict.
 
         This is useful whenever one wants to get rid of intermediate keys in a
         complicated graph, or when the presence of these keys may trigger unexpected
         behaviours.
 
         The original ``out_keys`` can still be accessed via ``module.out_keys_source``.
@@ -530,14 +618,126 @@
                     err_msg += f"Are you passing the keys in a list? Try unpacking as: `{', '.join(out_keys[0])}`"
                 raise ValueError(err_msg)
         self.register_forward_hook(_OutKeysSelect(out_keys))
         for hook in self._forward_hooks.values():
             hook._init(self)
         return self
 
+    @implement_for("torch", "2.0", None)
+    def select_out_keys(self, *out_keys):  # noqa: F811
+        """Selects the keys that will be found in the output tensordict.
+
+        This is useful whenever one wants to get rid of intermediate keys in a
+        complicated graph, or when the presence of these keys may trigger unexpected
+        behaviours.
+
+        The original ``out_keys`` can still be accessed via ``module.out_keys_source``.
+
+        Args:
+            *out_keys (a sequence of strings or tuples of strings): the
+                out_keys that should be found in the output tensordict.
+
+        Returns: the same module, modified in-place with updated ``out_keys``.
+
+        The simplest usage is with :class:`~.TensorDictModule`:
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> from tensordict.nn import TensorDictModule, TensorDictSequential
+            >>> import torch
+            >>> mod = TensorDictModule(lambda x, y: (x+2, y+2), in_keys=["a", "b"], out_keys=["c", "d"])
+            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> mod.select_out_keys("d")
+            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        This feature will also work with dispatched arguments:
+        Examples:
+            >>> mod(torch.zeros(()), torch.ones(()))
+            tensor(2.)
+
+        This change will occur in-place (ie the same module will be returned
+        with an updated list of out_keys). It can be reverted using the
+        :meth:`TensorDictModuleBase.reset_out_keys` method.
+
+        Examples:
+            >>> mod.reset_out_keys()
+            >>> mod(TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, []))
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        This will work with other classes too, such as Sequential:
+        Examples:
+            >>> from tensordict.nn import TensorDictSequential
+            >>> seq = TensorDictSequential(
+            ...     TensorDictModule(lambda x: x+1, in_keys=["x"], out_keys=["y"]),
+            ...     TensorDictModule(lambda x: x+1, in_keys=["y"], out_keys=["z"]),
+            ... )
+            >>> td = TensorDict({"x": torch.zeros(())}, [])
+            >>> seq(td)
+            TensorDict(
+                fields={
+                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> seq.select_out_keys("z")
+            >>> td = TensorDict({"x": torch.zeros(())}, [])
+            >>> seq(td)
+            TensorDict(
+                fields={
+                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        """
+        if len(out_keys) == 1:
+            if out_keys[0] not in self.out_keys:
+                err_msg = f"Can't select non existent key: {out_keys[0]}. "
+                if (
+                    out_keys[0]
+                    and isinstance(out_keys[0], (tuple, list))
+                    and out_keys[0][0] in self.out_keys
+                ):
+                    err_msg += f"Are you passing the keys in a list? Try unpacking as: `{', '.join(out_keys[0])}`"
+                raise ValueError(err_msg)
+        self.register_forward_hook(_OutKeysSelect(out_keys), with_kwargs=True)
+        for hook in self._forward_hooks.values():
+            hook._init(self)
+        return self
+
     def reset_out_keys(self):
         """Resets the ``out_keys`` attribute to its orignal value.
 
         Returns: the same module, with its original ``out_keys`` values.
 
         Examples:
             >>> from tensordict import TensorDict
@@ -774,19 +974,19 @@
 
         if isinstance(out_keys, (str, tuple)):
             out_keys = [out_keys]
         elif not isinstance(out_keys, list):
             raise ValueError(self._OUT_KEY_ERR)
         try:
             in_keys = [unravel_keys(in_key) for in_key in in_keys]
-        except ValueError:
+        except Exception:
             raise ValueError(self._IN_KEY_ERR)
         try:
             out_keys = [unravel_keys(out_key) for out_key in out_keys]
-        except ValueError:
+        except Exception:
             raise ValueError(self._OUT_KEY_ERR)
 
         if type(module) is type or not callable(module):
             raise ValueError(
                 f"Module {module} if type {type(module)} is not callable. "
                 f"Typical accepted types are nn.Module or TensorDictModule."
             )
```

## tensordict/nn/functional_modules.py

```diff
@@ -11,26 +11,99 @@
 import warnings
 from copy import deepcopy
 from functools import wraps
 from typing import Any, Callable, Iterable
 
 import torch
 from tensordict import TensorDict
-from tensordict.tensordict import is_tensor_collection, TensorDictBase
+from tensordict.tensordict import _is_tensor_collection, TensorDictBase
+
+from tensordict.utils import implement_for
 from torch import nn
 
+try:
+    from torch.nn.modules.module import _global_parameter_registration_hooks
+except ImportError:
+    # old torch version, passing
+    pass
+
+
+@implement_for("torch", "2.0", None)
+def _register_params(self, name, param):
+    """A simplified version of register_param where checks are skipped."""
+    for hook in _global_parameter_registration_hooks.values():
+        output = hook(self, name, param)
+        if output is not None:
+            param = output
+    self._parameters[name] = param
+
+
+@implement_for("torch", None, "2.0")
+def _register_params(self, name, param):  # noqa: F811
+    self.register_parameter(name, param)
+
 
 def set_tensor(module: "torch.nn.Module", name: str, tensor: torch.Tensor) -> None:
     """Simplified version of torch.nn.utils._named_member_accessor."""
     if name in module._parameters:
-        module._parameters[name] = tensor  # type: ignore[assignment]
-    elif name in module._buffers:
+        del module._parameters[name]  # type: ignore[assignment]
+    was_buffer = name in module._buffers
+    if was_buffer:
+        del module._buffers[name]
+    if isinstance(tensor, nn.Parameter):
+        module.__dict__.pop(name, None)
+        # module.register_parameter(name, tensor)
+        _register_params(module, name, tensor)
+    elif was_buffer and isinstance(tensor, Tensor):
         module._buffers[name] = tensor
     else:
-        setattr(module, name, tensor)
+        module.__dict__[name] = tensor
+
+
+@implement_for("torch", "2.0", None)
+def set_tensor_dict(  # noqa: F811
+    module_dict, module, name: str, tensor: torch.Tensor
+) -> None:
+    """Simplified version of torch.nn.utils._named_member_accessor."""
+    if name in module_dict["_parameters"]:
+        del module_dict["_parameters"][name]  # type: ignore[assignment]
+    was_buffer = name in module_dict["_buffers"]
+    if was_buffer:
+        del module_dict["_buffers"][name]
+    if isinstance(tensor, nn.Parameter):
+        module_dict.pop(name, None)
+        # module.register_parameter(name, tensor)
+        for hook in _global_parameter_registration_hooks.values():
+            output = hook(module, name, tensor)
+            if output is not None:
+                tensor = output
+        module_dict["_parameters"][name] = tensor
+    elif was_buffer and isinstance(tensor, Tensor):
+        module_dict["_buffers"][name] = tensor
+    else:
+        module_dict[name] = tensor
+
+
+@implement_for("torch", None, "2.0")
+def set_tensor_dict(  # noqa: F811
+    module_dict, module, name: str, tensor: torch.Tensor
+) -> None:
+    """Simplified version of torch.nn.utils._named_member_accessor."""
+    if name in module_dict["_parameters"]:
+        del module_dict["_parameters"][name]  # type: ignore[assignment]
+    was_buffer = name in module_dict["_buffers"]
+    if was_buffer:
+        del module_dict["_buffers"][name]
+    if isinstance(tensor, nn.Parameter):
+        module_dict.pop(name, None)
+        module.register_parameter(name, tensor)
+    elif was_buffer and isinstance(tensor, Tensor):
+        module_dict["_buffers"][name] = tensor
+    else:
+        module_dict[name] = tensor
 
 
 _RESET_OLD_TENSORDICT = True
 try:
     import torch._functorch.vmap as vmap_src
     from torch._functorch.vmap import (
         _add_batch_dim,
@@ -129,26 +202,32 @@
 
     def _create_batched_inputs(
         flat_in_dims: list[int], flat_args: list[Any], vmap_level: int, args_spec
     ) -> Any:
         # See NOTE [Ignored _remove_batch_dim, _add_batch_dim]
         # If tensordict, we remove the dim at batch_size[in_dim] such that the TensorDict can accept
         # the batched tensors. This will be added in _unwrap_batched
-        batched_inputs = [
-            arg
-            if in_dim is None
-            else arg.apply(
-                lambda _arg, in_dim=in_dim: _add_batch_dim(_arg, in_dim, vmap_level),
-                batch_size=[b for i, b in enumerate(arg.batch_size) if i != in_dim],
-                names=[name for i, name in enumerate(arg.names) if i != in_dim],
-            )
-            if isinstance(arg, TensorDictBase)
-            else _add_batch_dim(arg, in_dim, vmap_level)
-            for in_dim, arg in zip(flat_in_dims, flat_args)
-        ]
+
+        batched_inputs = []
+        for in_dim, arg in zip(flat_in_dims, flat_args):
+            if in_dim is None:
+                if isinstance(arg, TensorDictBase):
+                    # this may be a perf bottleneck and could benefit from caching
+                    # arg = cache(arg.clone)(False)
+                    arg = arg.clone(False)
+
+                batched_input = arg
+            else:
+                if isinstance(arg, TensorDictBase):
+                    batched_input = arg._add_batch_dim(
+                        in_dim=in_dim, vmap_level=vmap_level
+                    )
+                else:
+                    batched_input = _add_batch_dim(arg, in_dim, vmap_level)
+            batched_inputs.append(batched_input)
         return tree_unflatten(batched_inputs, args_spec)
 
     vmap_src._create_batched_inputs = _create_batched_inputs
 
     def _unwrap_batched(
         batched_outputs: Any,
         out_dims: int | tuple[int, ...],
@@ -186,30 +265,21 @@
                 out_dims = out_dims[0]
             else:
                 incompatible_error()
         else:
             flat_out_dims = _broadcast_to_and_flatten(out_dims, output_spec)
             if flat_out_dims is None:
                 incompatible_error()
-
         flat_outputs = []
         for batched_output, out_dim in zip(flat_batched_outputs, flat_out_dims):
             if not isinstance(batched_output, TensorDictBase):
                 out = _remove_batch_dim(batched_output, vmap_level, batch_size, out_dim)
             else:
-                new_batch_size = list(batched_output.batch_size)
-                new_batch_size.insert(out_dim, batch_size)
-                new_names = list(batched_output.names)
-                new_names.insert(out_dim, None)
-                out = batched_output.apply(
-                    lambda x, out_dim=out_dim: _remove_batch_dim(
-                        x, vmap_level, batch_size, out_dim
-                    ),
-                    batch_size=new_batch_size,
-                    names=new_names,
+                out = batched_output._remove_batch_dim(
+                    vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim
                 )
             flat_outputs.append(out)
         return tree_unflatten(flat_outputs, output_spec)
 
     vmap_src._unwrap_batched = _unwrap_batched
 
 
@@ -265,67 +335,125 @@
         module_tensordict = extract_weights_and_buffers(module)
         if module_tensordict is not None:
             tensordict[name] = module_tensordict
     model.__dict__["_is_stateless"] = True
     return TensorDict(tensordict, batch_size=torch.Size([]), _run_checks=False)
 
 
+# For bookkeeping: this function seems to have the same runtime but will not access
+# modules that don't have parameters if they're not registered as empty tensordicts
+# in the input. Hence they won't be turned as stateful, which could cause some bugs.
 def _swap_state(
     model: nn.Module,
     tensordict: TensorDict,
     is_stateless: bool,
     return_old_tensordict: bool = False,
     old_tensordict: dict[str, torch.Tensor] | TensorDict | None = None,
 ) -> dict[str, torch.Tensor] | TensorDict | None:
-    was_stateless = model.__dict__.get("_is_stateless", None)
+    __dict__ = model.__dict__
+    was_stateless = __dict__.get("_is_stateless", None)
     if was_stateless is None:
         raise Exception(f"{model}\nhas no stateless attribute.")
-    model.__dict__["_is_stateless"] = is_stateless
+    __dict__["_is_stateless"] = is_stateless
     # return_old_tensordict = return_old_tensordict and not was_stateless
     if old_tensordict is None:
-        old_tensordict = {}
-    keys = set(tensordict.keys())
-    children = []
-    for key, child in model.named_children():
-        try:
-            keys.remove(key)
-        except KeyError:
-            # if params are built externally, this could lead to a KeyError as some
-            # modules do not have params
-            pass
-        children.append(key)
-        value = tensordict.get(key, None)
-        if value is None:
-            # faster than get(key, Tensordict(...))
-            value = {}
-
-        _old_value = old_tensordict.get(key, None)
-        _old_value = _swap_state(
-            child,
-            value,
-            return_old_tensordict=return_old_tensordict,
-            old_tensordict=_old_value,
-            is_stateless=is_stateless,
-        )
-        old_tensordict[key] = _old_value
-    for key in keys:
-        value = tensordict.get(key)
-        is_param = key in model.__dict__.get("_parameters")
-        if return_old_tensordict:
-            old_attr = getattr(model, key)
-            if old_attr is None:
-                old_attr = torch.zeros(*value.shape, 0)
-            old_tensordict[key] = old_attr
-        if is_param:
-            delattr(model, key)
-        set_tensor(model, key, value)
+        old_tensordict_dict = old_tensordict = {}
+    else:
+        old_tensordict_dict = {}
+    for key, value in tensordict.items():
+        cls = value.__class__
+        if _is_tensor_collection(cls) or issubclass(cls, dict):
+            _old_value = old_tensordict.get(key, None)
+            _old_value = _swap_state(
+                __dict__["_modules"][key],
+                value,
+                is_stateless=is_stateless,
+                old_tensordict=_old_value,
+                return_old_tensordict=return_old_tensordict,
+            )
+            old_tensordict_dict[key] = _old_value
+        else:
+            _old_value = None
+            if return_old_tensordict:
+                _old_value = __dict__["_parameters"].get(key, None)
+                if _old_value is None:
+                    _old_value = __dict__["_buffers"].get(key, None)
+                if _old_value is None:
+                    _old_value = __dict__.get(key, None)
+                if _old_value is None:
+                    _old_value = torch.zeros(*value.shape, 0)
+                old_tensordict_dict[key] = _old_value
+                # old_tensordict_dict[key] = _old_value
+            set_tensor_dict(__dict__, model, key, value)
+    old_tensordict.update(old_tensordict_dict)
     if was_stateless or not return_old_tensordict:
         return old_tensordict
     else:
-        return TensorDict(old_tensordict, [])
+        return TensorDict(old_tensordict, [], _run_checks=False)
+
+
+# def _swap_state(
+#     model: nn.Module,
+#     tensordict: TensorDict,
+#     is_stateless: bool,
+#     return_old_tensordict: bool = False,
+#     old_tensordict: dict[str, torch.Tensor] | TensorDict | None = None,
+# ) -> dict[str, torch.Tensor] | TensorDict | None:
+#     __dict__ = model.__dict__
+#     was_stateless = __dict__.get("_is_stateless", None)
+#     if was_stateless is None:
+#         raise Exception(f"{model}\nhas no stateless attribute.")
+#     __dict__["_is_stateless"] = is_stateless
+#     # return_old_tensordict = return_old_tensordict and not was_stateless
+#     if old_tensordict is None:
+#         old_tensordict_dict = old_tensordict = {}
+#     else:
+#         old_tensordict_dict = {}
+#     # keys = set(tensordict.keys())
+#     children = set()
+#     # this loop ignores the memo from named children
+#     for key, child in __dict__["_modules"].items():  # model.named_children():
+#         children.add(key)
+#         value = tensordict.get(key, None)
+#         if value is None:
+#             # faster than get(key, Tensordict(...))
+#             value = {}
+#
+#         _old_value = old_tensordict.get(key, None)
+#         _old_value = _swap_state(
+#             child,
+#             value,
+#             return_old_tensordict=return_old_tensordict,
+#             old_tensordict=_old_value,
+#             is_stateless=is_stateless,
+#         )
+#         old_tensordict_dict[key] = _old_value
+#     for key in tensordict.keys():
+#         if key in children:
+#             continue
+#         value = tensordict.get(key)
+#         if return_old_tensordict:
+#             old_attr = __dict__["_parameters"].get(key, None)
+#             if old_attr is None:
+#                 old_attr = __dict__["_buffers"].get(key, None)
+#             if old_attr is None:
+#                 old_attr = __dict__.get(key, None)
+#             if old_attr is None:
+#                 old_attr = torch.zeros(*value.shape, 0)
+#             old_tensordict_dict[key] = old_attr
+#         # is_param = key in model.__dict__.get("_parameters")
+#         # if is_param:
+#         #     delattr(model, key)
+#         #     print(value)
+#         set_tensor_dict(__dict__, model, key, value)
+#     old_tensordict.update(old_tensordict_dict)
+#     if was_stateless or not return_old_tensordict:
+#         return old_tensordict
+#     else:
+#         return TensorDict(old_tensordict, [])
 
 
 def is_functional(module: nn.Module):
     """Checks if :func:`make_functional` has been called on the module."""
     return "_functionalized" in module.__dict__
 
 
@@ -358,15 +486,15 @@
     )
     if return_params and not _is_stateless:
         params = extract_weights_and_buffers(
             module,
         )
         if keep_params:
             repopulate_module(module, params)
-        return params
+        return params.lock_()
     elif return_params and _is_stateless:
         raise RuntimeError(
             "Calling make_functional with return_params=True on a functional, stateless module. "
         )
     elif not keep_params:
         extract_weights_and_buffers(module)
 
@@ -381,31 +509,33 @@
     repopulate_module(module, params)
     return out
 
 
 def _make_decorator(module: nn.Module, fun_name: str) -> Callable:
     fun = getattr(module, fun_name)
 
+    from tensordict.nn.common import TensorDictModuleBase
+
     @wraps(fun)
     def new_fun(self, *args, **kwargs):
         # 3 use cases: (1) params is the last arg, (2) params is in kwargs, (3) no params
         _is_stateless = self.__dict__.get("_is_stateless", False)
         params = kwargs.pop("params", None)
 
-        from tensordict.nn.common import TensorDictModuleBase
-
         if isinstance(self, TensorDictModuleBase):
             if (
                 params is None
                 and len(args) == 2
-                and all(is_tensor_collection(item) for item in args)
+                and all(_is_tensor_collection(item.__class__) for item in args)
             ):
                 params = args[1]
                 args = args[:1]
-        elif (len(args) and is_tensor_collection(args[0])) or "tensordict" in kwargs:
+        elif (
+            len(args) and _is_tensor_collection(args[0].__class__)
+        ) or "tensordict" in kwargs:
             warnings.warn(
                 "You are passing a tensordict/tensorclass instance to a module that "
                 "does not inherit from TensorDictModuleBase. This may lead to unexpected "
                 "behaviours with functional calls."
             )
         if _is_stateless or params is not None:
             if params is None:
```

## Comparing `tensordict_nightly-2023.6.8.dist-info/LICENSE` & `tensordict_nightly-2023.7.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `tensordict_nightly-2023.6.8.dist-info/METADATA` & `tensordict_nightly-2023.7.1.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 Metadata-Version: 2.1
 Name: tensordict-nightly
-Version: 2023.6.8
+Version: 2023.7.1
+Summary: UNKNOWN
 Home-page: https://github.com/pytorch-labs/tensordict
 Author: tensordict contributors
 Author-email: vmoens@fb.com
 License: BSD
+Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Development Status :: 4 - Beta
 Description-Content-Type: text/markdown
 License-File: LICENSE
@@ -375,7 +377,9 @@
 Hopefully these should not happen too often, as the current roadmap mostly 
 involves adding new features and building compatibility with the broader
 PyTorch ecosystem.
 
 ## License
 
 TensorDict is licensed under the MIT License. See [LICENSE](LICENSE) for details.
+
+
```

### html2text {}

```diff
@@ -1,21 +1,22 @@
-Metadata-Version: 2.1 Name: tensordict-nightly Version: 2023.6.8 Home-page:
-https://github.com/pytorch-labs/tensordict Author: tensordict contributors
-Author-email: vmoens@fb.com License: BSD Classifier: Programming Language ::
-Python :: 3.7 Classifier: Programming Language :: Python :: 3.8 Classifier:
-Programming Language :: Python :: 3.9 Classifier: Programming Language ::
-Python :: 3.10 Classifier: Development Status :: 4 - Beta Description-Content-
-Type: text/markdown License-File: LICENSE Requires-Dist: torch Requires-Dist:
-numpy Requires-Dist: cloudpickle Provides-Extra: checkpointing Requires-Dist:
-torchsnapshot-nightly ; extra == 'checkpointing' Provides-Extra: h5 Requires-
-Dist: h5py (>=3.8) ; extra == 'h5' Provides-Extra: tests Requires-Dist: pytest
-; extra == 'tests' Requires-Dist: pyyaml ; extra == 'tests' Requires-Dist:
-pytest-instafail ; extra == 'tests' Requires-Dist: pytest-rerunfailures ; extra
-== 'tests' Requires-Dist: pytest-benchmark ; extra == 'tests'   [![Docs -
-GitHub.io](https://img.shields.io/static/
+Metadata-Version: 2.1 Name: tensordict-nightly Version: 2023.7.1 Summary:
+UNKNOWN Home-page: https://github.com/pytorch-labs/tensordict Author:
+tensordict contributors Author-email: vmoens@fb.com License: BSD Platform:
+UNKNOWN Classifier: Programming Language :: Python :: 3.7 Classifier:
+Programming Language :: Python :: 3.8 Classifier: Programming Language ::
+Python :: 3.9 Classifier: Programming Language :: Python :: 3.10 Classifier:
+Development Status :: 4 - Beta Description-Content-Type: text/markdown License-
+File: LICENSE Requires-Dist: torch Requires-Dist: numpy Requires-Dist:
+cloudpickle Provides-Extra: checkpointing Requires-Dist: torchsnapshot-nightly
+; extra == 'checkpointing' Provides-Extra: h5 Requires-Dist: h5py (>=3.8) ;
+extra == 'h5' Provides-Extra: tests Requires-Dist: pytest ; extra == 'tests'
+Requires-Dist: pyyaml ; extra == 'tests' Requires-Dist: pytest-instafail ;
+extra == 'tests' Requires-Dist: pytest-rerunfailures ; extra == 'tests'
+Requires-Dist: pytest-benchmark ; extra == 'tests'   [![Docs - GitHub.io]
+(https://img.shields.io/static/
 v1?logo=github&style=flat&color=pink&label=docs&message=tensordict)][#docs-
 package] [![Benchmarks](https://img.shields.io/badge/Benchmarks-blue.svg)]
 [#docs-package-benchmark] [![Python version](https://img.shields.io/pypi/
 pyversions/tensordict.svg)](https://www.python.org/downloads/) [![GitHub
 license](https://img.shields.io/badge/license-MIT-blue.svg)][#github-license]
 [pypi_version] [pypi_nightly_version] [![Downloads](https://static.pepy.tech/
 personalized-badge/
```

